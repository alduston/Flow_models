{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rZU16q_rj09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b811311-43b4-42ce-d611-034613fa0e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 8 steps, Blend score( True ) : W2 =9.068e-02, W2_floor = 2.175e-02, score_RMSE=5.252e+01, time=7.3s\n",
            " \n"
          ]
        }
      ],
      "source": [
        "from re import S\n",
        "from ctypes import create_unicode_buffer\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "from itertools import combinations\n",
        "\n",
        "from math import ceil\n",
        "from functools import partial\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------\n",
        "# Multi-scale MMD divergence\n",
        "# ----------------------------------------------\n",
        "\n",
        "try:\n",
        "    import cupy as cp\n",
        "except Exception:\n",
        "    cp = None\n",
        "import numpy as np\n",
        "\n",
        "M_MMD_DEFAULTS = {\n",
        "    # kernel mixture\n",
        "    \"kernel\": \"rbf\",        # 'rbf' or 'imq'\n",
        "    \"scales\": None,         # list/array of bandwidths (RBF sigmas or IMQ c's). If None, use median heuristic.\n",
        "    \"scale_weights\": None,  # list/array of nonnegative weights (same length as scales). If None, uniform.\n",
        "    # median heuristic for automatic scales (used if scales=None)\n",
        "    \"use_median_heuristic\": True,\n",
        "    \"num_scales\": 5,        # number of scales in the mixture\n",
        "    \"scale_min\": 0.5,       # lower multiplier relative to base (logspace)\n",
        "    \"scale_max\": 2.0,       # upper multiplier relative to base (logspace)\n",
        "    # estimator & output\n",
        "    \"unbiased\": True,       # U-statistic (exclude diagonals in Kxx, Kyy)\n",
        "    \"squared\": False,       # return MMD^2 if True, else sqrt(max(MMD^2,0))\n",
        "    # kernel-specific params\n",
        "    \"imq_beta\": 0.5,        # k_IMQ(d2) = (1 + d2 / c^2)^(-beta)\n",
        "    # numerics\n",
        "    \"chunk_size\": 4096,     # process pairwise blocks to limit memory\n",
        "    \"return_per_scale\": False, # if True, also return dict with per-scale contributions\n",
        "    # optional per-sample weights (length N_x / N_y). If provided, computes *weighted* MMD.\n",
        "    \"x_weights\": None,\n",
        "    \"y_weights\": None,\n",
        "}\n",
        "\n",
        "def _xp_of(arr):\n",
        "    \"\"\"Return array module (np or cp) that matches arr.\"\"\"\n",
        "    if cp is not None and isinstance(arr, cp.ndarray):\n",
        "        return cp\n",
        "    return np\n",
        "\n",
        "def _as_xp(x, xp):\n",
        "    \"\"\"Ensure x is an xp array.\"\"\"\n",
        "    if xp is np:\n",
        "        return np.asarray(x)\n",
        "    else:\n",
        "        return x if isinstance(x, xp.ndarray) else xp.asarray(x)\n",
        "\n",
        "def _median_pairwise_sqdist(Z, xp, max_samples=1024):\n",
        "    \"\"\"Robust median of pairwise squared distances from a subsample.\"\"\"\n",
        "    n = Z.shape[0]\n",
        "    if n <= 1:\n",
        "        return xp.array(1.0, dtype=Z.dtype)\n",
        "    idx = xp.random.permutation(n)[:min(n, max_samples)]\n",
        "    S = Z[idx]\n",
        "    # pairwise squared distances for the subsample\n",
        "    S2 = xp.sum(S * S, axis=1, keepdims=True)\n",
        "    D2 = xp.maximum(S2 + S2.T - 2 * (S @ S.T), 0.0)\n",
        "    # exclude zeros on the diagonal\n",
        "    tri = D2[~xp.eye(D2.shape[0], dtype=bool)]\n",
        "    if tri.size == 0:\n",
        "        return xp.array(1.0, dtype=Z.dtype)\n",
        "    return xp.median(tri)\n",
        "\n",
        "def _kernel_from_d2(d2, kernel, scale, xp, beta):\n",
        "    \"\"\"Evaluate kernel from squared distances.\"\"\"\n",
        "    if kernel == \"rbf\":\n",
        "        # k(d2) = exp(- d2 / (2 sigma^2))\n",
        "        inv2sig2 = 1.0 / (2.0 * (scale ** 2))\n",
        "        return xp.exp(-d2 * inv2sig2)\n",
        "    elif kernel == \"imq\":\n",
        "        # k(d2) = (1 + d2 / c^2)^(-beta)\n",
        "        return (1.0 + d2 / (scale ** 2)) ** (-beta)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown kernel '{kernel}' (use 'rbf' or 'imq').\")\n",
        "\n",
        "def _sum_kernel_pairs_XY(X, Y, scales, weights, kernel, xp, beta, chunk, xw=None, yw=None):\n",
        "    \"\"\"\n",
        "    Sum over k(x_i, y_j) for all i,j across a *mixture* of kernels.\n",
        "    Returns per-scale sums as array [S] and normalization constant (weighted).\n",
        "    \"\"\"\n",
        "    nx, ny = X.shape[0], Y.shape[0]\n",
        "    S = xp.zeros((len(scales),), dtype=X.dtype)\n",
        "    # normalization (weighted or unweighted)\n",
        "    if xw is None and yw is None:\n",
        "        norm = float(nx) * float(ny)\n",
        "    else:\n",
        "        xw = xp.ones(nx, dtype=X.dtype) if xw is None else xw\n",
        "        yw = xp.ones(ny, dtype=Y.dtype) if yw is None else yw\n",
        "        norm = float(xp.sum(xw) * xp.sum(yw))\n",
        "\n",
        "    for i0 in range(0, nx, chunk):\n",
        "        Xi = X[i0:i0 + chunk]\n",
        "        # precompute norms for distance\n",
        "        Xi2 = xp.sum(Xi * Xi, axis=1, keepdims=True)  # (bi,1)\n",
        "        wi = None if xw is None else xw[i0:i0 + chunk]\n",
        "        for j0 in range(0, ny, chunk):\n",
        "            Yj = Y[j0:j0 + chunk]\n",
        "            Yj2 = xp.sum(Yj * Yj, axis=1, keepdims=True)  # (bj,1)\n",
        "            # (bi,bj) squared distances\n",
        "            D2 = xp.maximum(Xi2 + Yj2.T - 2.0 * (Xi @ Yj.T), 0.0)\n",
        "            # stack per-scale kernels with broadcasting: (S, bi, bj)\n",
        "            Ks = xp.stack([_kernel_from_d2(D2, kernel, s, xp, beta) for s in scales], axis=0)\n",
        "            if xw is None and yw is None:\n",
        "                S += Ks.sum(axis=(1, 2))\n",
        "            else:\n",
        "                wj = yw[j0:j0 + chunk]\n",
        "                W = (wi[:, None] * wj[None, :])  # (bi,bj)\n",
        "                S += xp.tensordot(Ks, W, axes=((1, 2), (0, 1)))  # sum_{bi,bj} Ks[...,bi,bj]*W[bi,bj]\n",
        "    # apply mixture weights here (leave per-scale sums unweighted for diagnostics)\n",
        "    if weights is not None:\n",
        "        total = float(xp.sum(weights * (S / norm)))\n",
        "    else:\n",
        "        total = float(xp.sum(S / norm))\n",
        "    return S, norm, total\n",
        "\n",
        "def _sum_kernel_pairs_XX(A, scales, weights, kernel, xp, beta, chunk, unbiased=True, aw=None):\n",
        "    \"\"\"\n",
        "    Sum over k(a_i, a_j) across a *mixture* of kernels for a single set A.\n",
        "    Returns per-scale sums as array [S] and normalization constant (weighted).\n",
        "    For unbiased=True, excludes diagonals (i != j) using ordered-pairs normalization n(n-1).\n",
        "    \"\"\"\n",
        "    n = A.shape[0]\n",
        "    S = xp.zeros((len(scales),), dtype=A.dtype)\n",
        "\n",
        "    if aw is None:\n",
        "        if unbiased:\n",
        "            norm = float(n) * float(max(n - 1, 1))\n",
        "        else:\n",
        "            norm = float(n) * float(n)\n",
        "    else:\n",
        "        aw = aw.astype(A.dtype, copy=False)\n",
        "        s1 = xp.sum(aw)\n",
        "        if unbiased:\n",
        "            norm = float(s1 * s1 - xp.sum(aw * aw))\n",
        "        else:\n",
        "            norm = float(s1 * s1)\n",
        "\n",
        "    for i0 in range(0, n, chunk):\n",
        "        Ai = A[i0:i0 + chunk]\n",
        "        Ai2 = xp.sum(Ai * Ai, axis=1, keepdims=True)\n",
        "        wi = None if aw is None else aw[i0:i0 + chunk]\n",
        "        j_start = i0  # upper blocks including diagonal\n",
        "        for j0 in range(j_start, n, chunk):\n",
        "            Aj = A[j0:j0 + chunk]\n",
        "            Aj2 = xp.sum(Aj * Aj, axis=1, keepdims=True)\n",
        "            D2 = xp.maximum(Ai2 + Aj2.T - 2.0 * (Ai @ Aj.T), 0.0)\n",
        "            Ks = xp.stack([_kernel_from_d2(D2, kernel, s, xp, beta) for s in scales], axis=0)  # (S,bi,bj)\n",
        "\n",
        "            if aw is None:\n",
        "                if i0 == j0:\n",
        "                    if unbiased:\n",
        "                        # subtract diagonals; keep off-diagonals both (i,j) and (j,i) inside the block\n",
        "                        diag = xp.stack([xp.diag(Ks[k]) for k in range(Ks.shape[0])], axis=0).sum(axis=1)\n",
        "                        S += Ks.sum(axis=(1, 2)) - diag\n",
        "                    else:\n",
        "                        S += Ks.sum(axis=(1, 2))\n",
        "                else:\n",
        "                    # off-diagonal blocks contribute both (i,j) and (j,i)\n",
        "                    S += 2.0 * Ks.sum(axis=(1, 2))\n",
        "            else:\n",
        "                wj = aw[j0:j0 + chunk]\n",
        "                W = (wi[:, None] * wj[None, :])  # (bi,bj)\n",
        "                if i0 == j0:\n",
        "                    if unbiased:\n",
        "                        # zero-out diagonal weights before summing\n",
        "                        # subtract weighted diagonal: sum_k Ks[k]*W with diag set to 0\n",
        "                        # Equivalent: total sum minus diag(K)*w^2\n",
        "                        diag = xp.stack([xp.diag(Ks[k]) for k in range(Ks.shape[0])], axis=0)  # (S, b)\n",
        "                        diag_w2 = (wi * wi)[None, :] * diag  # (S,b)\n",
        "                        S += xp.tensordot(Ks, W, axes=((1, 2), (0, 1))) - diag_w2.sum(axis=1)\n",
        "                    else:\n",
        "                        S += xp.tensordot(Ks, W, axes=((1, 2), (0, 1)))\n",
        "                else:\n",
        "                    # both (i,j) and (j,i) pairs\n",
        "                    S += 2.0 * xp.tensordot(Ks, W, axes=((1, 2), (0, 1)))\n",
        "    if weights is not None:\n",
        "        total = float(xp.sum(weights * (S / norm)))\n",
        "    else:\n",
        "        total = float(xp.sum(S / norm))\n",
        "    return S, norm, total\n",
        "\n",
        "def M_MMD(X, Y, params=None):\n",
        "    \"\"\"\n",
        "    Multi-scale MMD divergence between sample sets X ~ P and Y ~ Q.\n",
        "\n",
        "    Args:\n",
        "        X: (N_x, d) numpy or cupy array.\n",
        "        Y: (N_y, d) numpy or cupy array.\n",
        "        params: dict overriding M_MMD_DEFAULTS.\n",
        "            - kernel: 'rbf' or 'imq'\n",
        "            - scales: list/array of bandwidths (RBF sigmas or IMQ c's). If None, automatic.\n",
        "            - scale_weights: list/array (same length as scales). Defaults to uniform.\n",
        "            - use_median_heuristic: if True and scales=None, set base scale from combined median distance.\n",
        "                * RBF: base sigma = sqrt(0.5 * median(D^2)), then logspace multipliers [scale_min, scale_max]\n",
        "                * IMQ: base c = sqrt(median(D^2)), then logspace multipliers\n",
        "            - num_scales, scale_min, scale_max: for automatic scales (logspace).\n",
        "            - unbiased: U-statistic (exclude diagonals in within terms).\n",
        "            - squared: return MMD^2 if True; else return sqrt(max(MMD^2, 0)).\n",
        "            - imq_beta: exponent for IMQ kernel.\n",
        "            - chunk_size: block size for memory control.\n",
        "            - x_weights, y_weights: optional per-sample nonnegative weights.\n",
        "            - return_per_scale: if True, also returns a dict with per-scale pieces.\n",
        "\n",
        "    Returns:\n",
        "        mmd_value (float), and optionally (if return_per_scale) a dict with\n",
        "        'per_scale_mmd2', 'scales', 'scale_weights'.\n",
        "    \"\"\"\n",
        "    # merge params with defaults\n",
        "    p = dict(M_MMD_DEFAULTS)\n",
        "    if params is not None:\n",
        "        p.update(params)\n",
        "\n",
        "    # choose backend\n",
        "    xp = _xp_of(X)\n",
        "    X = _as_xp(X, xp)\n",
        "    Y = _as_xp(Y, xp)\n",
        "    X = X.reshape(X.shape[0], -1)\n",
        "    Y = Y.reshape(Y.shape[0], -1)\n",
        "\n",
        "    # weights (optional)\n",
        "    xw = None if p[\"x_weights\"] is None else _as_xp(p[\"x_weights\"], xp).astype(X.dtype, copy=False)\n",
        "    yw = None if p[\"y_weights\"] is None else _as_xp(p[\"y_weights\"], xp).astype(Y.dtype, copy=False)\n",
        "\n",
        "    # build scales & weights\n",
        "    if p[\"scales\"] is None:\n",
        "        # base scale from combined median distance\n",
        "        Z = xp.concatenate([X, Y], axis=0)\n",
        "        med2 = _median_pairwise_sqdist(Z, xp, max_samples=1024)\n",
        "        if p[\"kernel\"] == \"rbf\":\n",
        "            base = xp.sqrt(xp.maximum(0.5 * med2, 1e-12))\n",
        "        else:  # imq\n",
        "            base = xp.sqrt(xp.maximum(med2, 1e-12))\n",
        "        mul = xp.logspace(np.log10(p[\"scale_min\"]), np.log10(p[\"scale_max\"]), p[\"num_scales\"])\n",
        "        scales = base * mul\n",
        "    else:\n",
        "        scales = _as_xp(p[\"scales\"], xp).astype(X.dtype, copy=False)\n",
        "\n",
        "    if p[\"scale_weights\"] is None:\n",
        "        scale_weights = xp.ones((len(scales),), dtype=X.dtype) / float(len(scales))\n",
        "    else:\n",
        "        w = _as_xp(p[\"scale_weights\"], xp).astype(X.dtype, copy=False)\n",
        "        w_sum = xp.sum(w)\n",
        "        scale_weights = w / (w_sum if float(w_sum) > 0 else 1.0)\n",
        "\n",
        "    # compute per-set & cross sums, with mixture applied after normalization\n",
        "    Sxx, nxx, mix_xx = _sum_kernel_pairs_XX(X, scales, scale_weights, p[\"kernel\"], xp,\n",
        "                                            p[\"imq_beta\"], p[\"chunk_size\"], p[\"unbiased\"], xw)\n",
        "    Syy, nyy, mix_yy = _sum_kernel_pairs_XX(Y, scales, scale_weights, p[\"kernel\"], xp,\n",
        "                                            p[\"imq_beta\"], p[\"chunk_size\"], p[\"unbiased\"], yw)\n",
        "    Sxy, nxy, mix_xy = _sum_kernel_pairs_XY(X, Y, scales, scale_weights, p[\"kernel\"], xp,\n",
        "                                            p[\"imq_beta\"], p[\"chunk_size\"], xw, yw)\n",
        "\n",
        "    # Full mixture MMD^2\n",
        "    mmd2 = (mix_xx + mix_yy - 2.0 * mix_xy)\n",
        "    # numerical guard\n",
        "    if mmd2 < 0.0:\n",
        "        mmd2 = 0.0\n",
        "\n",
        "    if p[\"squared\"]:\n",
        "        out = float(mmd2)\n",
        "    else:\n",
        "        out = float(np.sqrt(mmd2))  # safe cast; cp scalar ok through numpy sqrt of float\n",
        "\n",
        "    if p[\"return_per_scale\"]:\n",
        "        # per-scale contributions *before* mixture weighting, but normalized\n",
        "        per_scale = (Sxx / nxx) + (Syy / nyy) - 2.0 * (Sxy / nxy)\n",
        "        # clamp small negatives\n",
        "        per_scale = xp.maximum(per_scale, 0.0)\n",
        "        return out, {\n",
        "            \"per_scale_mmd2\": _as_xp(per_scale, np),  # move to numpy for easy logging\n",
        "            \"scales\": _as_xp(scales, np),\n",
        "            \"scale_weights\": _as_xp(scale_weights, np),\n",
        "        }\n",
        "    return out\n",
        "\n",
        "\n",
        "# ------------------------------ Single-σ MMD ----------------------------------\n",
        "def OT_MMD(X, Y, *, sigma=1.0, unbiased=False, chunk=4096):\n",
        "    \"\"\"\n",
        "    Single-scale RBF MMD between X and Y with bandwidth `sigma`.\n",
        "    Works with NumPy or CuPy arrays; keeps everything on-device if CuPy.\n",
        "    \"\"\"\n",
        "    # choose xp\n",
        "    xp = cp if (cp is not None and isinstance(X, cp.ndarray)) or \\\n",
        "               (cp is not None and isinstance(Y, cp.ndarray)) else np\n",
        "    X = xp.asarray(X, dtype=xp.float64).reshape(X.shape[0], -1)\n",
        "    Y = xp.asarray(Y, dtype=xp.float64).reshape(Y.shape[0], -1)\n",
        "    n, m = X.shape[0], Y.shape[0]\n",
        "    inv2s2 = 1.0 / (2.0 * (float(sigma) ** 2))\n",
        "\n",
        "    def _rbf_sum(A, B):\n",
        "        SA = xp.sum(A * A, axis=1, keepdims=True)\n",
        "        SB = xp.sum(B * B, axis=1, keepdims=True)\n",
        "        D2 = xp.maximum(SA + SB.T - 2.0 * (A @ B.T), 0.0)\n",
        "        return xp.exp(-inv2s2 * D2).sum()\n",
        "\n",
        "    if unbiased:\n",
        "        # U-statistic: exclude self terms in Kxx, Kyy, normalize by n(n-1), m(m-1)\n",
        "        # Block to limit memory\n",
        "        def _self_sum(A):\n",
        "            s = xp.array(0.0, dtype=xp.float64)\n",
        "            NA = A.shape[0]\n",
        "            for i0 in range(0, NA, chunk):\n",
        "                Ai = A[i0:i0 + chunk]\n",
        "                Ai2 = xp.sum(Ai * Ai, axis=1, keepdims=True)\n",
        "                for j0 in range(0, NA, chunk):\n",
        "                    Aj = A[j0:j0 + chunk]\n",
        "                    Aj2 = xp.sum(Aj * Aj, axis=1, keepdims=True)\n",
        "                    D2 = xp.maximum(Ai2 + Aj2.T - 2.0 * (Ai @ Aj.T), 0.0)\n",
        "                    K = xp.exp(-inv2s2 * D2)\n",
        "                    if i0 == j0:\n",
        "                        K = K - xp.eye(K.shape[0], dtype=K.dtype)\n",
        "                    s += K.sum()\n",
        "            return s\n",
        "\n",
        "        s_xx = _self_sum(X) / max(n * (n - 1), 1)\n",
        "        s_yy = _self_sum(Y) / max(m * (m - 1), 1)\n",
        "\n",
        "    else:\n",
        "        # Biased estimator: include diagonals, normalize by n^2, m^2\n",
        "        s_xx = _rbf_sum(X, X) / max(n * n, 1)\n",
        "        s_yy = _rbf_sum(Y, Y) / max(m * m, 1)\n",
        "\n",
        "    s_xy = _rbf_sum(X, Y) / max(n * m, 1)\n",
        "    mmd2 = float((s_xx + s_yy - 2.0 * s_xy))\n",
        "    return math.sqrt(mmd2) if mmd2 > 0 else 0.0\n",
        "\n",
        "\n",
        "# Script One, works well, just fits np estimators uses for sampling\n",
        "\n",
        "\n",
        "def _to_numpy(arr):\n",
        "    \"\"\"Return a NumPy view; copy from GPU to CPU if the input is a CuPy array.\"\"\"\n",
        "    try:\n",
        "        import cupy as cp\n",
        "        if isinstance(arr, cp.ndarray):\n",
        "            return arr.get()          # move to host\n",
        "    except ImportError:\n",
        "        pass                          # no CuPy in this env\n",
        "    return np.asarray(arr)\n",
        "\n",
        "\n",
        "def prune_cp_arr(x_cp, p_percent = 1):\n",
        "    x = cp.asarray(x_cp)\n",
        "    if x.ndim != 2:\n",
        "        raise ValueError(\"expected a 2D CuPy array (N,d)\")\n",
        "    if not (0 <= p_percent <= 100):\n",
        "        raise ValueError(\"p_percent must be in [0, 100]\")\n",
        "\n",
        "    N = x.shape[0]\n",
        "    if N == 0:\n",
        "        return x, cp.zeros(0, dtype=cp.bool_)\n",
        "\n",
        "    finite_rows = cp.isfinite(x).all(axis=1)\n",
        "    idx_finite = cp.where(finite_rows)[0]\n",
        "    xf = x[finite_rows]\n",
        "    M = xf.shape[0]\n",
        "\n",
        "    if M == 0:\n",
        "        return x[:0], cp.zeros(N, dtype=cp.bool_)\n",
        "\n",
        "    if p_percent == 0:\n",
        "        keep_mask = cp.zeros(N, dtype=cp.bool_)\n",
        "        keep_mask[idx_finite] = True\n",
        "        return xf, keep_mask\n",
        "    if p_percent == 100:\n",
        "        return x[:0], cp.zeros(N, dtype=cp.bool_)\n",
        "\n",
        "    k = int(cp.ceil(M * (1.0 - p_percent / 100.0)).item())  # rows to keep among finite ones\n",
        "    if k <= 0:\n",
        "        return x[:0], cp.zeros(N, dtype=cp.bool_)\n",
        "\n",
        "    norms = cp.linalg.norm(xf, axis=1)\n",
        "    keep_local = cp.argpartition(norms, kth=k-1)[:k]  # indices of k smallest norms in xf\n",
        "\n",
        "    keep_mask = cp.zeros(N, dtype=cp.bool_)\n",
        "    keep_mask[idx_finite[keep_local]] = True\n",
        "    return x[keep_mask], keep_mask\n",
        "\n",
        "\n",
        "import cupy as cp\n",
        "\n",
        "\n",
        "\n",
        "def sample_gmm_gpu(n_samples, means, stds, weights, *, seed=None):\n",
        "    xp = cp\n",
        "    means   = xp.asarray(means,   dtype=xp.float64)\n",
        "    stds    = xp.asarray(stds,    dtype=xp.float64)\n",
        "    weights = xp.asarray(weights, dtype=xp.float64).reshape(-1)\n",
        "\n",
        "    C, D = means.shape\n",
        "    if stds.ndim == 1 and stds.shape == (C,):\n",
        "        cov_mode = \"isotropic\"\n",
        "    elif stds.ndim == 2 and stds.shape == (C, D):\n",
        "        cov_mode = \"diag\"\n",
        "    elif stds.ndim == 3 and stds.shape == (C, D, D):\n",
        "        cov_mode = \"full\"\n",
        "    else:\n",
        "        raise ValueError(\"stds must be (C,), (C,D), or (C,D,D)\")\n",
        "\n",
        "    probs = (weights / float(weights.sum())).ravel()\n",
        "    rng = xp.random.RandomState(seed) if seed is not None else xp.random\n",
        "    choices = rng.choice(C, size=int(n_samples), p=cp.asnumpy(probs))\n",
        "    out = xp.empty((n_samples, D), dtype=xp.float64)\n",
        "\n",
        "    for i in range(C):\n",
        "        mask = (choices == i); cnt = int(mask.sum())\n",
        "        if not cnt: continue\n",
        "        z = rng.standard_normal((cnt, D), dtype=xp.float64)\n",
        "        if cov_mode == \"isotropic\":\n",
        "            out[mask] = means[i] + z * stds[i]\n",
        "        elif cov_mode == \"diag\":\n",
        "            out[mask] = means[i] + z * stds[i]          # (cnt,D) * (D,)\n",
        "        else:  # full\n",
        "            L = xp.linalg.cholesky(stds[i])             # Σ^{1/2}\n",
        "            out[mask] = means[i] + z @ L.T\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "def score_gmm_gpu(x, means, stds, weights, batch_size=4096):\n",
        "    \"\"\"\n",
        "    ∇_x log ∑_c w_c N(x; μ_c, Σ_c)\n",
        "    stds:\n",
        "      - (C,)   : isotropic std per comp  (σ_c)          -> Σ_c = σ_c^2 I\n",
        "      - (C,D)  : diagonal std per comp   (σ_{c,d})      -> Σ_c = diag(σ^2)\n",
        "      - (C,D,D): full covariance per comp (Σ_c SPD)\n",
        "    \"\"\"\n",
        "    xp = cp\n",
        "    x       = xp.asarray(x,       dtype=xp.float64)\n",
        "    means   = xp.asarray(means,   dtype=xp.float64)\n",
        "    weights = xp.asarray(weights, dtype=xp.float64).reshape(-1)\n",
        "    stds    = xp.asarray(stds,    dtype=xp.float64)\n",
        "\n",
        "    N, D = x.shape\n",
        "    C    = means.shape[0]\n",
        "    if N == 0:\n",
        "        return xp.zeros((0, D), dtype=xp.float64)\n",
        "\n",
        "    const = -0.5 * D * xp.log(2.0 * xp.pi)\n",
        "    log_w = xp.log(weights + 1e-300)\n",
        "    out   = xp.zeros_like(x)\n",
        "\n",
        "    # ----- (1) Isotropic: stds = (C,)\n",
        "    if stds.ndim == 1:\n",
        "        inv_vars     = 1.0 / (stds**2)                 # (C,)\n",
        "        log_det_vars = D * xp.log(stds**2)             # (C,)\n",
        "        norm_const   = const - 0.5 * log_det_vars      # (C,)\n",
        "\n",
        "        for i in range(0, N, batch_size):\n",
        "            xb   = x[i:i+batch_size]                   # (B,D)\n",
        "            diff = means[None, :, :] - xb[:, None, :]  # (B,C,D)\n",
        "            quad = (diff**2).sum(axis=-1) * inv_vars[None, :]         # (B,C)\n",
        "\n",
        "            logp = log_w[None, :] + norm_const[None, :] - 0.5 * quad  # (B,C)\n",
        "            m    = xp.max(logp, axis=1, keepdims=True)\n",
        "            resp = xp.exp(logp - m); resp /= resp.sum(axis=1, keepdims=True) + 1e-300\n",
        "\n",
        "            contrib = (resp[:, :, None] * diff) * inv_vars[None, :, None]   # (B,C,D)\n",
        "            out[i:i+batch_size] = contrib.sum(axis=1)\n",
        "\n",
        "        return out\n",
        "\n",
        "    # ----- (2) Diagonal: stds = (C,D)\n",
        "    if stds.ndim == 2 and stds.shape[1] == D:\n",
        "        inv_vars     = 1.0 / (stds**2)                 # (C,D)\n",
        "        log_det_vars = xp.log(stds**2).sum(axis=1)     # (C,)\n",
        "        norm_const   = const - 0.5 * log_det_vars      # (C,)\n",
        "\n",
        "        for i in range(0, N, batch_size):\n",
        "            xb   = x[i:i+batch_size]                   # (B,D)\n",
        "            diff = means[None, :, :] - xb[:, None, :]  # (B,C,D)\n",
        "            quad = ((diff**2) * inv_vars[None, :, :]).sum(axis=-1)          # (B,C)\n",
        "\n",
        "            logp = log_w[None, :] + norm_const[None, :] - 0.5 * quad        # (B,C)\n",
        "            m    = xp.max(logp, axis=1, keepdims=True)\n",
        "            resp = xp.exp(logp - m); resp /= resp.sum(axis=1, keepdims=True) + 1e-300\n",
        "\n",
        "            contrib = (resp[:, :, None] * diff) * inv_vars[None, :, :]      # (B,C,D)\n",
        "            out[i:i+batch_size] = contrib.sum(axis=1)\n",
        "\n",
        "        return out\n",
        "\n",
        "    # ----- (3) Full: stds = (C,D,D)  (here 'stds' is Σ_c)\n",
        "    if stds.ndim == 3 and stds.shape[1] == D and stds.shape[2] == D:\n",
        "        # Cholesky once per component\n",
        "        Ls      = xp.linalg.cholesky(stds)                                     # (C,D,D), Σ = L L^T\n",
        "        log_det = 2.0 * xp.log(xp.diagonal(Ls, axis1=1, axis2=2)).sum(axis=1)  # (C,)\n",
        "        norm_const = const - 0.5 * log_det                                     # (C,)\n",
        "\n",
        "        for i in range(0, N, batch_size):\n",
        "            xb    = x[i:i+batch_size]                         # (B,D)\n",
        "            B     = xb.shape[0]\n",
        "            logp  = xp.empty((B, C), dtype=xp.float64)\n",
        "            grads = xp.empty((B, C, D), dtype=xp.float64)\n",
        "\n",
        "            for c in range(C):\n",
        "                L = Ls[c]                                      # (D,D)\n",
        "\n",
        "                # quad term: (x - μ)^T Σ^{-1} (x - μ) = || L^{-1}(x-μ) ||^2\n",
        "                diff_x = xb - means[c][None, :]                # (B,D)\n",
        "                v = xp.linalg.solve(L, diff_x.T)               # (D,B)\n",
        "                quad = xp.sum(v * v, axis=0)                   # (B,)\n",
        "                logp[:, c] = log_w[c] + norm_const[c] - 0.5 * quad\n",
        "\n",
        "                # gradient per comp: Σ^{-1}(μ - x) = L^{-T} L^{-1} (μ - x)\n",
        "                diff_m = means[c][None, :] - xb                # (B,D)\n",
        "                y = xp.linalg.solve(L, diff_m.T)               # (D,B)\n",
        "                g = xp.linalg.solve(L.T, y).T                  # (B,D)\n",
        "                grads[:, c, :] = g\n",
        "\n",
        "            m    = xp.max(logp, axis=1, keepdims=True)\n",
        "            resp = xp.exp(logp - m); resp /= resp.sum(axis=1, keepdims=True) + 1e-300\n",
        "\n",
        "            out[i:i+batch_size] = xp.einsum(\"bc,bcd->bd\", resp, grads)\n",
        "\n",
        "        return out\n",
        "\n",
        "    raise ValueError(\"stds must be (C,), (C,D), or (C,D,D) (the last is full covariances).\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def score_div_gmm(x, means, covs, weights, batch_size=4096):\n",
        "\n",
        "    xp = cp\n",
        "    x       = xp.asarray(x, dtype=xp.float64)\n",
        "    means   = xp.asarray(means, dtype=xp.float64)\n",
        "    covs    = xp.asarray(covs,  dtype=xp.float64)\n",
        "    weights = xp.asarray(weights, dtype=xp.float64).reshape(-1)\n",
        "\n",
        "    N, D = x.shape\n",
        "    C    = means.shape[0]\n",
        "    if N == 0:\n",
        "        return xp.zeros_like(x), xp.zeros((0,), dtype=xp.float64)\n",
        "\n",
        "    # --- weight normalization & constants\n",
        "    wsum = xp.sum(weights)\n",
        "    if not xp.isfinite(wsum) or wsum <= 0:\n",
        "        raise ValueError(\"Mixture weights must sum to a positive value.\")\n",
        "    log_w = xp.log(weights / wsum + 1e-300)\n",
        "    const = -0.5 * D * xp.log(2.0 * xp.pi)\n",
        "\n",
        "    # --- covariance handling\n",
        "    kind = None\n",
        "    if covs.ndim == 1 and covs.shape[0] == C:             # isotropic stds\n",
        "        kind     = \"iso\"\n",
        "        inv_vars = 1.0 / (covs**2)                         # (C,)\n",
        "        log_det  = D * xp.log(covs**2)                     # (C,)\n",
        "        trA      = D * inv_vars                            # (C,)\n",
        "    elif covs.ndim == 2 and covs.shape == (C, D):          # diagonal stds\n",
        "        kind     = \"diag\"\n",
        "        inv_vars = 1.0 / (covs**2)                         # (C,D)\n",
        "        log_det  = xp.log(covs**2).sum(axis=1)             # (C,)\n",
        "        trA      = inv_vars.sum(axis=1)                    # (C,)\n",
        "    elif covs.ndim == 3 and covs.shape[1:] == (D, D):      # full Σ\n",
        "        kind = \"full\"\n",
        "        Ls, log_det, trA = [], xp.empty(C), xp.empty(C)\n",
        "        I = xp.eye(D, dtype=xp.float64)\n",
        "        for c in range(C):\n",
        "            L = xp.linalg.cholesky(covs[c])\n",
        "            Ls.append(L)\n",
        "            log_det[c] = 2.0 * xp.sum(xp.log(xp.diag(L)))\n",
        "            Z = xp.linalg.solve(L, I)                      # L^{-1}\n",
        "            trA[c] = xp.sum(Z * Z)                         # ||L^{-1}||_F^2 = tr(Σ^{-1})\n",
        "    else:\n",
        "        raise ValueError(\"covs must be (C,), (C,D) with stds, or (C,D,D) with full covariances.\")\n",
        "\n",
        "    # --- outputs\n",
        "    score = xp.empty_like(x)\n",
        "    div   = xp.empty((N,), dtype=xp.float64)\n",
        "\n",
        "    for i in range(0, N, batch_size):\n",
        "        xb = x[i:i+batch_size]                             # (B,D)\n",
        "        B  = xb.shape[0]\n",
        "\n",
        "        # per-component log-densities and a_c = A_c (mu_c - x)\n",
        "        if kind in (\"iso\", \"diag\"):\n",
        "            diff = means[None, :, :] - xb[:, None, :]      # (B,C,D)\n",
        "            if kind == \"iso\":\n",
        "                quad = (diff**2).sum(-1) * inv_vars[None, :]          # (B,C)\n",
        "                a    = diff * inv_vars[None, :, None]                  # (B,C,D)\n",
        "            else:\n",
        "                quad = ((diff**2) * inv_vars[None, :, :]).sum(-1)      # (B,C)\n",
        "                a    = diff * inv_vars[None, :, :]                     # (B,C,D)\n",
        "        else:\n",
        "            quad = xp.empty((B, C), dtype=xp.float64)\n",
        "            a    = xp.empty((B, C, D), dtype=xp.float64)\n",
        "            for c in range(C):\n",
        "                diff = (means[c][None, :] - xb)                        # (B,D)\n",
        "                # a_c = Σ_c^{-1} diff via two triangular solves\n",
        "                z = xp.linalg.solve(Ls[c], diff.T)                     # (D,B)\n",
        "                u = xp.linalg.solve(Ls[c].T, z)                        # (D,B)\n",
        "                a_c = u.T                                              # (B,D)\n",
        "                a[:, c, :] = a_c\n",
        "                quad[:, c] = xp.sum(diff * a_c, axis=1)                # diff^T Σ^{-1} diff\n",
        "\n",
        "        log_comp = log_w[None, :] + const - 0.5*log_det[None, :] - 0.5*quad  # (B,C)\n",
        "        m   = xp.max(log_comp, axis=1, keepdims=True)\n",
        "        resp = xp.exp(log_comp - m)\n",
        "        resp /= (resp.sum(axis=1, keepdims=True) + 1e-300)             # (B,C)\n",
        "        s = xp.sum(resp[:, :, None] * a, axis=1)                        # (B,D)\n",
        "\n",
        "        a_norm2 = xp.sum(a * a, axis=2)                                 # (B,C)\n",
        "        s_dot_a = xp.sum(s[:, None, :] * a, axis=2)                     # (B,C)\n",
        "        div_b   = xp.sum(resp * (a_norm2 - s_dot_a - trA[None, :]), axis=1)  # (B,)\n",
        "\n",
        "        score[i:i+B] = s\n",
        "        div[i:i+B]   = div_b\n",
        "\n",
        "    return score, div\n",
        "\n",
        "\n",
        "def hyvarinen_to_target(X, score_fn, div_fn, batch_size=65536):\n",
        "    \"\"\"\n",
        "    Hyvärinen-to-target loss: E[ 0.5||s(x)||^2 + div s(x) ].\n",
        "    `score_fn` returns (N,D), `div_fn` returns (N,).\n",
        "    \"\"\"\n",
        "    xp = cp\n",
        "    X  = xp.asarray(X, dtype=xp.float64)\n",
        "    N  = X.shape[0]\n",
        "    acc = 0.0\n",
        "    cnt = 0\n",
        "    for i in range(0, N, int(batch_size)):\n",
        "        xb = X[i:i+batch_size]\n",
        "        s  = score_fn(xb)\n",
        "        dv = div_fn(xb)\n",
        "        acc += float(xp.mean(0.5 * xp.sum(s*s, axis=1) + dv))\n",
        "        cnt += 1\n",
        "    return acc / max(cnt, 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def density_gmm_gpu(x, means, stds, weights, mode=\"absolute\", iso_std=1.0, batch_size=4096):\n",
        "    xp = cp\n",
        "    x      = x.astype(xp.float64, copy=False)\n",
        "    means  = means.astype(xp.float64, copy=False)\n",
        "    stds   = stds.astype(xp.float64, copy=False)\n",
        "    weights= weights.astype(xp.float64, copy=False)\n",
        "\n",
        "    N, D = x.shape\n",
        "    if N == 0:\n",
        "        return xp.zeros((0,), dtype=xp.float64)\n",
        "\n",
        "    # Normalize mixture weights safely\n",
        "    wsum = weights.sum()\n",
        "    if wsum <= 0:\n",
        "        raise ValueError(\"Mixture weights must sum to a positive value.\")\n",
        "    w = weights / wsum\n",
        "    log_w = xp.log(w + 1e-40)  # [C]\n",
        "\n",
        "    # Handle variances and normalization constants\n",
        "    if stds.ndim == 1:\n",
        "        # Isotropic per-component: Σ_c = (std_c^2) * I\n",
        "        inv_vars = 1.0 / (stds**2)                 # [C]\n",
        "        log_det_vars = D * xp.log(stds**2)         # [C]  (since det = (std^2)^D)\n",
        "    elif stds.ndim == 2 and stds.shape[1] == D:\n",
        "        # Diagonal per-component: Σ_c = diag(std_{c,1}^2, ..., std_{c,D}^2)\n",
        "        inv_vars = 1.0 / (stds**2)                 # [C, D]\n",
        "        log_det_vars = xp.log(stds**2).sum(axis=1) # [C]  (since det = prod_d std_{c,d}^2)\n",
        "    else:\n",
        "        raise ValueError(\"`stds` must have shape (C,) or (C, D).\")\n",
        "\n",
        "    const = -0.5 * D * xp.log(2.0 * xp.pi)         # scalar\n",
        "    norm_const = const - 0.5 * log_det_vars        # [C]\n",
        "\n",
        "    out = xp.empty((N,), dtype=xp.float64)\n",
        "\n",
        "    for i in range(0, N, batch_size):\n",
        "        xb = x[i:i+batch_size]                     # [B, D]\n",
        "        diff = xb[:, None, :] - means[None, :, :]  # [B, C, D]\n",
        "\n",
        "        if inv_vars.ndim == 1:\n",
        "            # quadratic term = sum_d (diff^2) * (1/std_c^2)\n",
        "            d2 = (diff**2).sum(axis=-1)            # [B, C]\n",
        "            quad = d2 * inv_vars[None, :]          # [B, C]\n",
        "        else:\n",
        "            # quadratic term = sum_d (diff^2 / std_{c,d}^2)\n",
        "            quad = ((diff**2) * inv_vars[None, :, :]).sum(axis=-1)  # [B, C]\n",
        "\n",
        "        # log component densities (including weights)\n",
        "        log_comp = log_w[None, :] + norm_const[None, :] - 0.5 * quad  # [B, C]\n",
        "\n",
        "        # log-sum-exp over components for numerical stability\n",
        "        m = xp.max(log_comp, axis=1, keepdims=True)                   # [B, 1]\n",
        "        log_mix = xp.squeeze(m, 1) + xp.log(xp.exp(log_comp - m).sum(axis=1))  # [B]\n",
        "\n",
        "        if mode == \"absolute\":\n",
        "            out[i:i+batch_size] = xp.exp(log_mix)                     # p_mix(x)\n",
        "        elif mode == \"relative\":\n",
        "            s2 = float(iso_std) ** 2\n",
        "            # log phi_iso(x) for N(0, s2 I)\n",
        "            log_iso = -0.5 * D * xp.log(2.0 * xp.pi * s2) - 0.5 * (xb**2).sum(axis=1) / s2  # [B]\n",
        "            out[i:i+batch_size] = xp.exp(log_mix - log_iso)           # p_mix(x)/phi_iso(x)\n",
        "        else:\n",
        "            raise ValueError(\"mode must be 'absolute' or 'relative'.\")\n",
        "\n",
        "    return out\n",
        "import cupy as cp\n",
        "\n",
        "\n",
        "def get_ou_evolved_gmm_params(t, means0, stds0, w0):\n",
        "    xp = cp\n",
        "    means0 = xp.asarray(means0)\n",
        "    e = xp.exp(-t)\n",
        "    e2 = e * e\n",
        "\n",
        "    # Evolved means\n",
        "    m_t = e * means0\n",
        "\n",
        "    S0 = xp.asarray(stds0)\n",
        "    K = means0.shape[0]\n",
        "    D = means0.shape[1]\n",
        "\n",
        "    # --- Full covariance cases ---\n",
        "    if (S0.ndim == 3 and S0.shape[-1] == S0.shape[-2]) or (S0.ndim == 2 and S0.shape[0] == S0.shape[1]):\n",
        "        # Ensure shape (K, D, D)\n",
        "        if S0.ndim == 2:  # (D, D) for all components\n",
        "            S0 = xp.broadcast_to(S0[None, :, :], (K, D, D))\n",
        "        # Σ_t = e^{-2t} Σ_0 + (1 - e^{-2t}) I\n",
        "        eye = xp.eye(D, dtype=S0.dtype)[None, :, :]  # (1, D, D)\n",
        "        Sigma_t = e2 * S0 + (1.0 - e2) * eye\n",
        "        return m_t, Sigma_t, w0\n",
        "\n",
        "    # --- Diagonal / isotropic std cases ---\n",
        "    # Treat S0 as stds; compute variances elementwise with broadcasting\n",
        "    v0 = S0 * S0\n",
        "    v_t = e2 * v0 + (1.0 - e2)  # adds isotropic I term\n",
        "    std_t = xp.sqrt(v_t)\n",
        "    return m_t, std_t, w0\n",
        "\n",
        "\n",
        "def OU_evolve_samples(y_cp, t_scalar):\n",
        "    import cupy as cp\n",
        "    y = cp.asarray(y_cp)\n",
        "    t = float(t_scalar)\n",
        "    et = cp.exp(-t)\n",
        "    sigma = cp.sqrt(cp.maximum(1.0 - et*et, 0.0))\n",
        "    z = cp.random.standard_normal(y.shape, dtype=y.dtype)\n",
        "    return et * y + sigma * z\n",
        "\n",
        "\n",
        "def samples_to_sampler(samples):\n",
        "    samples = np.asarray(samples)\n",
        "    N = samples.shape[0]\n",
        "    def sampler(n):\n",
        "        if n <= N:\n",
        "            return samples[:n].copy()\n",
        "        k = n // N; r = n % N\n",
        "        out = [samples for _ in range(k)]\n",
        "        if r: out.append(samples[:r])\n",
        "        return np.concatenate(out, axis=0).copy()\n",
        "    return sampler\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "try:\n",
        "    import cupy as cp\n",
        "except Exception:\n",
        "    cp = None\n",
        "\n",
        "import numpy as np\n",
        "try:\n",
        "    import cupy as cp\n",
        "except Exception:\n",
        "    cp = None\n",
        "\n",
        "\n",
        "\n",
        "def normalize_gmm_params(means, stds, weights, *, mode=\"global\", eps=1e-12):\n",
        "    \"\"\"\n",
        "    Center & scale a GMM so the mixture mean is 0 and the marginal variance is 1.\n",
        "\n",
        "    Inputs\n",
        "    ------\n",
        "    means   : (C, D)\n",
        "    stds    : (C,)  or (C, D)     # isotropic or diagonal per component (std, not var)\n",
        "    weights : (C,)\n",
        "    mode    : \"global\" -> one scalar scale s so average marginal variance == 1\n",
        "              \"per_dim\" -> vector scale s_d so EACH marginal variance == 1\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    means_n : (C, D)\n",
        "    stds_n  : (C,)  if input was (C,) and mode=\"global\"\n",
        "              (C,D) if mode=\"per_dim\" or input was (C,D)\n",
        "    weights_n : (C,)   (re-normalized to sum to 1)\n",
        "    \"\"\"\n",
        "    # pick backend\n",
        "    try:\n",
        "        import cupy as cp\n",
        "        xp = cp if isinstance(means, cp.ndarray) else __import__(\"numpy\")\n",
        "    except Exception:\n",
        "        import numpy as np\n",
        "        xp = np\n",
        "\n",
        "    m = xp.asarray(means,   dtype=xp.float64)\n",
        "    s = xp.asarray(stds,    dtype=xp.float64)\n",
        "    w = xp.asarray(weights, dtype=xp.float64)\n",
        "\n",
        "    # normalize weights\n",
        "    w = xp.clip(w, 0, xp.inf)\n",
        "    ws = w.sum()\n",
        "    if not xp.isfinite(ws) or ws <= 0:\n",
        "        raise ValueError(\"weights must sum to a positive value.\")\n",
        "    w = w / ws\n",
        "\n",
        "    C, D = m.shape\n",
        "\n",
        "    # per-dimension variance contributions\n",
        "    if s.ndim == 1:          # isotropic per component\n",
        "        var_terms = (s**2)[:, None]                # (C,1) -> broadcast to (C,D)\n",
        "    elif s.ndim == 2 and s.shape[1] == D:\n",
        "        var_terms = s**2                            # (C,D)\n",
        "    else:\n",
        "        raise ValueError(\"stds must have shape (C,) or (C,D).\")\n",
        "\n",
        "    # mixture stats\n",
        "    mu_mix  = w @ m                                  # (D,)\n",
        "    Ex2     = (w[:, None] * (var_terms + m**2)).sum(axis=0)  # (D,)\n",
        "    var_mix = xp.maximum(Ex2 - mu_mix**2, eps)       # (D,)\n",
        "\n",
        "    if mode == \"global\":\n",
        "        scale = xp.sqrt(var_mix.mean())              # scalar s\n",
        "        m_n = (m - mu_mix[None, :]) / scale\n",
        "        s_n = s / scale                              # keeps shape of s\n",
        "    elif mode == \"per_dim\":\n",
        "        scale = xp.sqrt(var_mix)                     # (D,)\n",
        "        m_n = (m - mu_mix[None, :]) / scale[None, :]\n",
        "        if s.ndim == 1:\n",
        "            s_n = s[:, None] / scale[None, :]        # becomes diagonal (C,D)\n",
        "        else:\n",
        "            s_n = s / scale[None, :]\n",
        "    else:\n",
        "        raise ValueError('mode must be \"global\" or \"per_dim\".')\n",
        "\n",
        "    return (m_n.astype(means.dtype, copy=False),\n",
        "            s_n.astype(stds.dtype,  copy=False),\n",
        "            w.astype(weights.dtype, copy=False))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_gmm_funcs(\n",
        "    num_c: int = 200,\n",
        "    k_dim: int = 5,\n",
        "    variant: str = \"helix\",\n",
        "    comp_std: float = 0.10,\n",
        "    overall_scale: float = 1.0,\n",
        "    rs=None,\n",
        "    m_dim: int = 3,\n",
        "    powerlaw_weights: bool = True,\n",
        "    seed: int = 0,\n",
        "    stds_specified = None,\n",
        "    weights_specified = None,\n",
        "    preset_params = None,\n",
        "    normalize = False,\n",
        "    size = 1,\n",
        "    embedding_mode: str = \"linear\"  # NEW\n",
        "):\n",
        "    # ---- RNG (seeded if requested) -----------------------------------------\n",
        "    if rs is not None and not isinstance(rs, np.random.RandomState):\n",
        "        raise TypeError(\"rs must be a numpy.random.RandomState\")\n",
        "    rng = rs if rs is not None else (np.random.RandomState(seed) if seed is not None\n",
        "                                     else np.random.RandomState())\n",
        "\n",
        "    xp = cp\n",
        "    m_embed = m_dim  # intrinsic dimension placeholder; updated by some variants\n",
        "\n",
        "    # -------------------------- helpers (NEW) -------------------------------\n",
        "    def _qr_tall(k, m):\n",
        "        \"\"\"Q in R^{k×m} with orthonormal columns (k >= m).\"\"\"\n",
        "        Q, _ = np.linalg.qr(rng.randn(k, m))\n",
        "        return Q[:, :m]\n",
        "\n",
        "    def _qr_wide(m, k):\n",
        "        \"\"\"A in R^{m×k} with orthonormal columns (k <= m).\"\"\"\n",
        "        G = rng.randn(m, k)\n",
        "        # Orthonormalize columns of G via SVD\n",
        "        U, _, Vt = np.linalg.svd(G, full_matrices=False)\n",
        "        return (U @ Vt)  # shape (m×k), columns orthonormal\n",
        "\n",
        "    def _orthonormal_complement(Q, k, s):\n",
        "        \"\"\"Given Q (k×m) with orthonormal cols, return Q_perp (k×s) orthonormal & Q^T Q_perp = 0.\"\"\"\n",
        "        if s <= 0:\n",
        "            return None\n",
        "        G = rng.randn(k, s)\n",
        "        G = G - Q @ (Q.T @ G)\n",
        "        Qp, _ = np.linalg.qr(G)\n",
        "        return Qp[:, :s]\n",
        "\n",
        "    def _embed_linear(X_intr):\n",
        "        # Preserve original behavior (branch uses m_dim, not m_embed)\n",
        "        if k_dim > m_dim:\n",
        "            Q = _qr_tall(k_dim, X_intr.shape[1])  # k×m_embed\n",
        "            Z = X_intr @ Q.T                      # num_c×k\n",
        "            covs = np.array([ (comp_std**2) * (Q @ Q.T) + (1e-4*comp_std)**2*np.eye(k_dim)\n",
        "                              for _ in range(num_c) ])\n",
        "        else:\n",
        "            Z = X_intr[:, :k_dim]\n",
        "            covs = np.array([ (comp_std**2) * np.eye(k_dim) for _ in range(num_c) ])\n",
        "        return Z, covs\n",
        "\n",
        "    def _embed_sine_wiggle(X_intr, amp=0.25, freq=1.0):\n",
        "        m = X_intr.shape[1]\n",
        "        # base linear map into k-dim\n",
        "        if k_dim >= m:\n",
        "            Q = _qr_tall(k_dim, m)               # k×m\n",
        "            Zlin = X_intr @ Q.T                  # num_c×k\n",
        "            # add wiggle in orthogonal directions\n",
        "            s = min(max(1, m), max(0, k_dim - m))\n",
        "            Q_perp = _orthonormal_complement(Q, k_dim, s)\n",
        "            if Q_perp is not None:\n",
        "                R = rng.randn(s, m) * freq\n",
        "                b = rng.uniform(0.0, 2*np.pi, size=(s,))\n",
        "                H = np.sin(X_intr @ R.T + b)     # num_c×s\n",
        "                Zwig = H @ Q_perp.T              # num_c×k\n",
        "                Z = Zlin + amp * Zwig\n",
        "            else:\n",
        "                Z = Zlin\n",
        "        else:\n",
        "            # reduce then wiggle using internal orthonormal columns\n",
        "            A = _qr_wide(m, k_dim)               # m×k\n",
        "            Zlin = X_intr @ A                    # num_c×k\n",
        "            # small nonlinear bump inside the current k-subspace\n",
        "            R = rng.randn(k_dim, m) * freq\n",
        "            b = rng.uniform(0.0, 2*np.pi, size=(k_dim,))\n",
        "            H = np.sin(X_intr @ R.T + b)         # num_c×k\n",
        "            Z = Zlin + amp * H\n",
        "        covs = np.array([ (comp_std**2) * np.eye(k_dim) for _ in range(num_c) ])\n",
        "        return Z, covs\n",
        "\n",
        "    def _embed_rff(X_intr, freq=1.0):\n",
        "        m = X_intr.shape[1]\n",
        "        M = max(1, k_dim // 2)\n",
        "        W = rng.randn(M, m) * freq\n",
        "        b = rng.uniform(0.0, 2*np.pi, size=(M,))\n",
        "        C = np.cos(X_intr @ W.T + b)\n",
        "        S = np.sin(X_intr @ W.T + b)\n",
        "        Z = np.concatenate([C, S], axis=1)\n",
        "        if Z.shape[1] < k_dim:\n",
        "            pad = X_intr @ rng.randn(m, k_dim - Z.shape[1])\n",
        "            Z = np.concatenate([Z, pad], axis=1)\n",
        "        else:\n",
        "            Z = Z[:, :k_dim]\n",
        "        covs = np.array([ (comp_std**2) * np.eye(k_dim) for _ in range(num_c) ])\n",
        "        return Z, covs\n",
        "\n",
        "    def _embed_radial(X_intr):\n",
        "        r = np.linalg.norm(X_intr, axis=1, keepdims=True)\n",
        "        feats = [X_intr, r, r**2, np.sin(r), np.cos(r)]\n",
        "        Z = np.concatenate(feats, axis=1)\n",
        "        while Z.shape[1] < k_dim:\n",
        "            w = rng.randn(X_intr.shape[1])\n",
        "            Z = np.hstack([Z, (X_intr @ w[:, None])**2])\n",
        "        Z = Z[:, :k_dim]\n",
        "        covs = np.array([ (comp_std**2) * np.eye(k_dim) for _ in range(num_c) ])\n",
        "        return Z, covs\n",
        "\n",
        "    # --------------------------- custom branch ------------------------------\n",
        "    if variant == \"custom\":\n",
        "        if preset_params is not None:\n",
        "            means_np, stds_np, weights_np = preset_params\n",
        "        else:\n",
        "            # draw random means in R^k_dim\n",
        "            means_np = rng.randn(num_c, k_dim) * overall_scale\n",
        "            # isotropic component stds\n",
        "            stds_np  = np.full(num_c, comp_std, dtype=float)\n",
        "            # random non-uniform weights\n",
        "            weights_np = rng.rand(num_c)\n",
        "            weights_np /= weights_np.sum()\n",
        "        # move to GPU\n",
        "        means   = xp.asarray(means_np, dtype=xp.float64)\n",
        "        stds    = xp.asarray(stds_np,   dtype=xp.float64)\n",
        "        weights = xp.asarray(weights_np, dtype=xp.float64)\n",
        "\n",
        "        def sampler(n, seed=None, debug=False):\n",
        "            return sample_gmm_gpu(n, means, stds, weights, seed=seed)\n",
        "\n",
        "        def score_func(x):\n",
        "            return score_gmm_gpu(x, means, stds, weights)\n",
        "\n",
        "        def density_func(x):\n",
        "            return density_gmm_gpu(x, means, stds, weights, mode='absolute')\n",
        "\n",
        "        def score_div_func(x):\n",
        "            _, d = score_div_gmm(x, means, stds, weights)\n",
        "            return d\n",
        "\n",
        "        params = (means, stds, weights)\n",
        "        return params, sampler, score_func, density_func, score_div_func\n",
        "\n",
        "    # ---------- 1) build intrinsic manifold in R^{m_embed} -----------------\n",
        "    if variant == \"helix\":\n",
        "        t = np.linspace(0, 4*np.pi, num_c)\n",
        "        r = 1.0 + 0.25 * np.sin(3*t)\n",
        "        xyz = np.stack([r*np.cos(t), r*np.sin(t), 0.4*t], axis=1)\n",
        "        X_intr = xyz; m_embed = 3\n",
        "\n",
        "    elif variant == \"concentric\":\n",
        "        shells    = 3\n",
        "        shell_id  = np.repeat(np.arange(shells),\n",
        "                              int(np.ceil(num_c / shells)))[:num_c]\n",
        "        radii     = 0.7 + 0.7 * shell_id\n",
        "        vecs      = rng.randn(num_c, m_dim)\n",
        "        vecs     /= np.linalg.norm(vecs, axis=1, keepdims=True)\n",
        "        X_intr    = radii[:, None] * vecs\n",
        "        m_embed   = m_dim\n",
        "\n",
        "    elif variant == \"sparse\":\n",
        "        xyz       = rng.uniform(-2.5, 2.5, size=(num_c, m_dim))\n",
        "        if m_dim >= 2:\n",
        "            xyz[:, 1] *= 0.3\n",
        "        if m_dim >= 3:\n",
        "            xyz[:, 2] *= 0.1\n",
        "        if m_dim > 3:\n",
        "            xyz[:, 3:] *= 0.05\n",
        "        X_intr = xyz; m_embed = m_dim\n",
        "\n",
        "    elif variant == \"knotted_torus\":\n",
        "        t = np.linspace(0, 2*np.pi, num_c)\n",
        "        r = 0.1\n",
        "        knot_x = (2 + np.cos(5*t)) * np.cos(3*t)\n",
        "        knot_y = (2 + np.cos(5*t)) * np.sin(3*t)\n",
        "        knot_z = np.sin(5*t)\n",
        "        base = np.stack([knot_x, knot_y, knot_z], axis=1)\n",
        "\n",
        "        tan = np.gradient(base, axis=0)\n",
        "        tan /= np.linalg.norm(tan, axis=1, keepdims=True) + 1e-12\n",
        "        rnd = rng.randn(num_c, 3)\n",
        "        n1 = rnd - (rnd * tan).sum(1, keepdims=True) * tan\n",
        "        n1 /= np.linalg.norm(n1, axis=1, keepdims=True) + 1e-12\n",
        "        n2 = np.cross(tan, n1)\n",
        "        n2 /= np.linalg.norm(n2, axis=1, keepdims=True) + 1e-12\n",
        "\n",
        "        theta  = rng.uniform(0, 2*np.pi, num_c)\n",
        "        tube_r = r * (0.7 + 0.3 * np.sin(6*t + 0.5))\n",
        "        xyz = base + tube_r[:, None] * (np.cos(theta)[:, None]*n1 + np.sin(theta)[:, None]*n2)\n",
        "        xyz += 0.05 * rng.randn(*xyz.shape) * (0.5 + 0.5 * np.sin(8*t)[:, None])\n",
        "        X_intr = xyz; m_embed = 3\n",
        "\n",
        "    elif variant == \"crossing_torus\":\n",
        "        # ---------------- knobs that amplify Tweedie/DSM failures ----------------\n",
        "        r        = 0.08          # tube radius (keep thin to keep strands distinct)\n",
        "        sig_t    = 0.10 * r      # along-tangent elongation (anisotropy ↑)\n",
        "        sig_n    = 0.02 * r      # normal-plane jitter (keep small)\n",
        "        pull_frac = 0.25         # how hard to snap near-crossing pairs together\n",
        "        keep_frac = 1/6          # fraction of points to snap (densify crossings)\n",
        "        p1, q1   = 5,  3         # knot 1 winding numbers\n",
        "        p2, q2   = 7, -3         # knot 2 (opposite toroidal winding -> transversality)\n",
        "        phase2   = np.pi / 7     # small phase shift -> many near-crossings\n",
        "        # -------------------------------------------------------------------------\n",
        "\n",
        "        def torus_knot(p, q, t):\n",
        "            x = (2.0 + np.cos(p * t)) * np.cos(q * t)\n",
        "            y = (2.0 + np.cos(p * t)) * np.sin(q * t)\n",
        "            z = np.sin(p * t)\n",
        "            return np.stack([x, y, z], axis=1)\n",
        "\n",
        "        def make_tube(base, t):\n",
        "            tan = np.gradient(base, axis=0)\n",
        "            tan /= np.linalg.norm(tan, axis=1, keepdims=True) + 1e-12\n",
        "\n",
        "            rnd = rng.randn(base.shape[0], 3)\n",
        "            n1 = rnd - (rnd * tan).sum(1, keepdims=True) * tan\n",
        "            n1 /= np.linalg.norm(n1, axis=1, keepdims=True) + 1e-12\n",
        "            n2 = np.cross(tan, n1)\n",
        "            n2 /= np.linalg.norm(n2, axis=1, keepdims=True) + 1e-12\n",
        "\n",
        "            theta  = rng.uniform(0, 2*np.pi, base.shape[0])\n",
        "            # slight modulation keeps visual texture but equalizes density overall\n",
        "            tube_r = r * (0.7 + 0.3 * np.cos(6 * t))\n",
        "\n",
        "            xyz = base + tube_r[:, None] * (\n",
        "                np.cos(theta)[:, None] * n1 + np.sin(theta)[:, None] * n2\n",
        "            )\n",
        "            # anisotropic jitter: long along tangent, tiny in normals\n",
        "            xyz += sig_t * rng.randn(base.shape[0], 1) * tan\n",
        "            xyz += sig_n * rng.randn(base.shape[0], 1) * n1\n",
        "            xyz += sig_n * rng.randn(base.shape[0], 1) * n2\n",
        "            return xyz\n",
        "\n",
        "        # split components between two strands\n",
        "        k  = num_c // 2\n",
        "        t1 = np.linspace(0, 2*np.pi, k, endpoint=False)\n",
        "        t2 = np.linspace(0, 2*np.pi, k, endpoint=False)\n",
        "\n",
        "        base1 = torus_knot(p1, q1, t1)\n",
        "        base2 = torus_knot(p2, q2, t2 + phase2)\n",
        "\n",
        "        xyz1 = make_tube(base1, t1)\n",
        "        xyz2 = make_tube(base2, t2)\n",
        "\n",
        "        # --- concentrate mass where strands approach: snap closest pairs together ---\n",
        "        # (This makes the posterior most bimodal -> Tweedie averaging worst.)\n",
        "        # For large k, this is ~k^2 memory; if needed, subsample before computing dmat.\n",
        "        dmat = np.linalg.norm(xyz1[:, None, :] - xyz2[None, :, :], axis=2)\n",
        "        i_close = np.argpartition(dmat.min(axis=1), int(k * keep_frac))[: int(k * keep_frac)]\n",
        "        j_close = np.argmin(dmat[i_close], axis=1)\n",
        "\n",
        "        mid = 0.5 * (xyz1[i_close] + xyz2[j_close])\n",
        "        xyz1[i_close] = mid + (1.0 - pull_frac) * (xyz1[i_close] - mid)\n",
        "        xyz2[j_close] = mid + (1.0 - pull_frac) * (xyz2[j_close] - mid)\n",
        "        # ----------------------------------------------------------------------------\n",
        "\n",
        "        xyz = np.concatenate([xyz1, xyz2], axis=0)\n",
        "\n",
        "        # if num_c is odd, pad one more point from strand 1\n",
        "        if xyz.shape[0] < num_c:\n",
        "            xyz = np.concatenate([xyz, make_tube(base1[:1], t1[:1])], axis=0)\n",
        "\n",
        "        X_intr  = xyz\n",
        "        m_embed = 3\n",
        "    else:\n",
        "        raise ValueError(\"unknown variant\")\n",
        "\n",
        "    # ---------- 2) embed into R^{k_dim} per embedding_mode -----------------\n",
        "    if embedding_mode == \"linear\":\n",
        "        means_np, covs = _embed_linear(X_intr)\n",
        "    elif embedding_mode == \"sine_wiggle\":\n",
        "        means_np, covs = _embed_sine_wiggle(X_intr, amp=0.25, freq=1.0)\n",
        "    elif embedding_mode == \"rff\":\n",
        "        means_np, covs = _embed_rff(X_intr, freq=1.0)\n",
        "    elif embedding_mode == \"radial\":\n",
        "        means_np, covs = _embed_radial(X_intr)\n",
        "    else:\n",
        "        raise ValueError(f\"unknown embedding_mode: {embedding_mode}\")\n",
        "\n",
        "    # ---------- 3) center & scale ------------------------------------------\n",
        "    means_np *= overall_scale / np.max(np.abs(means_np))\n",
        "    means  = xp.asarray(means_np, dtype=xp.float64)\n",
        "\n",
        "    # ---------- 4) stds/weights (respect variant-provided if any) ----------\n",
        "    if not stds_specified:\n",
        "        stds_np    = np.full(num_c, comp_std, dtype=float)\n",
        "    if not weights_specified:\n",
        "        weights_np = np.full(num_c, 1.0/num_c, dtype=float)\n",
        "\n",
        "    stds    = xp.asarray(stds_np, dtype=xp.float64)\n",
        "    weights = xp.asarray(weights_np, dtype=xp.float64)\n",
        "\n",
        "    # Optional normalization to a target size box\n",
        "    if normalize:\n",
        "        means, stds, weights = normalize_gmm_params(means, stds, weights)\n",
        "        means, stds = size * means, size * stds\n",
        "\n",
        "    # ---------- 5) public API: sampler/score/density -----------------------\n",
        "    def sampler(n, seed=None):\n",
        "        return sample_gmm_gpu(n, means, stds, weights, seed=seed)\n",
        "\n",
        "    def score_func(x):\n",
        "        return score_gmm_gpu(x, means, stds, weights)\n",
        "\n",
        "    def density_func(x):\n",
        "        return density_gmm_gpu(x, means, stds, weights, mode='absolute')\n",
        "\n",
        "    def score_div_func(x):\n",
        "        _, d = score_div_gmm(x, means, stds, weights)\n",
        "        return d\n",
        "\n",
        "    params = (means, stds, weights)  # covs computed above if you need it internally\n",
        "    return params, sampler, score_func, density_func, score_div_func\n",
        "\n",
        "\n",
        "def _rand_orth(d, k, seed=None):\n",
        "    rs = cp.random.RandomState(seed) if seed is not None else cp.random\n",
        "    G = rs.standard_normal((d, k), dtype=cp.float64)\n",
        "    # thin QR, make orthonormal columns\n",
        "    Q, _ = cp.linalg.qr(G, mode='reduced')\n",
        "    return Q  # d x k\n",
        "\n",
        "\n",
        "def embed_base(dist, d: int, *, sigma_perp=0.0, rotate=True, seed=None):\n",
        "    \"\"\"\n",
        "    dist: object with 2 callables\n",
        "        - dist['sample_k'](n, seed) -> cp.ndarray [n,k]\n",
        "        - dist['score_k'](Z)        -> cp.ndarray [n,k]\n",
        "    Returns (sampler_func, score_func) on R^d.\n",
        "\n",
        "    Embedding: x = Q_k z  +  Q_perp eps,  eps ~ N(0, sigma_perp^2 I_{d-k})\n",
        "    Score:     s(x) = Q_k s_z  +  Q_perp s_eps,  s_eps = -(1/sigma_perp^2) eps\n",
        "    \"\"\"\n",
        "    k = dist['sample_k'](1, seed=seed).shape[1]\n",
        "    assert 1 <= k <= d\n",
        "    if rotate:\n",
        "        Qk = _rand_orth(d, k, seed=seed)\n",
        "        # Build an orthonormal complement (cheap Gram-Schmidt on random)\n",
        "        Qp = _rand_orth(d, d, seed=(None if seed is None else seed+1))\n",
        "        # Re-orth so [Qk | Qperp] spans R^d with Qk preserved\n",
        "        # Householder trick: project Qp off Qk\n",
        "        Qp = Qp - Qk @ (Qk.T @ Qp)\n",
        "        # Orthonormalize columns and take first d-k\n",
        "        Qp, _ = cp.linalg.qr(Qp, mode='reduced')\n",
        "        Qp = Qp[:, :d-k] if d>k else cp.zeros((d,0), dtype=cp.float64)\n",
        "    else:\n",
        "        Qk = cp.pad(cp.eye(k, dtype=cp.float64), ((0, d-k),(0,0)))  # top k of I_d\n",
        "        Qp = cp.pad(cp.eye(d-k, dtype=cp.float64), ((k,0),(0,0)))\n",
        "\n",
        "    sigma_perp = float(sigma_perp)\n",
        "    inv_perp2  = 0.0 if sigma_perp==0.0 else 1.0/(sigma_perp**2)\n",
        "\n",
        "    def sampler_func(n, seed=None):\n",
        "        Z = dist['sample_k'](int(n), seed=seed)                  # [n,k]\n",
        "        if d == k:\n",
        "            return (Qk @ Z.T).T\n",
        "        rs = cp.random.RandomState(seed) if seed is not None else cp.random\n",
        "        eps = rs.standard_normal((int(n), d-k), dtype=cp.float64) * sigma_perp\n",
        "        X = (Qk @ Z.T).T + (Qp @ eps.T).T\n",
        "        return X\n",
        "\n",
        "    def score_func(X):\n",
        "        X = cp.asarray(X, dtype=cp.float64)\n",
        "        Z   = (Qk.T @ X.T).T                                     # [n,k]\n",
        "        s_k = dist['score_k'](Z)                                 # [n,k]\n",
        "        if d == k:\n",
        "            return (Qk @ s_k.T).T\n",
        "        eps = (Qp.T @ X.T).T                                     # [n,d-k]\n",
        "        s_perp = -inv_perp2 * eps\n",
        "        S = (Qk @ s_k.T).T + (Qp @ s_perp.T).T\n",
        "        return S\n",
        "\n",
        "    return sampler_func, score_func\n",
        "\n",
        "\n",
        "def make_banana_2d(beta=0.25, a=1.0, sigma1=1.0, sigma2=0.1):\n",
        "    def sample_k(n, seed=None):\n",
        "        rs = cp.random.RandomState(seed) if seed is not None else cp.random\n",
        "        z1 = rs.standard_normal(int(n)) * sigma1\n",
        "        u  = rs.standard_normal(int(n)) * sigma2\n",
        "        z2 = u - beta*(z1**2 - a)\n",
        "        return cp.stack([z1, z2], axis=1).astype(cp.float64)\n",
        "\n",
        "    def score_k(Z):\n",
        "        Z = cp.asarray(Z, dtype=cp.float64)\n",
        "        z1, z2 = Z[:,0], Z[:,1]\n",
        "        y = z2 + beta*(z1*z1 - a)\n",
        "        s1 = - z1/(sigma1**2) - (2*beta*z1*y)/(sigma2**2)\n",
        "        s2 = - y/(sigma2**2)\n",
        "        return cp.stack([s1, s2], axis=1)\n",
        "    return {'sample_k': sample_k, 'score_k': score_k}\n",
        "\n",
        "\n",
        "def make_ring_2d(R=3.0, sigma_r=0.1, with_jacobian=True):\n",
        "    def sample_k(n, seed=None):\n",
        "        rs = cp.random.RandomState(seed) if seed is not None else cp.random\n",
        "        theta = rs.uniform(0, 2*cp.pi, int(n))\n",
        "        r = R + rs.standard_normal(int(n)) * sigma_r\n",
        "        z = cp.stack([r*cp.cos(theta), r*cp.sin(theta)], axis=1)\n",
        "        return z.astype(cp.float64)\n",
        "\n",
        "    def score_k(Z):\n",
        "        Z = cp.asarray(Z, dtype=cp.float64)\n",
        "        r = cp.linalg.norm(Z, axis=1) + 1e-12\n",
        "        coef = -(r - R)/(sigma_r**2)\n",
        "        if with_jacobian:\n",
        "            coef = coef + 1.0/r\n",
        "        return (coef[:,None] * (Z / r[:,None])).astype(cp.float64)\n",
        "\n",
        "    return {'sample_k': sample_k, 'score_k': score_k}\n",
        "\n",
        "\n",
        "\n",
        "def _xp_random(seed=None, use_cupy=True):\n",
        "    if use_cupy and cp is not None:\n",
        "        if seed is not None: cp.random.seed(seed)\n",
        "        return cp\n",
        "    rng = np.random.RandomState(seed)\n",
        "    class NPShim:\n",
        "        random = rng\n",
        "        def asarray(self, x, dtype=None): return np.asarray(x, dtype=dtype)\n",
        "        def zeros(self, shape, dtype=None): return np.zeros(shape, dtype=dtype)\n",
        "        def ones(self, shape, dtype=None): return np.ones(shape, dtype=dtype)\n",
        "        def exp(self, x): return np.exp(x)\n",
        "        def sin(self, x): return np.sin(x)\n",
        "        def cos(self, x): return np.cos(x)\n",
        "        def dot(self, a,b): return a @ b\n",
        "        def maximum(self, a,b): return np.maximum(a,b)\n",
        "        def minimum(self, a,b): return np.minimum(a,b)\n",
        "        def sqrt(self, x): return np.sqrt(x)\n",
        "        def where(self, cond, a, b): return np.where(cond, a, b)\n",
        "        def eye(self, n): return np.eye(n)\n",
        "        def stack(self, arrs, axis=0): return np.stack(arrs, axis=axis)\n",
        "        def concatenate(self, arrs, axis=0): return np.concatenate(arrs, axis=axis)\n",
        "        def norm(self, x, axis=None): return np.linalg.norm(x, axis=axis)\n",
        "    return NPShim()\n",
        "\n",
        "def make_corrugated_gaussian(k=6, r=None, omega=80.0, c=0.6, use_cupy=True, seed=0):\n",
        "    \"\"\"\n",
        "    Toy distribution in R^k with 'r' sinusoidal corrugations along orthonormal directions.\n",
        "    Returns a dict with 'sample_k' and 'score_k' just like the banana/ring toys.\n",
        "\n",
        "    log p(x) = -1/2 ||x||^2  - (c/omega) * sum_{j=1}^r sin(omega v_j^T x)\n",
        "    ⇒ score(x) = -x - c * sum_j cos(omega v_j^T x) v_j\n",
        "    The likelihood ratio to N(0,I) is bounded by exp(± r*c/omega) (so no huge spikes if r*c/omega is modest).\n",
        "    \"\"\"\n",
        "    # pick backend RNG/array module (your helper keeps cp when available)\n",
        "    xp = _xp_random(seed=seed, use_cupy=use_cupy)\n",
        "    if r is None:\n",
        "        r = min(3, k)  # default: a few ripples but <= k\n",
        "    assert 1 <= r <= k, \"require 1 <= r <= k\"\n",
        "\n",
        "    # Build V ∈ R^{k×r} with orthonormal columns in the SAME array library\n",
        "    # Generate Gaussian (k×r), QR -> V\n",
        "    if xp is cp:\n",
        "        W = xp.random.randn(k, r)\n",
        "        Q, _ = xp.linalg.qr(W, mode='reduced')   # (k,r)\n",
        "        V = Q\n",
        "    else:\n",
        "        W = np.random.RandomState(seed).randn(k, r)\n",
        "        Q, _ = np.linalg.qr(W)                   # (k,r)\n",
        "        V = xp.asarray(Q)\n",
        "\n",
        "    omega = float(omega); c = float(c)\n",
        "    rho = c / omega   # appears in the target potential, controls closeness to Gaussian\n",
        "\n",
        "    def _proj(X):  # (n,k) -> (n,r)\n",
        "        return X @ V\n",
        "\n",
        "    # exact score in R^k\n",
        "    def score_k(Z):\n",
        "        Z = xp.asarray(Z, dtype=xp.float64)\n",
        "        U = _proj(Z)                              # (n,r)\n",
        "        C = xp.cos(omega * U)                     # (n,r)\n",
        "        return (-Z - (c * C) @ V.T).astype(xp.float64)\n",
        "\n",
        "    # rejection sample from N(0,I) with envelope exp(r * rho)\n",
        "    # acceptance ≥ exp(-r*rho) and no host/device copying\n",
        "    def sample_k(n, seed=None, batch=8192):\n",
        "        xp_loc = _xp_random(seed=seed, use_cupy=use_cupy)\n",
        "        out = []\n",
        "        need = int(n)\n",
        "        logC = r * rho\n",
        "        while need > 0:\n",
        "            m = max(batch, need)\n",
        "            Z = xp_loc.random.randn(m, k).astype(xp.float64)\n",
        "            U = Z @ V                              # (m,r)\n",
        "            w = xp_loc.exp(-rho * xp_loc.sin(omega * U)).prod(axis=1)\n",
        "            acc = w / xp_loc.exp(logC)\n",
        "            u = xp_loc.random.rand(m)\n",
        "            keep = u < acc\n",
        "            if keep.sum() > 0:\n",
        "                out.append(Z[keep])\n",
        "                need -= int(keep.sum())\n",
        "        return xp.concatenate(out, axis=0)[:n]\n",
        "\n",
        "    return {'sample_k': sample_k, 'score_k': score_k}\n",
        "\n",
        "\n",
        "\n",
        "def make_funnel(m):\n",
        "    d = m + 1\n",
        "    def sample_k(n, seed=None):\n",
        "        rs = cp.random.RandomState(seed) if seed is not None else cp.random\n",
        "        z  = rs.standard_normal(int(n))\n",
        "        x  = rs.standard_normal((int(n), m)) * cp.exp(z)[:,None]\n",
        "        return cp.concatenate([x, z[:,None]], axis=1).astype(cp.float64)\n",
        "\n",
        "    def score_k(Z):\n",
        "        Z = cp.asarray(Z, dtype=cp.float64)\n",
        "        x, z = Z[:, :m], Z[:, m]\n",
        "        ezm2 = cp.exp(-2.0*z)\n",
        "        s_x  = - (ezm2[:,None] * x)\n",
        "        s_z  = - z + ezm2 * cp.sum(x*x, axis=1) - m\n",
        "        return cp.concatenate([s_x, s_z[:,None]], axis=1)\n",
        "    return {'sample_k': sample_k, 'score_k': score_k}\n",
        "\n",
        "\n",
        "\n",
        "def make_student_t(k, nu=5.0, Sigma=None, seed=None):\n",
        "    rs = cp.random.RandomState(seed) if seed is not None else cp.random\n",
        "    if Sigma is None:\n",
        "        # strong anisotropy\n",
        "        scales = cp.asarray(cp.logspace(np.log10(0.1), np.log10(5.0), num=k), dtype=cp.float64)\n",
        "        L = cp.diag(scales)\n",
        "        Sinv = cp.diag(1.0/(scales**2))\n",
        "    else:\n",
        "        S = cp.asarray(Sigma, dtype=cp.float64)\n",
        "        L = cp.linalg.cholesky(S)\n",
        "        Sinv = cp.linalg.inv(S)\n",
        "\n",
        "    def sample_k(n, seed=None):\n",
        "        rng = cp.random.RandomState(seed) if seed is not None else cp.random\n",
        "        eps = rng.standard_normal((int(n), k), dtype=cp.float64)\n",
        "        g   = rng.chisquare(df=nu, size=int(n)).astype(cp.float64)\n",
        "        x   = (eps @ L.T) / cp.sqrt(g/nu)[:,None]\n",
        "        return x\n",
        "\n",
        "    def score_k(Z):\n",
        "        Z = cp.asarray(Z, dtype=cp.float64)\n",
        "        Mah = cp.einsum('nd,dd,nd->n', Z, Sinv, Z)           # x^T S^{-1} x\n",
        "        coef = - (nu + k) / (nu + Mah)\n",
        "        return coef[:,None] * (Z @ Sinv.T)\n",
        "    return {'sample_k': sample_k, 'score_k': score_k}\n",
        "\n",
        "\n",
        "\n",
        "def make_k_rotated_banana(K=6, beta=0.25, a=1.0, sigma1=1.0, sigma2=0.1):\n",
        "    base = make_banana_2d(beta, a, sigma1, sigma2)\n",
        "    thetas = cp.linspace(0, 2*cp.pi, K, endpoint=False)\n",
        "    Rks = cp.stack([cp.stack([cp.cos(t), -cp.sin(t)], axis=0)\n",
        "                    for t in thetas], axis=0)  # [K,2]\n",
        "    Rks = cp.stack([cp.stack([Rks[i], cp.stack([cp.sin(thetas[i]), cp.cos(thetas[i])])],axis=0)\n",
        "                    for i in range(K)], axis=0)  # [K,2,2]\n",
        "\n",
        "    def sample_k(n, seed=None):\n",
        "        rs = cp.random.RandomState(seed) if seed is not None else cp.random\n",
        "        idx = rs.randint(0, K, size=int(n))\n",
        "        Z   = base['sample_k'](int(n), seed=None)\n",
        "        # rotate each sample by chosen R_k\n",
        "        X = cp.einsum('nij,nj->ni', Rks[idx], Z)\n",
        "        return X\n",
        "\n",
        "    def _logp_k(Z):\n",
        "        # compute log p_k for each k by un-rotating into banana frame\n",
        "        # log p up to constant from base form\n",
        "        Z = cp.asarray(Z, dtype=cp.float64)\n",
        "        Y = cp.einsum('kij,nj->nki', Rks.transpose(0,2,1), Z)  # [n,K,2]\n",
        "        z1, z2 = Y[...,0], Y[...,1]\n",
        "        y = z2 + beta*(z1*z1 - a)\n",
        "        lp = -0.5*(z1*z1/(sigma1**2) + y*y/(sigma2**2))  # [n,K]\n",
        "        return lp\n",
        "\n",
        "    def score_k(Z):\n",
        "        Z = cp.asarray(Z, dtype=cp.float64)\n",
        "        lp = _logp_k(Z)                       # [n,K]\n",
        "        m  = lp.max(axis=1, keepdims=True)\n",
        "        W  = cp.exp(lp - m); W /= W.sum(1, keepdims=True) + 1e-30\n",
        "        # component scores in x-space: rotate base score\n",
        "        Y = cp.einsum('kij,nj->nki', Rks.transpose(0,2,1), Z)         # un-rotate\n",
        "        sb = base['score_k'](Y.reshape(-1,2)).reshape(Z.shape[0], K, 2)\n",
        "        sx = cp.einsum('kij,nkj->nki', Rks, sb)                       # rotate back\n",
        "        s  = cp.einsum('nk,nki->ni', W, sx)                           # mixture score\n",
        "        return s\n",
        "\n",
        "    return {'sample_k': sample_k, 'score_k': score_k}\n",
        "\n",
        "\n",
        "\n",
        "def make_folded_2d(c=1.0):\n",
        "    def sample_k(n, seed=None):\n",
        "        rs = cp.random.RandomState(seed) if seed is not None else cp.random\n",
        "        u1 = rs.standard_normal(int(n))\n",
        "        u2 = rs.standard_normal(int(n))\n",
        "        z1 = u1\n",
        "        z2 = u2 + c*cp.abs(u1)\n",
        "        return cp.stack([z1, z2], axis=1).astype(cp.float64)\n",
        "\n",
        "    def score_k(Z):\n",
        "        Z = cp.asarray(Z, dtype=cp.float64)\n",
        "        z1, z2 = Z[:,0], Z[:,1]\n",
        "        y = z2 - c*cp.abs(z1)\n",
        "        s2 = -y\n",
        "        s1 = -z1 - y*c*cp.sign(z1)\n",
        "        return cp.stack([s1, s2], axis=1)\n",
        "    return {'sample_k': sample_k, 'score_k': score_k}\n",
        "\n",
        "\n",
        "\n",
        "def get_spiky_gmm_funcs(\n",
        "    num_c=200, k_dim=6, base_std=0.15, spike_std=0.001, spike_frac=0.1, seed=0\n",
        "):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    C_spike = max(1, int(num_c * spike_frac))\n",
        "    C_base  = num_c - C_spike\n",
        "\n",
        "    means_base = rng.randn(C_base, k_dim)\n",
        "    means_spk  = rng.randn(C_spike, k_dim)*0.5 + 3.0*rng.choice([-1,1], size=(C_spike,k_dim))\n",
        "    means = np.concatenate([means_base, means_spk], 0)\n",
        "\n",
        "    stds  = np.concatenate([np.full(C_base, base_std), np.full(C_spike, spike_std)])\n",
        "    w     = np.full(num_c, 1.0/num_c)\n",
        "\n",
        "    means = cp.asarray(means, dtype=cp.float64)\n",
        "    stds  = cp.asarray(stds,  dtype=cp.float64)\n",
        "    w     = cp.asarray(w,     dtype=cp.float64)\n",
        "\n",
        "    def sampler(n, seed=None):\n",
        "        return sample_gmm_gpu(n, means, stds, w, seed=seed)\n",
        "    def score(x): return score_gmm_gpu(x, means, stds, w)\n",
        "    return sampler, score\n",
        "\n",
        "\n",
        "def make_pathological_gmm(\n",
        "    C=500, d=39, sep=5.0, cov_scale=0.05, seed=0):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    # place means on a d‐dim hypercube corners or random large coordinates\n",
        "    means = rng.randn(C, d) * sep\n",
        "    # tiny isotropic covariances\n",
        "    stds = np.full(C, cov_scale)\n",
        "    # nonuniform weights\n",
        "    weights = rng.rand(C)\n",
        "    weights /= weights.sum()\n",
        "    # wrap for existing API\n",
        "    params = (means, stds, weights)\n",
        "    sampler, score = get_gmm_funcs(\n",
        "        num_c=C, k_dim=d, variant='custom', comp_std=cov_scale,\n",
        "        overall_scale=sep, rs=rng)\n",
        "    return params, sampler, score\n",
        "\n",
        "\n",
        "def resample_by_alpha(X, alpha, N, return_indices=False, rng=None):\n",
        "    xp = np\n",
        "    is_cupy = type(X).__module__.split('.')[0] == 'cupy' or type(alpha).__module__.split('.')[0] == 'cupy'\n",
        "    if is_cupy:\n",
        "        import cupy as cp\n",
        "        xp = cp\n",
        "\n",
        "    X = xp.asarray(X)\n",
        "    alpha = xp.asarray(alpha).reshape(-1)\n",
        "    M = alpha.size\n",
        "    if X.shape[0] != M:\n",
        "        raise ValueError(f\"Length mismatch: len(alpha)={M} but X.shape[0]={X.shape[0]}\")\n",
        "\n",
        "    # Normalize weights safely\n",
        "    w = xp.clip(alpha, 0, xp.inf).astype(xp.float64)\n",
        "    w_sum = w.sum()\n",
        "    if not xp.isfinite(w_sum) or w_sum <= 0:\n",
        "        # Fallback: uniform sampling if weights are invalid or all zero\n",
        "        p = xp.full(M, 1.0 / M, dtype=xp.float64)\n",
        "    else:\n",
        "        p = w / w_sum\n",
        "\n",
        "    # Draw indices\n",
        "    if rng is None:\n",
        "        idx = xp.random.choice(M, size=int(N), replace=True, p=p)\n",
        "    else:\n",
        "        # rng must have a .choice method (NumPy/CuPy Generator)\n",
        "        idx = rng.choice(M, size=int(N), replace=True, p=p)\n",
        "\n",
        "    Y = X[idx]\n",
        "    return (Y, idx) if return_indices else Y\n",
        "\n",
        "# --- Rank-k linear likelihood on GPU (CuPy) ---\n",
        "\n",
        "\n",
        "\n",
        "def _orthonormal_rows_A(D, k, seed=None):\n",
        "    \"\"\"\n",
        "    Build A with k orthonormal rows in R^{k x D}.\n",
        "    Uses QR on a (D x k) Gaussian and transposes.\n",
        "    \"\"\"\n",
        "    rs = cp.random.RandomState(int(seed)) if (seed is not None and seed != 'rand') else cp.random\n",
        "    G = rs.standard_normal((D, k), dtype=cp.float64)      # (D,k)\n",
        "    Q, _ = cp.linalg.qr(G, mode='reduced')                # (D,k) with orthonormal columns\n",
        "    A = Q.T                                               # (k,D) with orthonormal rows\n",
        "    return A.astype(cp.float64, copy=False)\n",
        "\n",
        "def make_rank_k_likelihood(\n",
        "    D, rank_k, obs_sigma,\n",
        "    sampler_func=None,            # optional: to synthesize y as A x* + eps\n",
        "    y_obs=None,                   # optional: provide your own observed y in R^k\n",
        "    seed=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Build rank-k linear Gaussian likelihood:\n",
        "        log p(y | x) = - (1/(2 sigma^2)) || y - A x ||^2\n",
        "        grad_x log p  = (1/sigma^2) A^T ( y - A x )\n",
        "\n",
        "    Returns:\n",
        "      A              : (k, D) CuPy array\n",
        "      y_obs_k        : (k,)   CuPy array\n",
        "      likelyhood_func: (N,D) -> (N,)  log-lik up to additive const\n",
        "      log_likelyhood_func: alias to likelyhood_func\n",
        "      loglik_grad_fn : (N,D) -> (N,D)\n",
        "    \"\"\"\n",
        "    k = int(rank_k)\n",
        "    assert 1 <= k <= D, f\"rank_k must be in [1, {D}]\"\n",
        "    A = _orthonormal_rows_A(D, k, seed=seed)             # (k,D)\n",
        "    inv_var = cp.asarray(1.0 / (obs_sigma * obs_sigma), dtype=cp.float64)\n",
        "    alpha = 0.5 * inv_var                                 # = 1/(2 sigma^2)\n",
        "\n",
        "    if y_obs is None:\n",
        "        if sampler_func is None:\n",
        "            raise ValueError(\"Provide sampler_func to synthesize y_obs or pass y_obs directly.\")\n",
        "        x_star = sampler_func(1, seed=seed).reshape(-1)   # (D,)\n",
        "        eps = cp.random.normal(0.0, obs_sigma, size=(k,)).astype(cp.float64)\n",
        "        y_obs_k = (A @ x_star.astype(cp.float64)) + eps   # (k,)\n",
        "    else:\n",
        "        y_obs_k = cp.asarray(y_obs, dtype=cp.float64).reshape(k)\n",
        "        # if user passed a D-dim y, project it:\n",
        "        if y_obs_k.size == D:\n",
        "            y_obs_k = (A @ y_obs_k)\n",
        "\n",
        "    def likelyhood_func(x_ref):\n",
        "        \"\"\"\n",
        "        x_ref: (N,D) -> returns log-likelihood up to constant: -(1/(2σ^2)) ||y - A x||^2\n",
        "        \"\"\"\n",
        "        X = cp.asarray(x_ref, dtype=cp.float64)\n",
        "        Ax = X @ A.T                                      # (N,k)\n",
        "        resid = y_obs_k[None, :] - Ax                     # (N,k)\n",
        "        return -alpha * cp.sum(resid * resid, axis=1)     # (N,)\n",
        "\n",
        "    def loglik_grad_fn(x_ref):\n",
        "        \"\"\"\n",
        "        x_ref: (N,D) -> gradient wrt x: (1/σ^2) A^T (y - A x)\n",
        "        \"\"\"\n",
        "        X = cp.asarray(x_ref, dtype=cp.float64)\n",
        "        Ax = X @ A.T                                      # (N,k)\n",
        "        resid = y_obs_k[None, :] - Ax                     # (N,k)\n",
        "        return (inv_var * (resid @ A))                    # (N,D)\n",
        "\n",
        "    # keep your API surface:\n",
        "    def log_likelyhood_func(x_ref):\n",
        "        # already a log-likelihood; return it directly (do NOT take cp.log again)\n",
        "        return likelyhood_func(x_ref)\n",
        "\n",
        "    return A, y_obs_k, likelyhood_func, log_likelyhood_func, loglik_grad_fn\n",
        "\n",
        "\n",
        "def calculate_true_score_at_t(y, t, means0, stds0, w0, batch_size):\n",
        "    m, s, w = get_ou_evolved_gmm_params(t, means0, stds0, w0)\n",
        "    return score_gmm_gpu(y, m, s, w, batch_size)\n",
        "\n",
        "\n",
        "def ou_logweights_with_lik(x, t, x_ref, loglik_ref=None):\n",
        "    \"\"\"\n",
        "    x:      [B, d] current particles (y_t in your notation)\n",
        "    x_ref:  [N, d] reference particles ~ p(x0)\n",
        "    t:      float\n",
        "    loglik_ref: [N] log p(y | x_ref) (any constant offset OK)\n",
        "\n",
        "    returns: normalized weights wbar [B, N], plus cached scalars\n",
        "    \"\"\"\n",
        "    xp = cp\n",
        "    et  = xp.exp(-t)\n",
        "    sig2 = 1.0 - xp.exp(-2*t)             # sigma_t^2\n",
        "\n",
        "    # pairwise squared distances ||x - e^{-t} x_i||^2\n",
        "    diff = x[:, None, :] - et * x_ref[None, :, :]\n",
        "    sq   = xp.sum(diff*diff, axis=-1)     # [B, N]\n",
        "\n",
        "    logw = -0.5 * sq / sig2               # OU kernel (const cancels)\n",
        "    if loglik_ref is not None:\n",
        "        logw = logw + loglik_ref[None, :] # add the likelihood term\n",
        "\n",
        "    # stabilize & normalize\n",
        "    m = xp.max(logw, axis=1, keepdims=True)\n",
        "    w = xp.exp(logw - m)\n",
        "    wbar = w / xp.sum(w, axis=1, keepdims=True)  # [B, N]\n",
        "    return wbar, et, sig2\n",
        "\n",
        "\n",
        "def gaussian_obs_loglik(x_ref, y, alpha):\n",
        "    diff = x_ref - y[None, :]\n",
        "    return -alpha * cp.sum(diff*diff, axis=1)     # [N]\n",
        "\n",
        "\n",
        "# --- helper (drop near your other utils) ------------------------------------\n",
        "def _resolve_loglik(x_ref, xp, loglik_ref=None, loglik_fn=None):\n",
        "    \"\"\"Return xp.ndarray of shape [N] with per-ref log-likelihoods, or None.\"\"\"\n",
        "    if loglik_fn is not None:\n",
        "        ll = loglik_fn(x_ref)\n",
        "    else:\n",
        "        ll = loglik_ref\n",
        "    if ll is None:\n",
        "        return None\n",
        "    ll = xp.asarray(ll).reshape(-1)\n",
        "    assert ll.shape[0] == x_ref.shape[0], \"loglik_ref must have shape [N]\"\n",
        "    return ll\n",
        "\n",
        "\n",
        "def kss_score_t(y_batch, t, x_ref, s_ref,\n",
        "                loglik_ref=None, loglik_fn=None,\n",
        "                w_correct=1.0,\n",
        "                grad_loglik_ref=None, grad_loglik_fn=None):\n",
        "    xp = cp\n",
        "    ll = _resolve_loglik(x_ref, xp, loglik_ref, loglik_fn)\n",
        "\n",
        "    # build posterior-at-0 reference score if gradient supplied\n",
        "    if grad_loglik_ref is None and grad_loglik_fn is not None:\n",
        "        grad_loglik_ref = grad_loglik_fn(x_ref)           # [N,d]\n",
        "\n",
        "    s_ref_eff = s_ref if grad_loglik_ref is None else (s_ref + grad_loglik_ref)\n",
        "\n",
        "    var = 1.0 - xp.exp(-2.0*t); inv_v = 1.0/var; et = xp.exp(-t)\n",
        "    diff   = y_batch[:, None, :] - et * x_ref[None, :, :]\n",
        "    logits = -0.5 * inv_v * xp.sum(diff**2, axis=-1)\n",
        "    if ll is not None:\n",
        "        logits = logits + ll[None, :]\n",
        "\n",
        "    m = xp.max(logits, axis=1, keepdims=True)\n",
        "    w = xp.exp(logits - m)\n",
        "    w *= w_correct\n",
        "    w = w / (xp.sum(w, axis=1, keepdims=True) + 1e-30)\n",
        "\n",
        "    return xp.exp(t) * (w @ s_ref_eff)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def tweedie_score_t(y, t, x_ref, loglik_ref=None, loglik_fn=None , w_correct = 1.0):\n",
        "    xp = cp\n",
        "    if t < 1e-12:\n",
        "        raise ValueError(\"t must be >0 for Tweedie estimator.\")\n",
        "    ll = _resolve_loglik(x_ref, xp, loglik_ref, loglik_fn)\n",
        "\n",
        "    B, D = y.shape\n",
        "    var   = 1.0 - xp.exp(-2.0 * t)\n",
        "    inv_v = 1.0 / var\n",
        "    et    = xp.exp(-t)\n",
        "\n",
        "    mu_i  = et * x_ref                                  # (N,D)\n",
        "    diff  = y[:, None, :] - mu_i[None, :, :]            # (B,N,D)\n",
        "\n",
        "    logits = -0.5 * inv_v * xp.sum(diff**2, axis=-1)    # (B,N)\n",
        "    if ll is not None:\n",
        "        logits = logits + ll[None, :]                   # add log-lik\n",
        "\n",
        "    m = xp.max(logits, axis=1, keepdims=True)\n",
        "\n",
        "    w = xp.exp(logits - m)\n",
        "    w *= w_correct\n",
        "    w = w / (xp.sum(w, axis=1, keepdims=True) + 1e-30)  # (B,N)\n",
        "\n",
        "\n",
        "    score = - xp.einsum('bn, bnd -> bd', w, diff) * inv_v\n",
        "    return score\n",
        "\n",
        "def proxy_tweedie_score_t(y, t, mu_ref, loglik_ref=None, loglik_fn=None, w_correct=1.0):\n",
        "    \"\"\"\n",
        "    Tweedie NP score but centered on proxy means mu_i (from local Gaussian fits),\n",
        "    i.e. weights and conditional mean are computed w.r.t. e^{-t} mu_i.\n",
        "    \"\"\"\n",
        "    xp = cp\n",
        "    if t < 1e-12:\n",
        "        raise ValueError(\"t must be >0 for Tweedie estimator.\")\n",
        "    Z = xp.asarray(mu_ref, dtype=xp.float64)  # basis = mu_i\n",
        "\n",
        "    # optional likelihood values evaluated at mu_i\n",
        "    if loglik_fn is not None:\n",
        "        ll = loglik_fn(Z)\n",
        "    else:\n",
        "        ll = loglik_ref\n",
        "    if ll is not None:\n",
        "        ll = xp.asarray(ll).reshape(-1)\n",
        "\n",
        "    B, D = y.shape\n",
        "    var   = 1.0 - xp.exp(-2.0 * t)\n",
        "    inv_v = 1.0 / var\n",
        "    et    = xp.exp(-t)\n",
        "\n",
        "    mu_i_t = et * Z                                      # (N,D)\n",
        "    diff   = y[:, None, :] - mu_i_t[None, :, :]          # (B,N,D)\n",
        "    logits = -0.5 * inv_v * xp.sum(diff**2, axis=-1)     # (B,N)\n",
        "    if ll is not None:\n",
        "        logits = logits + ll[None, :]\n",
        "\n",
        "    m = xp.max(logits, axis=1, keepdims=True)\n",
        "    w = xp.exp(logits - m)\n",
        "    w *= w_correct\n",
        "    w = w / (xp.sum(w, axis=1, keepdims=True) + 1e-30)   # (B,N)\n",
        "\n",
        "    score = - xp.einsum('bn,bnd->bd', w, diff) * inv_v\n",
        "    return score\n",
        "\n",
        "\n",
        "def snis_blend(y_batch, x_ref, s0_ref, t, *,\n",
        "               xp=None, eps=1e-12, return_details=False,\n",
        "               loglik_ref=None, loglik_fn=None, w_correct=1.0,\n",
        "               grad_loglik_ref=None, grad_loglik_fn=None,\n",
        "               twd_basis=None):   # <-- NEW\n",
        "    if xp is None:\n",
        "        try: import cupy as _cp; xp = _cp\n",
        "        except: import numpy as _np; xp = _np\n",
        "\n",
        "    # Use proxy basis for Tweedie if provided; else fall back to x_ref\n",
        "    Z_ref = x_ref if twd_basis is None else xp.asarray(twd_basis, dtype=xp.float64)\n",
        "\n",
        "    # likelihood for the basis used in Tweedie weights\n",
        "    ll = _resolve_loglik(Z_ref, xp, loglik_ref, loglik_fn)\n",
        "\n",
        "    if grad_loglik_ref is None and grad_loglik_fn is not None:\n",
        "        grad_loglik_ref = grad_loglik_fn(x_ref)  # still eval grad at true anchors\n",
        "\n",
        "    s_ref_eff = s0_ref if grad_loglik_ref is None else (s0_ref + grad_loglik_ref)\n",
        "\n",
        "    B, d = y_batch.shape\n",
        "    var  = xp.maximum(1.0 - xp.exp(-2.0*t), eps)\n",
        "    inv_v = 1.0/var\n",
        "    et    = xp.exp(-t)\n",
        "\n",
        "    # weights built against Z_ref (proxy means if provided)\n",
        "    diff   = y_batch[:, None, :] - et * Z_ref[None, :, :]\n",
        "    logits = -0.5 * inv_v * xp.sum(diff*diff, axis=-1)\n",
        "    if ll is not None:\n",
        "        logits = logits + ll[None, :]\n",
        "\n",
        "    m  = xp.max(logits, axis=1, keepdims=True)\n",
        "    w  = xp.exp(logits - m)\n",
        "    w *= w_correct\n",
        "    w /= xp.sum(w, axis=1, keepdims=True) + eps\n",
        "\n",
        "    w2 = w * w\n",
        "    S0 = xp.sum(w2, axis=1)\n",
        "\n",
        "    # scores (KSS uses same weights w; Tweedie uses Z_ref)\n",
        "    s_kss = xp.exp(t) * (w @ s_ref_eff)\n",
        "    mu_x  = w @ Z_ref\n",
        "    s_twd = -(inv_v) * (y_batch - et * mu_x)\n",
        "\n",
        "    den_sn = xp.maximum(1.0 - S0, eps)\n",
        "\n",
        "    # --- Vk ---\n",
        "    a_i = xp.exp(t) * s_ref_eff\n",
        "    a_norm2 = xp.sum(a_i * a_i, axis=1)\n",
        "    S1a = w2 @ a_norm2\n",
        "    S2a = w2 @ a_i\n",
        "    mu_a = s_kss\n",
        "    mu_a_norm2 = xp.sum(mu_a * mu_a, axis=1)\n",
        "    num_Vk = S1a - 2.0 * xp.sum(mu_a * S2a, axis=1) + mu_a_norm2 * S0\n",
        "    Vk = num_Vk / den_sn\n",
        "\n",
        "    # --- Vt (now uses Z_ref everywhere) ---\n",
        "    y_norm2 = xp.sum(y_batch*y_batch, axis=1)\n",
        "    z_norm2 = xp.sum(Z_ref*Z_ref, axis=1)\n",
        "    w2_z    = w2 @ Z_ref\n",
        "    y_dot_zW = xp.sum(y_batch * w2_z, axis=1)\n",
        "    S1b = (y_norm2 * S0 - 2.0 * et * y_dot_zW + (et**2) * (w2 @ z_norm2)) * (inv_v**2)\n",
        "    S2b = -(inv_v) * (y_batch * S0[:, None] - et * w2_z)\n",
        "    mu_b = s_twd\n",
        "    mu_b_norm2 = xp.sum(mu_b * mu_b, axis=1)\n",
        "    num_Vt = S1b - 2.0 * xp.sum(mu_b * S2b, axis=1) + mu_b_norm2 * S0\n",
        "    Vt = num_Vt / den_sn\n",
        "\n",
        "    # --- C (cross, use Z_ref in the a⋅z term) ---\n",
        "    a_dot_z = xp.sum(a_i * Z_ref, axis=1)      # [N]\n",
        "    Wa = w2 @ a_i\n",
        "    term1 = xp.sum(Wa * y_batch, axis=1)\n",
        "    term2 = w2 @ a_dot_z\n",
        "    Sab = -(inv_v) * (term1 - et * term2)\n",
        "    num_C = (Sab\n",
        "             - xp.sum(mu_a * S2b, axis=1)\n",
        "             - xp.sum(mu_b * S2a, axis=1)\n",
        "             + xp.sum(mu_a * mu_b, axis=1) * S0)\n",
        "    C = num_C / den_sn\n",
        "\n",
        "    denom = xp.maximum(Vk + Vt - 2.0 * C, eps)\n",
        "    lam = (Vk - C) / denom\n",
        "    lam = xp.clip(lam, 0.0, 1.0)\n",
        "    lam = xp.nan_to_num(lam, nan=0.5, posinf=1.0, neginf=0.0)\n",
        "\n",
        "    if return_details:\n",
        "        ess = 1.0 / (S0 + eps)\n",
        "        return lam, dict(w=w, ess=ess, s_kss=s_kss, s_twd=s_twd, Vk=Vk, Vt=Vt, C=C)\n",
        "    return lam\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "def s_blend(x, t, x_ref, s_ref, loglik_ref=None, loglik_fn=None,\n",
        "            loglik_grad_fn = None, w_correct = 1.0):\n",
        "    s_tweedie = tweedie_score_t(x, t, x_ref,\n",
        "        loglik_ref=loglik_ref, loglik_fn=loglik_fn, w_correct=w_correct)\n",
        "    s_kss = kss_score_t(x, t, x_ref, s_ref, grad_loglik_fn = loglik_grad_fn,\n",
        "        loglik_ref=loglik_ref, loglik_fn=loglik_fn, w_correct=w_correct)\n",
        "    wt = snis_blend(x, x_ref, s_ref, t, grad_loglik_fn = loglik_grad_fn,\n",
        "        loglik_ref=loglik_ref, w_correct=w_correct)\n",
        "    return (1.0 - wt)[:, None] * s_kss + wt[:, None] * s_tweedie\n",
        "'''\n",
        "\n",
        "\n",
        "def s_blend(x, t, x_ref, s_ref, loglik_ref=None, loglik_fn=None,\n",
        "            loglik_grad_fn=None, w_correct=1.0,\n",
        "            twd_basis=None):   # <-- NEW (None = use x_ref; array = use mu_i)\n",
        "    if twd_basis is None:\n",
        "        # original path (real Tweedie + KSS)\n",
        "        s_tweedie = tweedie_score_t(x, t, x_ref,\n",
        "            loglik_ref=loglik_ref, loglik_fn=loglik_fn, w_correct=w_correct)\n",
        "        s_kss = kss_score_t(x, t, x_ref, s_ref, grad_loglik_fn=loglik_grad_fn,\n",
        "            loglik_ref=loglik_ref, loglik_fn=loglik_fn, w_correct=w_correct)\n",
        "        wt = snis_blend(x, x_ref, s_ref, t, grad_loglik_fn=loglik_grad_fn,\n",
        "            loglik_ref=loglik_ref, loglik_fn=loglik_fn, w_correct=w_correct)\n",
        "        return (1.0 - wt)[:, None] * s_kss + wt[:, None] * s_tweedie\n",
        "    else:\n",
        "        # proxy path: get λ, s_kss, s_twd consistently from the same weights\n",
        "        lam, det = snis_blend(x, x_ref, s_ref, t, return_details=True,\n",
        "                              grad_loglik_fn=loglik_grad_fn,\n",
        "                              loglik_ref=loglik_ref, loglik_fn=loglik_fn,\n",
        "                              w_correct=w_correct, twd_basis=twd_basis)\n",
        "        s_kss = det['s_kss']\n",
        "        s_twd = det['s_twd']\n",
        "        return (1.0 - lam)[:, None] * s_kss + lam[:, None] * s_twd\n",
        "\n",
        "\n",
        "\n",
        "def gmm_posterior_params(\n",
        "    y, pis, mus, Sigmas, *, A=None, b=None, R=None, alpha=None, out=\"full\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Prior:\n",
        "        x ~ sum_k pi_k N(mu_k, Σ_k)\n",
        "        Σ_k may be isotropic: Sigmas.shape == (K,)\n",
        "              or diagonal :   Sigmas.shape == (K, D)\n",
        "\n",
        "    Likelihood:\n",
        "        y ~ N(A x + b, R)     (general case)\n",
        "        or, if A,b,R omitted and alpha given: identity with sigma^2 = 1/(2*alpha).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    if out == \"full\":\n",
        "        w_post, means_post, covs_post        # covs_post: (K, D, D)\n",
        "    elif out == \"diag\":\n",
        "        w_post, means_post, stds_post        # stds_post: (K, D) (sqrt of diag cov)\n",
        "    \"\"\"\n",
        "    # --- backend ---\n",
        "    try:\n",
        "        import cupy as cp\n",
        "        xp = cp.get_array_module(mus)\n",
        "    except Exception:\n",
        "        import numpy as np\n",
        "        xp = np\n",
        "    import numpy as _np\n",
        "\n",
        "    K, D = mus.shape\n",
        "    pis = xp.asarray(pis, dtype=mus.dtype).reshape(K)\n",
        "    mus = xp.asarray(mus, dtype=mus.dtype).reshape(K, D)\n",
        "\n",
        "    # Likelihood set-up\n",
        "    if A is None and alpha is not None:\n",
        "        sigma2 = 1.0 / (2.0 * alpha)\n",
        "        A = xp.eye(D, dtype=mus.dtype)\n",
        "        b = xp.zeros(D, dtype=mus.dtype)\n",
        "        R = sigma2 * xp.eye(D, dtype=mus.dtype)\n",
        "    elif A is None:\n",
        "        raise ValueError(\"Provide (A,b,R) or alpha.\")\n",
        "    A = xp.asarray(A, dtype=mus.dtype)\n",
        "    k = int(A.shape[0])\n",
        "\n",
        "    y = xp.asarray(y, dtype=mus.dtype).reshape(k)\n",
        "    b = xp.zeros(k, dtype=mus.dtype) if b is None else xp.asarray(b, dtype=mus.dtype).reshape(k)\n",
        "    if R is None and alpha is not None:\n",
        "        sigma2 = 1.0 / (2.0 * alpha)\n",
        "        R = sigma2 * xp.eye(k, dtype=mus.dtype)\n",
        "    elif R is None:\n",
        "        raise ValueError(\"R must be provided when using general A without alpha.\")\n",
        "    R = xp.asarray(R, dtype=mus.dtype).reshape(k, k)\n",
        "\n",
        "    # Prior variances (support isotropic or diagonal)\n",
        "    Sigmas = xp.asarray(Sigmas, dtype=mus.dtype)\n",
        "    if Sigmas.ndim == 1 and Sigmas.shape[0] == K:\n",
        "        s2 = (Sigmas**2)[:, None] * xp.ones((1, D), dtype=mus.dtype)     # (K, D)\n",
        "    elif Sigmas.ndim == 2 and Sigmas.shape == (K, D):\n",
        "        s2 = Sigmas**2                                                  # (K, D)\n",
        "    else:\n",
        "        raise ValueError(\"Sigmas must have shape (K,) or (K,D) (isotropic or diagonal).\")\n",
        "    inv_s2 = 1.0 / xp.maximum(s2, 1e-300)                                # (K, D)\n",
        "\n",
        "    # Data terms: J = A^T R^{-1} A, h = A^T R^{-1} (y - b)\n",
        "    Rinv_A = xp.linalg.solve(R, A)               # (k, D)\n",
        "    J = A.T @ Rinv_A                             # (D, D)\n",
        "    h = A.T @ xp.linalg.solve(R, (y - b))        # (D,)\n",
        "\n",
        "    means_post = xp.empty((K, D), dtype=mus.dtype)\n",
        "    stds_post  = xp.empty((K, D), dtype=mus.dtype) if out == \"diag\" else None\n",
        "    covs_post  = xp.empty((K, D, D), dtype=mus.dtype) if out == \"full\" else None\n",
        "    logw       = xp.empty(K, dtype=mus.dtype)\n",
        "\n",
        "    TWO_PI = xp.asarray(2.0 * _np.pi, dtype=mus.dtype)\n",
        "    y_center = y[None, :] - (A @ mus.T).T - b[None, :]                   # (K, k)\n",
        "    I_D = xp.eye(D, dtype=mus.dtype)\n",
        "\n",
        "    for i in range(K):\n",
        "        # Posterior precision: P_i = J + diag(inv_s2[i])\n",
        "        P_i = J + xp.diag(inv_s2[i])\n",
        "\n",
        "        # Mean: m_i = P_i^{-1} (diag(inv_s2[i]) mu_i + h) via Cholesky\n",
        "        L = xp.linalg.cholesky(P_i)                                      # lower\n",
        "        rhs = inv_s2[i] * mus[i] + h\n",
        "        z = xp.linalg.solve(L, rhs)\n",
        "        m_i = xp.linalg.solve(L.T, z)\n",
        "        means_post[i] = m_i\n",
        "\n",
        "        # Covariance: S_i = P_i^{-1}; compute from Cholesky\n",
        "        # L L^T = P_i => P_i^{-1} = L^{-T} L^{-1}\n",
        "        Z = xp.linalg.solve(L, I_D)                                      # L^{-1}\n",
        "        if out == \"full\":\n",
        "            covs_post[i] = Z.T @ Z\n",
        "        else:  # \"diag\"\n",
        "            diag_S = xp.sum(Z * Z, axis=0)\n",
        "            stds_post[i] = xp.sqrt(xp.maximum(diag_S, 0.0))\n",
        "\n",
        "        # Evidence weight for component i: N(y; A mu_i + b, A Σ_i A^T + R)\n",
        "        AiSigmaAT = (A * s2[i][None, :]) @ A.T                           # A diag(s2_i) A^T\n",
        "        C_i = AiSigmaAT + R                                              # (k, k)\n",
        "        Lc = xp.linalg.cholesky(C_i)\n",
        "        v  = y_center[i]\n",
        "        zc = xp.linalg.solve(Lc, v)\n",
        "        #quad = zc @ xp.linalg.solve(Lc.T, zc)\n",
        "        quad = zc @ zc\n",
        "        log_det = 2.0 * xp.sum(xp.log(xp.diag(Lc)))\n",
        "        log_like = -0.5 * (k * xp.log(TWO_PI) + log_det + quad)\n",
        "        logw[i] = xp.log(pis[i] + 1e-300) + log_like\n",
        "\n",
        "    # normalize mixture weights\n",
        "    mlog = xp.max(logw)\n",
        "    w_post = xp.exp(logw - mlog)\n",
        "    w_post /= xp.sum(w_post) + 1e-300\n",
        "\n",
        "    if out == \"full\":\n",
        "        return w_post, means_post, covs_post\n",
        "    else:\n",
        "        return w_post, means_post, stds_post\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ─── 2. SLICED-WASSERSTEIN-2 HELPER ────────────────────────────────────────────\n",
        "def sliced_wasserstein2(a, b, n_proj: int = 256, max_pts: int = 5000):\n",
        "    xp = cp                                             # keep GPU\n",
        "\n",
        "    # ----- subsample to equal size -----------------------------------------\n",
        "    n = min(max_pts, a.shape[0], b.shape[0])\n",
        "    ia = xp.random.choice(a.shape[0], n, replace=False)\n",
        "    ib = xp.random.choice(b.shape[0], n, replace=False)\n",
        "    a_, b_ = a[ia], b[ib]\n",
        "\n",
        "    # ----- random directions in R^D ----------------------------------------\n",
        "    D = a_.shape[1]\n",
        "    dirs = xp.random.randn(n_proj, D, dtype=xp.float64)\n",
        "    dirs /= xp.linalg.norm(dirs, axis=1, keepdims=True) + 1e-12   # unit vectors\n",
        "\n",
        "    # ----- project, sort, average squared diffs ----------------------------\n",
        "    pa = xp.sort(a_ @ dirs.T, axis=0)\n",
        "    pb = xp.sort(b_ @ dirs.T, axis=0)\n",
        "    w2_sq = xp.mean((pa - pb)**2)\n",
        "\n",
        "    return float(xp.sqrt(w2_sq).get())          # move single value to host\n",
        "\n",
        "\n",
        "\n",
        "def _median_pairwise_dist(X):\n",
        "    # median ||x_i - x_j|| over i<j (approx via full matrix; subsample with max_pts)\n",
        "    x2 = cp.sum(X*X, axis=1, keepdims=True)\n",
        "    D2 = cp.maximum(x2 + x2.T - 2.0*(X @ X.T), 0.0)\n",
        "    i = cp.triu_indices(D2.shape[0], 1)\n",
        "    d = cp.sqrt(D2[i])\n",
        "    return float(cp.median(d))\n",
        "\n",
        "def _median_knn_dist(X, k=5):\n",
        "    # median distance to the k-th NN (leave-one-out via inf diagonal)\n",
        "    x2 = cp.sum(X*X, axis=1, keepdims=True)\n",
        "    D2 = cp.maximum(x2 + x2.T - 2.0*(X @ X.T), 0.0)\n",
        "    cp.fill_diagonal(D2, cp.inf)\n",
        "    # kth smallest along rows\n",
        "    kth = cp.partition(D2, kth=k-1, axis=1)[:, k-1]\n",
        "    return float(cp.sqrt(cp.median(kth)))\n",
        "\n",
        "def _choose_sigmas(X, mode=\"auto_pair+mnn\", knn_k=5,\n",
        "                   mnn_multipliers=(0.5, 1.0, 2.0, 4.0),\n",
        "                   pair_multipliers=(0.5, 1.0, 2.0)):\n",
        "    sigs = []\n",
        "    if \"pair\" in mode:\n",
        "        md = _median_pairwise_dist(X)\n",
        "        sigs += [md*m for m in pair_multipliers]\n",
        "    if \"mnn\" in mode:\n",
        "        mnn = _median_knn_dist(X, k=knn_k)\n",
        "        sigs += [mnn*m for m in mnn_multipliers]\n",
        "    sigs = sorted({float(s) for s in sigs if s > 0})\n",
        "    if not sigs:  # fallback\n",
        "        md = _median_pairwise_dist(X)\n",
        "        sigs = [md]\n",
        "    return cp.asarray(sigs, dtype=cp.float64)\n",
        "\n",
        "def auto_multiscale_ksd(\n",
        "    x, score_fn, *,\n",
        "    sigmas=\"auto_pair+mnn\",      # \"auto_mnn\", \"auto_pair\", \"auto_pair+mnn\", or array-like\n",
        "    knn_k=5,\n",
        "    mnn_multipliers=(0.5, 1.0, 2.0, 4.0),\n",
        "    pair_multipliers=(0.5, 1.0, 2.0),\n",
        "    max_pts=5000,\n",
        "    agg=\"mean\",                  # \"mean\" or \"max\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Multiscale KSD with data-adaptive σ including median k-NN spacing.\n",
        "    Uses your existing RBF-Stein formula and averages (or maxes) across σ's.\n",
        "    \"\"\"\n",
        "    xp = cp\n",
        "    X = xp.asarray(x, dtype=xp.float64)\n",
        "    if X.shape[0] > max_pts:\n",
        "        idx = xp.random.choice(X.shape[0], max_pts, replace=False)\n",
        "        X = X[idx]\n",
        "    N, D = X.shape\n",
        "\n",
        "    s = score_fn(X)                             # (N,D)\n",
        "    diff = X[:, None, :] - X[None, :, :]        # (N,N,D)\n",
        "    r2 = xp.sum(diff*diff, axis=-1)             # (N,N)\n",
        "\n",
        "    if isinstance(sigmas, str) and sigmas.startswith(\"auto\"):\n",
        "        sig = _choose_sigmas(\n",
        "            X, mode=sigmas, knn_k=knn_k,\n",
        "            mnn_multipliers=mnn_multipliers,\n",
        "            pair_multipliers=pair_multipliers\n",
        "        )\n",
        "    else:\n",
        "        sig = xp.asarray(sigmas, dtype=xp.float64).ravel()\n",
        "    sig = sig[xp.isfinite(sig) & (sig > 0)]\n",
        "    assert sig.size >= 1, \"No valid σ found.\"\n",
        "\n",
        "    # Precompute score inner products\n",
        "    sdot = s @ s.T                                # (N,N)\n",
        "    # r·s terms\n",
        "    r_dot_sx = xp.einsum('ijd,id->ij', diff, s)   # (N,N)\n",
        "    r_dot_sy = xp.einsum('ijd,jd->ij', diff, s)   # (N,N)\n",
        "\n",
        "    vals = []\n",
        "    for σ in sig:\n",
        "        σ2 = σ*σ\n",
        "        K = xp.exp(-r2 / (2.0*σ2))                # RBF\n",
        "        term1 = sdot * K\n",
        "        term2 = (r_dot_sx - r_dot_sy) * (K / σ2)\n",
        "        term3 = (D/σ2 - r2/(σ2*σ2)) * K\n",
        "        U = term1 + term2 + term3\n",
        "        ksd2 = xp.sum(U) / (N*N)\n",
        "        vals.append(ksd2)\n",
        "\n",
        "    vals = xp.asarray(vals, dtype=xp.float64)\n",
        "    out = (float(xp.mean(vals)) if agg == \"mean\" else float(xp.max(vals)))\n",
        "    return out, cp.asnumpy(sig)  # return σ’s too for debugging\n",
        "\n",
        "\n",
        "def compute_multiscale_ksd(x, score_fn, sigmas = (0.1, 0.2, 0.4, .8)): #sigmas=(0.33, 0.5, 1.0, 2.0)):\n",
        "    xp = cp\n",
        "    N, D = x.shape\n",
        "    s = score_fn(x)\n",
        "    diff = x[:,None,:] - x[None,:,:]\n",
        "    r2   = xp.sum(diff**2, axis=-1)\n",
        "    ksd2 = 0.0\n",
        "    for σ in sigmas:\n",
        "        K = xp.exp(-r2 / (2*σ*σ))\n",
        "        sdot = s @ s.T\n",
        "        r_dot_sx = xp.einsum('ijd,id->ij', diff, s)\n",
        "        r_dot_sy = xp.einsum('ijd,jd->ij', diff, s)\n",
        "        term1 = sdot * K\n",
        "        term2 = (r_dot_sx - r_dot_sy) / (σ*σ) * K\n",
        "        term3 = (D/(σ*σ) - r2/(σ**4)) * K\n",
        "        U = term1 + term2 + term3\n",
        "        ksd2 += xp.sum(U) / (N*N)\n",
        "    return float(ksd2 / len(sigmas))\n",
        "\n",
        "\n",
        "def xp_rmse(x,y):\n",
        "  xp = cp\n",
        "  diff = xp.square(x-y)\n",
        "  return xp.sqrt(xp.mean(diff))\n",
        "\n",
        "\n",
        "\n",
        "def blend_diagnostics(y, t, x_ref, s0_ref, *, xp=cp):\n",
        "    # weights\n",
        "    var = xp.maximum(1.0 - xp.exp(-2.0*t), 1e-12)\n",
        "    inv_v = 1.0 / var\n",
        "    et    = xp.exp(-t)\n",
        "    diff  = y[:, None, :] - et * x_ref[None, :, :]\n",
        "    logits = -0.5 * inv_v * xp.sum(diff*diff, axis=-1)\n",
        "    m = xp.max(logits, axis=1, keepdims=True)\n",
        "    w = xp.exp(logits - m); w /= xp.sum(w, axis=1, keepdims=True) + 1e-12\n",
        "\n",
        "    # scores\n",
        "    s_twd = -(inv_v) * (y - et * (w @ x_ref))\n",
        "    s_kss = xp.exp(t) * (w @ s0_ref)\n",
        "\n",
        "    # cosine\n",
        "    num = xp.sum(s_twd * s_kss, axis=1)\n",
        "    den = xp.linalg.norm(s_twd, axis=1) * xp.linalg.norm(s_kss, axis=1) + 1e-12\n",
        "    cos = num / den\n",
        "\n",
        "    # ESS and λ (using your current snis_blend)\n",
        "    lam, det = snis_blend(y, x_ref, s0_ref, t, return_details=True)\n",
        "    ess = det['ess']\n",
        "    return dict(\n",
        "        cos=cos, ess=ess, lam=lam, w=w,\n",
        "        s_twd=s_twd, s_kss=s_kss\n",
        "    )\n",
        "\n",
        "def summarize_diag(diag, tag=\"\"):\n",
        "    to_np = lambda z: cp.asnumpy(z)\n",
        "    cos = to_np(diag['cos']); ess = to_np(diag['ess']); lam = to_np(diag['lam'])\n",
        "    print(f\"[{tag}] cos  median/05%/95%: {np.median(cos):+.3f}  {np.percentile(cos,5):+.3f}  {np.percentile(cos,95):+.3f}\")\n",
        "    print(f\"[{tag}] ESS  median/min/max: {np.median(ess):.1f}   {ess.min():.1f}  {ess.max():.1f}\")\n",
        "    print(f\"[{tag}] lam  median/min/max: {np.median(lam):.3f}   {lam.min():.3f}  {lam.max():.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "def blend_sampler(\n",
        "    N_part, N_ref, time_pts, batch_size, sampler_func, score_func, mode,\n",
        "    likelyhood_func=None, loglik_grad_fn=None, true_score_func=None,\n",
        "    h_coeff=0.5, seed='rand', weight_mode='snis', w_correct_func=None,\n",
        "    use_proxy_tweedie=False,          # <-- NEW\n",
        "    proxy_mu_func=None,               # <-- NEW: callable x_ref -> mu_ref\n",
        "):\n",
        "    xp = cp\n",
        "    rmse_list = []\n",
        "    dim = int(sampler_func(5).shape[1])\n",
        "    y = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "    x0_ref = sampler_func(N_ref) if seed == 'rand' else sampler_func(N_ref, seed=seed)\n",
        "\n",
        "    if w_correct_func is None:\n",
        "        w_correct = 1.0\n",
        "    else:\n",
        "        w_correct = w_correct_func(x0_ref)\n",
        "\n",
        "    s0_ref = score_func(x0_ref)\n",
        "\n",
        "    # if requested, build proxy means once\n",
        "    mu_ref = None\n",
        "    if use_proxy_tweedie:\n",
        "        if proxy_mu_func is None:\n",
        "            raise ValueError(\"use_proxy_tweedie=True requires proxy_mu_func.\")\n",
        "        mu_ref, _ = proxy_mu_func(x0_ref) if isinstance(proxy_mu_func, type(lambda:0)) else proxy_mu_func(x0_ref)\n",
        "        # allow proxy_mu_func to return (mu,var) or just mu\n",
        "        if isinstance(mu_ref, tuple):\n",
        "            mu_ref = mu_ref[0]\n",
        "\n",
        "    for k_step in range(len(time_pts)-1):\n",
        "        t_cur, t_prev = time_pts[k_step], time_pts[k_step+1]\n",
        "        dt = t_cur - t_prev\n",
        "        noise = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "        if weight_mode == 'snis' or not true_score_func:\n",
        "            S_cur = s_blend(y, t_cur, x0_ref, s0_ref,\n",
        "                            loglik_fn=likelyhood_func, loglik_grad_fn=loglik_grad_fn,\n",
        "                            w_correct=w_correct, twd_basis=mu_ref)\n",
        "            y_hat = y + dt * (y + 2 * S_cur) + xp.sqrt(2.0 * dt) * noise\n",
        "            S_proxy = s_blend(y_hat, t_prev, x0_ref, s0_ref,\n",
        "                              loglik_fn=likelyhood_func, loglik_grad_fn=loglik_grad_fn,\n",
        "                              w_correct=w_correct, twd_basis=mu_ref)\n",
        "        elif weight_mode == 'oracle':\n",
        "            S_cur = true_score_func(y, t_cur)\n",
        "            y_hat = y + dt * (y + 2 * S_cur) + xp.sqrt(2.0 * dt) * noise\n",
        "            S_proxy = true_score_func(y_hat, t_prev)\n",
        "\n",
        "        if mode == 'ou_sde':\n",
        "            y = y_hat\n",
        "        elif mode == 'pf_ode':\n",
        "            y = y + dt * (y + S_cur)\n",
        "        elif mode == 'heun_pc':\n",
        "            drift_avg = 0.5 * ((y + 2 * S_cur) + (y_hat + 2 * S_proxy))\n",
        "            y = y + dt * drift_avg + xp.sqrt(2.0 * dt) * noise\n",
        "        elif mode == 'heun_hop':\n",
        "            A, B = xp.exp(dt), (xp.exp(2 * dt) - 1.0)\n",
        "            C, D = xp.sqrt(B), B / A\n",
        "            y = A * y + h_coeff * B * S_proxy + (1 - h_coeff) * D * S_cur + C * noise\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mode {mode}\")\n",
        "\n",
        "    if true_score_func:\n",
        "        S_true1 = true_score_func(y, t_cur)\n",
        "        S_true2 = true_score_func(y_hat, t_prev)\n",
        "        rmse_list.append(float(xp_rmse(S_cur,  S_true1)))\n",
        "        rmse_list.append(float(xp_rmse(S_proxy, S_true2)))\n",
        "        return y, rmse_list\n",
        "    else:\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "# ─── 3. SAMPLER WITH K PRE-IMAGES FOR OU-HOP ──────────────────────────────────\n",
        "def tweedie_sampler(\n",
        "    N_part, N_ref, time_pts, batch_size,\n",
        "    sampler_func, mode, likelyhood_func = None,\n",
        "    true_score_func=None, h_coeff=0.5, seed='rand', w_correct_func = None):\n",
        "\n",
        "\n",
        "    xp = cp\n",
        "    rmse_list = []\n",
        "    dim = int(sampler_func(5).shape[1]) # Get the dimension from means0\n",
        "    y = xp.random.randn(N_part, dim).astype(xp.float64) # Initialize y with the correct dimension\n",
        "\n",
        "\n",
        "    if seed == 'rand':\n",
        "      x0_ref = sampler_func(N_ref)\n",
        "    else:\n",
        "      x0_ref = sampler_func(N_ref, seed = seed)\n",
        "\n",
        "    if w_correct_func is None:\n",
        "      w_correct = 1.0\n",
        "    else:\n",
        "      w_correct =w_correct_func(x0_ref)\n",
        "\n",
        "\n",
        "    for k_step in range(len(time_pts)-1):\n",
        "        t_cur, t_prev = time_pts[k_step], time_pts[k_step+1]\n",
        "\n",
        "        dt = t_cur - t_prev\n",
        "        noise = xp.random.randn(N_part,dim).astype(xp.float64)\n",
        "\n",
        "\n",
        "        A = xp.exp(dt)\n",
        "        B = (xp.exp(2 * dt) - 1.0)\n",
        "        C = xp.sqrt((xp.exp(2.0 * dt) - 1.0))\n",
        "        D = B/A\n",
        "        E = xp.log(A)\n",
        "\n",
        "        S_cur = tweedie_score_t(y, t_cur, x0_ref, loglik_fn = likelyhood_func, w_correct = w_correct)\n",
        "        y_hat = y + dt * (y + 2 * S_cur) + xp.sqrt(2.0 * dt) * noise\n",
        "        S_proxy = tweedie_score_t(y_hat, t_prev, x0_ref, loglik_fn = likelyhood_func, w_correct = w_correct)\n",
        "\n",
        "        if mode == 'ou_sde':\n",
        "            y = y_hat\n",
        "\n",
        "        elif mode == 'pf_ode':\n",
        "            y =  y + dt * (y + S_cur)\n",
        "\n",
        "        elif mode == 'heun_pc':\n",
        "            drift_avg = 0.5 * ((y + 2 * S_cur) + (y_hat + 2 * S_proxy))\n",
        "            y = y + dt * drift_avg + xp.sqrt(2.0 * dt) * noise\n",
        "\n",
        "        elif mode == 'heun_hop':\n",
        "           A,B = xp.exp(dt),(xp.exp(2 * dt) - 1.0),\n",
        "           C,D = xp.sqrt(B), B/A\n",
        "           y = A * y + h_coeff * B * S_proxy + (1-h_coeff) * D * S_cur + C * noise\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mode {mode}\")\n",
        "\n",
        "    if true_score_func:\n",
        "            S_true1 = true_score_func(y, t_cur)   # params passed in closure\n",
        "            S_true2 = true_score_func(y_hat, t_prev)\n",
        "\n",
        "            S_1_hat = S_cur\n",
        "            S_2_hat = S_proxy\n",
        "\n",
        "            rmse_list.append(float(xp_rmse(S_1_hat, S_true1)))\n",
        "            rmse_list.append(float(xp_rmse(S_2_hat, S_true2)))\n",
        "            return y, rmse_list\n",
        "    else:\n",
        "      return y\n",
        "\n",
        "\n",
        "\n",
        "# ─── 3. SAMPLER WITH K PRE-IMAGES FOR OU-HOP ──────────────────────────────────\n",
        "def kss_sampler(\n",
        "    N_part, N_ref, time_pts, batch_size,\n",
        "    sampler_func, score_func, mode, likelyhood_func = None, loglik_grad_fn = None,\n",
        "    true_score_func=None, h_coeff=0.5, seed='rand', w_correct_func = 1.0):\n",
        "\n",
        "    xp = cp\n",
        "    rmse_list = []\n",
        "    dim = int(sampler_func(5).shape[1]) # Get the dimension from means0\n",
        "    y = xp.random.randn(N_part, dim).astype(xp.float64) # Initialize y with the correct dimension\n",
        "\n",
        "\n",
        "    if seed == 'rand':\n",
        "      x0_ref = sampler_func(N_ref)\n",
        "    else:\n",
        "      x0_ref = sampler_func(N_ref, seed = seed)\n",
        "\n",
        "    s0_ref =  score_func(x0_ref)\n",
        "\n",
        "    if w_correct_func is None:\n",
        "      w_correct = 1.0\n",
        "    else:\n",
        "      w_correct =w_correct_func(x0_ref)\n",
        "\n",
        "    for k_step in range(len(time_pts)-1):\n",
        "        t_cur, t_prev = time_pts[k_step], time_pts[k_step+1]\n",
        "\n",
        "        dt = t_cur - t_prev\n",
        "        noise = xp.random.randn(N_part,dim).astype(xp.float64)\n",
        "\n",
        "        S_cur = kss_score_t(y, t_cur, x0_ref, s0_ref, loglik_fn = likelyhood_func,\n",
        "                            loglik_grad_fn = loglik_grad_fn, w_correct = w_correct)\n",
        "        y_hat = y + dt * (y + 2 * S_cur) + xp.sqrt(2.0 * dt) * noise\n",
        "        S_proxy = kss_score_t(y_hat, t_prev, x0_ref, s0_ref, loglik_fn = likelyhood_func,\n",
        "                              loglik_grad_fn = loglik_grad_fn, w_correct = w_correct)\n",
        "\n",
        "        if mode == 'ou_sde':\n",
        "            y = y_hat\n",
        "\n",
        "        elif mode == 'pf_ode':\n",
        "            y =  y + dt * (y + S_cur)\n",
        "\n",
        "        elif mode == 'heun_pc':\n",
        "            drift_avg = 0.5 * ((y + 2 * S_cur) + (y_hat + 2 * S_proxy))\n",
        "            y = y + dt * drift_avg + xp.sqrt(2.0 * dt) * noise\n",
        "\n",
        "        elif mode == 'heun_hop':\n",
        "           A,B = xp.exp(dt),(xp.exp(2 * dt) - 1.0),\n",
        "           C,D = xp.sqrt(B), B/A\n",
        "           y = A * y + h_coeff * B * S_proxy + (1-h_coeff) * D * S_cur + C * noise\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mode {mode}\")\n",
        "\n",
        "    if true_score_func:\n",
        "            S_true1 = true_score_func(y, t_cur)   # params passed in closure\n",
        "            S_true2 = true_score_func(y_hat, t_prev)\n",
        "\n",
        "            S_1_hat = S_cur\n",
        "            S_2_hat = S_proxy\n",
        "\n",
        "            rmse_list.append(float(xp_rmse(S_1_hat, S_true1)))\n",
        "            rmse_list.append(float(xp_rmse(S_2_hat, S_true2)))\n",
        "            return y, rmse_list\n",
        "    else:\n",
        "      return y\n",
        "\n",
        "\n",
        "def blackbox_sampler(\n",
        "    N_part, time_pts, custom_score, sampler_func, *,\n",
        "    mode=\"heun_hop\", h_coeff=0.5, true_score_func=None,\n",
        "    p_prune = 0, likelyhood_func = None, loglik_grad_fn = None):\n",
        "    xp  = cp\n",
        "    dim = int(sampler_func(5).shape[1])           # match model/data dim\n",
        "    y   = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "    rmse_list = []                                # collect RMSE vs oracle\n",
        "\n",
        "    for k in range(len(time_pts)-1):\n",
        "        t_cur, t_prev = time_pts[k], time_pts[k+1]\n",
        "        dt    = t_cur - t_prev\n",
        "        noise = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "        S_cur  = custom_score(y, t_cur)           # [N_part, dim]\n",
        "        y_hat  = y + dt*(y + 2*S_cur) + xp.sqrt(2.0*dt)*noise\n",
        "        S_prev = custom_score(y_hat, t_prev)\n",
        "\n",
        "        if mode == \"heun_hop\":\n",
        "            A = xp.exp(dt); B = xp.exp(2*dt) - 1.0; C = xp.sqrt(B); D = B / A\n",
        "            y = A*y + h_coeff*B*S_prev + (1-h_coeff)*D*S_cur + C*noise\n",
        "        elif mode == \"ou_sde\":\n",
        "            y = y_hat\n",
        "        elif mode == \"pf_ode\":\n",
        "            y = y + dt*(y + S_cur)\n",
        "        elif mode == \"heun_pc\":\n",
        "            drift_avg = 0.5*(y + 2*S_cur) + 0.5*(y_hat + 2*S_prev)\n",
        "            y = y + dt*drift_avg + xp.sqrt(2.0 * dt) * noise\n",
        "        else:\n",
        "            raise ValueError(mode)\n",
        "\n",
        "    if true_score_func is not None:\n",
        "        S_cur_true = true_score_func(y, t_cur)\n",
        "        S_prev_true = true_score_func(y_hat, t_prev)\n",
        "\n",
        "        S_cur_safe, cur_idx_safe = prune_cp_arr(S_cur, p_percent = p_prune)\n",
        "        S_prev_safe, prev_idx_safe = prune_cp_arr(S_prev, p_percent = p_prune)\n",
        "\n",
        "        RMSE_cur = float(xp_rmse(S_cur_safe, S_cur_true[cur_idx_safe]))\n",
        "        RMSE_prev = float(xp_rmse(S_prev_safe, S_prev_true[prev_idx_safe]))\n",
        "\n",
        "        rmse_list.append(RMSE_cur)\n",
        "        rmse_list.append(RMSE_prev)\n",
        "        return y, rmse_list\n",
        "    else:\n",
        "      return y\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "def _to_numpy(arr):\n",
        "    \"\"\"Return a NumPy array, copying from GPU if input is CuPy.\"\"\"\n",
        "    try:\n",
        "        import cupy as cp\n",
        "        if isinstance(arr, cp.ndarray):\n",
        "            return arr.get()\n",
        "    except ImportError:\n",
        "        pass\n",
        "    return np.asarray(arr)\n",
        "\n",
        "\n",
        "def div_unique_pairs(n: int):\n",
        "    pairs, cnt, out = list(combinations(range(1, n + 1), 2)), [0]*(n + 1), []\n",
        "    while pairs:\n",
        "        p = min(pairs, key=lambda t: (cnt[t[0]] + cnt[t[1]], t))\n",
        "        pairs.remove(p); out.append(p)\n",
        "        cnt[p[0]] += 1; cnt[p[1]] += 1\n",
        "    return out\n",
        "\n",
        "\n",
        "def get_row_norm(norm_ref, i,j, bins, norm_mode = \"max\"):\n",
        "    H_true, xedges, yedges = np.histogram2d(\n",
        "          norm_ref[:, i], norm_ref[:, j], bins=bins, density=False)\n",
        "    density_true = H_true / norm_ref.shape[0]\n",
        "    if norm_mode == \"max\":\n",
        "        vmax = density_true.max()\n",
        "    elif norm_mode == \"median\":\n",
        "        vmax = 2.0 * np.median(density_true[density_true > 0])\n",
        "    else:\n",
        "        raise ValueError(\"norm_mode must be 'max' or 'median'.\")\n",
        "    norm = plt.Normalize(vmin=0.0, vmax=vmax)\n",
        "    return norm\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_likelihood_contours(\n",
        "    ax,\n",
        "    likelihood_func,\n",
        "    xlim, ylim,\n",
        "    *,\n",
        "    levels=6,\n",
        "    color=\"white\",\n",
        "    alpha=0.9,\n",
        "    linewidths=1.2,\n",
        "    linestyles=\"solid\",\n",
        "    grid_res=200,\n",
        "    is_log_density=False,\n",
        "    normalize=True,\n",
        "    transform=None,\n",
        "    chunk_size=200_000,\n",
        "    zorder=12,\n",
        "    backend=\"auto\",\n",
        "    full_dim=None,\n",
        "    ij=None,\n",
        "    anchor=None,\n",
        "    levels_mode=\"auto\",\n",
        "    percentile_range=(75, 99.5),\n",
        "    coverage_clip=.4,\n",
        "    min_level=None,\n",
        "    mass_levels=None,\n",
        "    # --- debug ---\n",
        "    debug_save_path=None,\n",
        "    debug_save_raw=False,\n",
        "    verbose=False,\n",
        "):\n",
        "    xs = np.linspace(xlim[0], xlim[1], int(grid_res))\n",
        "    ys = np.linspace(ylim[0], ylim[1], int(grid_res))\n",
        "    X, Y = np.meshgrid(xs, ys, indexing=\"xy\")\n",
        "    P2 = np.stack([X.ravel(), Y.ravel()], axis=1)\n",
        "\n",
        "    try:\n",
        "        import cupy as cp\n",
        "    except Exception:\n",
        "        cp = None\n",
        "\n",
        "    def is_cupy(a): return (cp is not None) and isinstance(a, cp.ndarray)\n",
        "    def to_backend(a, use_cp):\n",
        "        if use_cp:\n",
        "            if cp is None: raise RuntimeError(\"backend='cupy' but CuPy unavailable.\")\n",
        "            return cp.asarray(a)\n",
        "        return np.asarray(a)\n",
        "\n",
        "    use_cp = (backend == \"cupy\") or (backend == \"auto\" and cp is not None)\n",
        "\n",
        "    # ---- lift 2D -> d if requested ----\n",
        "    if full_dim is not None and ij is not None:\n",
        "        i, j = ij\n",
        "        xp   = cp if use_cp else np\n",
        "        P2xp = to_backend(P2, use_cp)\n",
        "        if anchor is None:\n",
        "            anchor_xp = xp.zeros(full_dim, dtype=P2xp.dtype)\n",
        "        else:\n",
        "            anchor_xp = xp.asarray(anchor, dtype=P2xp.dtype)\n",
        "            if anchor_xp.shape != (full_dim,):\n",
        "                raise ValueError(f\"`anchor` must have shape ({full_dim},), got {anchor_xp.shape}\")\n",
        "        U = xp.broadcast_to(anchor_xp, (P2xp.shape[0], full_dim)).copy()\n",
        "        U[:, i] = P2xp[:, 0]\n",
        "        U[:, j] = P2xp[:, 1]\n",
        "    else:\n",
        "        U = to_backend(transform(P2) if transform is not None else P2, use_cp)\n",
        "\n",
        "    # ---- evaluate likelihood ----\n",
        "    try:\n",
        "        vals = likelihood_func(U)\n",
        "    except Exception:\n",
        "        out = []\n",
        "        N = U.shape[0]\n",
        "        for k in range(0, N, chunk_size):\n",
        "            out.append(likelihood_func(U[k:k+chunk_size]))\n",
        "        vals = (cp.concatenate(out, 0) if use_cp else np.concatenate(out, 0))\n",
        "\n",
        "    Z = cp.asnumpy(vals) if is_cupy(vals) else np.asarray(vals)\n",
        "    Z = Z.reshape(Y.shape)\n",
        "\n",
        "    # convert log-like -> like\n",
        "    if is_log_density:\n",
        "        Z = Z - np.nanmax(Z)\n",
        "        Z = np.exp(Z)\n",
        "\n",
        "    # optional normalization (does not affect mass-level ordering)\n",
        "    if normalize:\n",
        "        m = np.nanmax(Z)\n",
        "        if np.isfinite(m) and m > 0:\n",
        "            Z = Z / m\n",
        "\n",
        "    zfin = Z[np.isfinite(Z)]\n",
        "    if zfin.size == 0:\n",
        "        if verbose: print(\"[contours] all values non-finite; skip\")\n",
        "        return None\n",
        "\n",
        "    # === NEW: HDR mass-based levels ===\n",
        "    if mass_levels is not None:\n",
        "        masses = np.sort(np.clip(np.asarray(mass_levels, float), 0.0, 0.999999))\n",
        "        # compute highest-density thresholds: find t s.t. ∫_{Z>=t} Z dx dy = m * ∫ Z dx dy\n",
        "        dx = (xs[-1] - xs[0]) / (len(xs) - 1) if len(xs) > 1 else 1.0\n",
        "        dy = (ys[-1] - ys[0]) / (len(ys) - 1) if len(ys) > 1 else 1.0\n",
        "        area = dx * dy\n",
        "        vals1d = Z.ravel()\n",
        "        order = np.argsort(vals1d)[::-1]\n",
        "        sorted_vals = vals1d[order]\n",
        "        cums = np.cumsum(sorted_vals) * area\n",
        "        total = cums[-1] if cums.size else 1.0\n",
        "        lev = []\n",
        "        actual = []\n",
        "        for m in masses:\n",
        "            k = np.searchsorted(cums, m * total, side=\"left\")\n",
        "            thr = sorted_vals[k] if k < sorted_vals.size else sorted_vals[-1]\n",
        "            lev.append(thr)\n",
        "            # report actual enclosed mass with this threshold\n",
        "            mask = Z >= thr\n",
        "            actual_mass = (Z[mask].sum() * area) / total\n",
        "            actual.append(actual_mass)\n",
        "        lev = np.array(sorted(lev))  # ascending for contour\n",
        "        if verbose:\n",
        "            print(\"[contours] HDR mass levels requested:\", masses)\n",
        "            print(\"[contours] actual enclosed masses:    \", np.round(actual, 4))\n",
        "    else:\n",
        "        # --- old value-based level selection (kept for compatibility) ---\n",
        "        if isinstance(levels, int):\n",
        "            zmin, zmax = float(np.min(zfin)), float(np.max(zfin))\n",
        "            if levels_mode == \"percentile\":\n",
        "                lo, hi = np.percentile(zfin, percentile_range)\n",
        "            elif levels_mode == \"linear\":\n",
        "                lo, hi = zmin, zmax\n",
        "            else:\n",
        "                p95 = np.percentile(zfin, 95.0)\n",
        "                lo, hi = (np.percentile(zfin, percentile_range)\n",
        "                          if p95 < 0.15 else (zmin, zmax))\n",
        "            eps = 1e-12 * max(1.0, abs(hi))\n",
        "            lev = np.linspace(lo + eps, hi - eps, max(1, levels))\n",
        "        else:\n",
        "            lev = np.asarray(levels, float)\n",
        "\n",
        "        if min_level is not None:\n",
        "            lev = lev[lev >= float(min_level)]\n",
        "\n",
        "        if coverage_clip is not None and lev.size > 0:\n",
        "            keep = []\n",
        "            for lv in lev:\n",
        "                cov = np.mean(Z >= lv)\n",
        "                keep.append(cov <= coverage_clip)\n",
        "            keep = np.array(keep, bool)\n",
        "            if keep.any():\n",
        "                lev = lev[keep]\n",
        "            if lev.size == 0:\n",
        "                lev = np.sort(np.unique(np.percentile(zfin, np.linspace(85, 99.5, max(3, levels if isinstance(levels, int) else 6)))))\n",
        "\n",
        "    if lev.size == 0:\n",
        "        if verbose: print(\"[contours] no levels; skip\")\n",
        "        return None\n",
        "\n",
        "    cs = ax.contour(\n",
        "        X, Y, Z, levels=lev,\n",
        "        colors=color, linewidths=linewidths, linestyles=linestyles,\n",
        "        alpha=alpha, zorder=zorder, antialiased=True,\n",
        "    )\n",
        "\n",
        "    if debug_save_path is not None:\n",
        "        fig, axd = plt.subplots(1, 1, dpi=200)\n",
        "        im = axd.imshow(Z, origin=\"lower\",\n",
        "                        extent=[xs[0], xs[-1], ys[0], ys[-1]],\n",
        "                        aspect=\"equal\", cmap=\"magma\")\n",
        "        fig.colorbar(im, ax=axd, fraction=0.046, pad=0.04).set_label(\"likelihood (scaled)\" if normalize else \"likelihood\")\n",
        "        axd.contour(X, Y, Z, levels=lev, colors=\"w\", linewidths=1.2)\n",
        "        axd.set_xlim(xlim); axd.set_ylim(ylim)\n",
        "        axd.set_title(\"Likelihood contours (HDR mass levels)\" if mass_levels is not None else \"Likelihood contours\")\n",
        "        fig.savefig(debug_save_path, bbox_inches=\"tight\"); plt.close(fig)\n",
        "        if debug_save_raw:\n",
        "            base = debug_save_path.rsplit(\".\", 1)[0]\n",
        "            np.savez(base + \".npz\", X=X, Y=Y, Z=Z, levels=np.asarray(lev),\n",
        "                     xlim=np.asarray(xlim), ylim=np.asarray(ylim))\n",
        "\n",
        "    return cs\n",
        "\n",
        "\n",
        "\n",
        "import itertools\n",
        "import numpy as np\n",
        "try:\n",
        "    import cupy as cp\n",
        "except Exception:\n",
        "    cp = None\n",
        "\n",
        "\n",
        "def get_best_div_indexes(\n",
        "    y_finals_dict: dict,\n",
        "    compare_keys: dict,\n",
        "    n_rows: int,\n",
        "    div=sliced_wasserstein2,\n",
        "    *,\n",
        "    prior_samples=None,\n",
        "    max_dim_pairs: int | None = None,\n",
        "    eps: float = 1e-12,\n",
        ") -> list[tuple[int, int]]:\n",
        "    \"\"\"\n",
        "    Find the (i,j) dim pairs where the quality gap between two samplers\n",
        "    is largest, measured by ratio r(i,j) = d1(i,j)/d2(i,j) with\n",
        "      d1(i,j) = div( y[key_best][:, [i,j]], prior[:, [i,j]] )\n",
        "      d2(i,j) = div( y[key_worst][:, [i,j]], prior[:, [i,j]] )\n",
        "\n",
        "    Returns 1-based indices: [(i+1, j+1), ...] (length n_rows).\n",
        "\n",
        "    Args:\n",
        "      y_finals_dict: dict[str, array-like], each value is (N, D)\n",
        "      compare_keys: {'best': key1, 'worst': key2}\n",
        "      n_rows: number of pairs to return (lowest ratios first)\n",
        "      div: callable(a, b) -> float, default sliced_wasserstein2\n",
        "      prior_samples: array-like (N0, D), REQUIRED (used as reference)\n",
        "      max_dim_pairs: optional cap on the number of pairs to scan\n",
        "      eps: small positive to avoid divide-by-zero\n",
        "\n",
        "    Raises:\n",
        "      ValueError on missing keys, missing prior, or dim mismatches.\n",
        "    \"\"\"\n",
        "    if prior_samples is None:\n",
        "        raise ValueError(\"get_best_div_indexes: 'prior_samples' is required.\")\n",
        "    if 'best' not in compare_keys or 'worst' not in compare_keys:\n",
        "        raise ValueError(\"compare_keys must be a dict with keys {'best','worst'}.\")\n",
        "\n",
        "    key1 = compare_keys['best']\n",
        "    key2 = compare_keys['worst']\n",
        "    if key1 not in y_finals_dict or key2 not in y_finals_dict:\n",
        "        raise ValueError(f\"compare_keys refer to '{key1}' and '{key2}', \"\n",
        "                         f\"but available keys are {list(y_finals_dict.keys())}.\")\n",
        "\n",
        "    A = y_finals_dict[key1]\n",
        "    B = y_finals_dict[key2]\n",
        "    P = prior_samples\n",
        "\n",
        "    # Convert to numpy for shape checks, we’ll move slices to CuPy for div()\n",
        "    A_np = np.asarray(A)\n",
        "    B_np = np.asarray(B)\n",
        "    P_np = np.asarray(P)\n",
        "\n",
        "    if A_np.ndim != 2 or B_np.ndim != 2 or P_np.ndim != 2:\n",
        "        raise ValueError(\"All inputs must be 2D (N, D).\")\n",
        "\n",
        "    D = A_np.shape[1]\n",
        "    if B_np.shape[1] != D or P_np.shape[1] != D:\n",
        "        raise ValueError(\"Dimension mismatch across inputs for compare/div.\")\n",
        "\n",
        "    # Build list of unique unordered pairs (i<j)\n",
        "    all_pairs = list(itertools.combinations(range(D), 2))\n",
        "    if max_dim_pairs is not None:\n",
        "        all_pairs = all_pairs[:max_dim_pairs]\n",
        "\n",
        "    # Ensure CuPy is available for the provided div (your sliced_wasserstein2 uses cp)\n",
        "    if cp is None:\n",
        "        raise RuntimeError(\"CuPy not available; required for sliced_wasserstein2.\")\n",
        "\n",
        "    ratios: list[tuple[float, int, int]] = []\n",
        "\n",
        "    # Pre-cast full arrays to CuPy once to avoid repeated host->device copies\n",
        "    A_cp = cp.asarray(A_np)\n",
        "    B_cp = cp.asarray(B_np)\n",
        "    P_cp = cp.asarray(P_np)\n",
        "\n",
        "    for (i, j) in all_pairs:\n",
        "        # Slice the two coordinates; shape (N, 2) possibly with different N across A/B/P\n",
        "        A_ij = A_cp[:, [i, j]]\n",
        "        B_ij = B_cp[:, [i, j]]\n",
        "        P_ij = P_cp[:, [i, j]]\n",
        "\n",
        "        d1 = div(A_ij, P_ij)\n",
        "        d2 = div(B_ij, P_ij)\n",
        "        r = d1 / (d2 + eps)\n",
        "        ratios.append((r, i, j))\n",
        "\n",
        "    # Sort by ascending ratio; lowest means key1 is relatively closer to prior than key2\n",
        "    ratios.sort(key=lambda t: t[0])\n",
        "\n",
        "    # Return top n_rows as 1-based pairs\n",
        "    top = ratios[:max(1, n_rows)]\n",
        "    return [(i + 1, j + 1) for (_, i, j) in top]\n",
        "\n",
        "\n",
        "\n",
        "from typing import Callable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "try:\n",
        "    import cupy as cp\n",
        "except Exception:\n",
        "    cp = None\n",
        "\n",
        "\n",
        "from typing import Callable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "try:\n",
        "    import cupy as cp\n",
        "except Exception:\n",
        "    cp = None\n",
        "\n",
        "\n",
        "def plot_pair_histograms(\n",
        "        y_finals_dict: dict,\n",
        "        prior_samples,\n",
        "        post_samples,\n",
        "        save_path: str,\n",
        "        dim_pairs: list[tuple[int, int]] = None,\n",
        "        *,\n",
        "        bins: int = 100,\n",
        "        norm_mode: str = \"max\",\n",
        "        hist_norm: float = 1.25,\n",
        "        nrows: int = 3,\n",
        "        mode: str = 'first',\n",
        "        display_mode: str = 'standard',\n",
        "        pre_process_f=None,       # optional preprocessing callable\n",
        "        colormap: str = 'inferno',\n",
        "        post_ref = True,\n",
        "        likelyhood_func = None,\n",
        "        draw_countours = True,\n",
        "        prior_scale = True,\n",
        "        plot_prior = True,\n",
        "        plot_post = True,\n",
        "        post_hist_norm: float = 0.8,\n",
        "        # ---- comparison / selection controls ----\n",
        "        compare_keys: dict | None = None,\n",
        "        div_fn: Callable = sliced_wasserstein2,\n",
        "        # ---- NEW: per-panel normalization instead of shared reference ----\n",
        "        self_normalize: bool = False,\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Plot 2D histograms for selected dimension pairs across prior and samplers.\n",
        "\n",
        "    If self_normalize=True, each panel computes its own normalization\n",
        "    (via get_row_norm) using the panel's own data on (i,j). Otherwise,\n",
        "    normalization is shared within each row and computed from either\n",
        "    post_samples (when post_ref=True and provided) or prior_samples.\n",
        "    \"\"\"\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # helpers\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def _to_numpy_local(arr):\n",
        "        try:\n",
        "            import cupy as _cp\n",
        "            if isinstance(arr, _cp.ndarray):\n",
        "                return arr.get()\n",
        "        except Exception:\n",
        "            pass\n",
        "        return np.asarray(arr)\n",
        "\n",
        "    def _all_pairs(D: int):\n",
        "        return [(i+1, j+1) for i in range(D) for j in range(i+1, D)]\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # prep data & optional preprocessing\n",
        "    # ------------------------------------------------------------------ #\n",
        "    true_np = _to_numpy_local(prior_samples)\n",
        "\n",
        "    if post_samples is not None and plot_post:\n",
        "        post_dict = {'Posterior': _to_numpy_local(post_samples)}\n",
        "        y_finals_dict = {**post_dict, **y_finals_dict}\n",
        "\n",
        "    y_finals_np = {k: _to_numpy_local(v) for k, v in y_finals_dict.items()}\n",
        "\n",
        "    # apply preprocessing if provided\n",
        "    if pre_process_f is not None:\n",
        "        true_np = _to_numpy_local(pre_process_f(true_np))\n",
        "        y_finals_np = {k: _to_numpy_local(pre_process_f(v)) for k, v in y_finals_np.items()}\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # choose coordinate pairs on the (possibly transformed) space\n",
        "    # ------------------------------------------------------------------ #\n",
        "    if dim_pairs is None:\n",
        "        d = true_np.shape[1]\n",
        "        if mode == 'first':\n",
        "            pairs_all = _all_pairs(d)\n",
        "            dim_pairs = pairs_all[: min(nrows, len(pairs_all))]\n",
        "\n",
        "        elif mode == 'rand':\n",
        "            pairs_all = _all_pairs(d)\n",
        "            k = min(nrows, len(pairs_all))\n",
        "            idx = np.random.choice(len(pairs_all), k, replace=False)\n",
        "            dim_pairs = [pairs_all[i] for i in idx]\n",
        "\n",
        "        elif mode == 'pca':\n",
        "            pca = PCA(n_components=min(32, d))\n",
        "            pca.fit(true_np)\n",
        "            sorted_idx = np.argsort(-pca.explained_variance_)\n",
        "            dim_pairs = [(sorted_idx[2*k] + 1, sorted_idx[2*k+1] + 1)\n",
        "                        for k in range(min(nrows, d // 2))]\n",
        "\n",
        "        elif mode == 'best_compare':\n",
        "            if compare_keys is None:\n",
        "                raise ValueError(\n",
        "                    \"mode='best_compare' requires compare_keys={'best': key1, 'worst': key2}.\"\n",
        "                )\n",
        "            # Use preprocessed arrays for consistency with plot\n",
        "            dim_pairs = get_best_div_indexes(\n",
        "                y_finals_np, compare_keys, n_rows=nrows, div=div_fn, prior_samples=true_np\n",
        "            )\n",
        "            if len(dim_pairs) == 0:\n",
        "                raise ValueError(\"best_compare produced no dimension pairs.\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mode {mode}\")\n",
        "\n",
        "    # convert to 0-based indices\n",
        "    pairs = [(i - 1, j - 1) for (i, j) in dim_pairs]\n",
        "    max_dim = max(max(p) for p in pairs)\n",
        "    if true_np.shape[1] <= max_dim:\n",
        "        raise ValueError(\"Requested dimension exceeds data dimensionality.\")\n",
        "    for name, gen in y_finals_np.items():\n",
        "        if gen.shape[1] <= max_dim:\n",
        "            raise ValueError(f\"{name!r} has too few dimensions for selected pairs.\")\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # figure grid\n",
        "    # ------------------------------------------------------------------ #\n",
        "    n_rows = len(pairs)\n",
        "    if plot_prior:\n",
        "        n_cols = 1 + len(y_finals_np)\n",
        "    else:\n",
        "        n_cols = len(y_finals_np)\n",
        "\n",
        "    fig_w = 4 * n_cols\n",
        "    fig_h = 4 * n_rows\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(fig_w, fig_h), squeeze=False)\n",
        "\n",
        "    # spacing\n",
        "    if display_mode == 'min_label':\n",
        "        plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
        "    else:\n",
        "        plt.subplots_adjust(wspace=0.25, hspace=0.25)\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # normalization references for shared (non-self) mode\n",
        "    # ------------------------------------------------------------------ #\n",
        "    if post_samples is not None and post_ref:\n",
        "        norm_ref = _to_numpy_local(post_samples)\n",
        "        h_ref = post_hist_norm\n",
        "    else:\n",
        "        norm_ref = true_np\n",
        "        h_ref = hist_norm\n",
        "\n",
        "    # choose which data determines 2D bins/extent\n",
        "    plot_refs = true_np if prior_scale else norm_ref\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # per-row rendering\n",
        "    # ------------------------------------------------------------------ #\n",
        "    for row_idx, (i, j) in enumerate(pairs):\n",
        "\n",
        "        # establish common edges for the row (kept same even in self_normalize)\n",
        "        _, xedges, yedges = np.histogram2d(\n",
        "            plot_refs[:, i], plot_refs[:, j], bins=bins, density=False\n",
        "        )\n",
        "\n",
        "        # Shared normalization (legacy behavior): one Normalize per row.\n",
        "        # Self-normalize: computed per panel from that panel's data.\n",
        "        if not self_normalize:\n",
        "            shared_norm = get_row_norm(_to_numpy_local(norm_ref), i, j, bins, norm_mode)\n",
        "        else:\n",
        "            shared_norm = None  # unused\n",
        "\n",
        "        # likelihood wrapper\n",
        "        if likelyhood_func is not None and draw_countours:\n",
        "            def like_wrapper(U_np):\n",
        "                return cp.asnumpy(likelyhood_func(cp.asarray(U_np))) if cp is not None else None\n",
        "        else:\n",
        "            like_wrapper = None\n",
        "\n",
        "        # ------------------------------------------------------------------ #\n",
        "        # drawing helper (now supports per-panel norm)\n",
        "        # ------------------------------------------------------------------ #\n",
        "        def _draw(ax, data_np, n_samples, title, draw_title: bool,\n",
        "                  h_norm: float,\n",
        "                  likelihood_func=None,\n",
        "                  plt_norm=None,\n",
        "                  like_levels=5, like_color=\"#D3D3D3\",\n",
        "                  like_alpha=.92, like_linewidths=1.5, like_linestyles=\"solid\",\n",
        "                  like_grid_res=200, like_is_log_density=False,\n",
        "                  like_normalize=True, like_transform=None,\n",
        "                  like_backend=\"auto\", like_full_dim=None,\n",
        "                  mass_levels=None,  # e.g. [0.65, .75, 0.85, 0.95, .99]\n",
        "                  percentiles=(75, 99.5),\n",
        "                  like_ij=None, like_anchor=None, debug_save_path=None):\n",
        "\n",
        "            # compute panel-specific normalization if requested\n",
        "            if self_normalize:\n",
        "                plt_norm = get_row_norm(data_np, i, j, bins, norm_mode)\n",
        "\n",
        "            H, _, _ = np.histogram2d(\n",
        "                data_np[:, i], data_np[:, j],\n",
        "                bins=[xedges, yedges], density=False\n",
        "            )\n",
        "            density = (h_norm * H / max(1, n_samples)) ** 0.95\n",
        "\n",
        "            ax.imshow(\n",
        "                density.T,\n",
        "                origin=\"lower\",\n",
        "                cmap=colormap,\n",
        "                norm=plt_norm if plt_norm is not None else shared_norm,\n",
        "                extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]],\n",
        "                aspect=\"auto\",\n",
        "                interpolation=\"nearest\",\n",
        "            )\n",
        "\n",
        "            if display_mode == 'min_label':\n",
        "                ax.set_xticks([]); ax.set_yticks([])\n",
        "            else:\n",
        "                ax.set_xlabel(f\"$d_{{{i+1}}}$\")\n",
        "                ax.set_ylabel(f\"$d_{{{j+1}}}$\")\n",
        "            if draw_title:\n",
        "                ax.set_title(title if isinstance(title, str) else \"\", fontsize=16)\n",
        "\n",
        "            if likelihood_func is not None:\n",
        "                xlim = ax.get_xlim()\n",
        "                ylim = ax.get_ylim()\n",
        "                plot_likelihood_contours(\n",
        "                    ax, likelihood_func, xlim, ylim,\n",
        "                    levels=like_levels, color=like_color,\n",
        "                    alpha=like_alpha, linewidths=like_linewidths,\n",
        "                    linestyles=like_linestyles,\n",
        "                    grid_res=like_grid_res, is_log_density=like_is_log_density,\n",
        "                    normalize=like_normalize, transform=like_transform,\n",
        "                    backend=like_backend, full_dim=like_full_dim,\n",
        "                    ij=like_ij, anchor=like_anchor,\n",
        "                    percentile_range=percentiles,\n",
        "                    mass_levels=mass_levels,\n",
        "                    debug_save_path=debug_save_path\n",
        "                )\n",
        "\n",
        "        # ------------------------------------------------------------------ #\n",
        "        # column 0: prior\n",
        "        # ------------------------------------------------------------------ #\n",
        "        if plot_prior:\n",
        "            prior_panel_norm = None if self_normalize else get_row_norm(true_np, i, j, bins, norm_mode)\n",
        "            _draw(\n",
        "                axes[row_idx, 0],\n",
        "                true_np, true_np.shape[0],\n",
        "                True,  # title placeholder\n",
        "                draw_title=(row_idx == 0),\n",
        "                h_norm=hist_norm,\n",
        "                likelihood_func=(\n",
        "                    (lambda U_np: cp.asnumpy(likelyhood_func(cp.asarray(U_np))))\n",
        "                    if (likelyhood_func is not None and draw_countours and cp is not None)\n",
        "                    else None\n",
        "                ),\n",
        "                plt_norm=prior_panel_norm,\n",
        "                like_full_dim=true_np.shape[1],\n",
        "                like_is_log_density=True,\n",
        "                like_ij=(i, j),\n",
        "                like_anchor=None,\n",
        "                debug_save_path=None\n",
        "            )\n",
        "\n",
        "        # ------------------------------------------------------------------ #\n",
        "        # remaining columns: each sampler\n",
        "        # ------------------------------------------------------------------ #\n",
        "        for col_idx, (lab, y_gen) in enumerate(y_finals_np.items(),\n",
        "                                               start=int(plot_prior)):\n",
        "            gen_panel_norm = None  # computed in _draw if self_normalize else use shared\n",
        "            _draw(\n",
        "                axes[row_idx, col_idx],\n",
        "                y_gen, y_gen.shape[0],\n",
        "                lab,\n",
        "                draw_title=(row_idx == 0),\n",
        "                h_norm=h_ref,\n",
        "                likelihood_func=None,\n",
        "                plt_norm=gen_panel_norm\n",
        "            )\n",
        "\n",
        "        # compact y-label column (min_label mode)\n",
        "        if display_mode == 'min_label':\n",
        "            ax0 = axes[row_idx, 0]\n",
        "            label = f\"$d_{{{i+1}}}\\\\,v\\\\,d_{{{j+1}}}$\"\n",
        "            ax0.set_ylabel(label, rotation=90, labelpad=20,\n",
        "                           va='center', ha='center', fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def lambda_two_step(N, T_end, T_target):\n",
        "    # --- first step only ----------------\n",
        "    delta = T_target ** (1.0 / N)\n",
        "    dt0   = T_end * (1.0 - delta)\n",
        "\n",
        "    B0    = math.exp(2*dt0) - 1.0\n",
        "    D0    = B0 / math.exp(dt0)\n",
        "    R0    = (math.exp(dt0) - 1.0 - dt0) / (dt0*math.exp(dt0)-math.exp(dt0)+1)\n",
        "    lam1  = (R0 * D0) / (B0 + R0 * D0)\n",
        "\n",
        "    # --- two-step cancellation ----------\n",
        "    dt1   = dt0 * delta\n",
        "    def R(dt): return (math.exp(dt)-1-dt)/(dt*math.exp(dt)-math.exp(dt)+1)\n",
        "    def B(dt): return math.exp(2*dt)-1\n",
        "    def D(dt): return B(dt)/math.exp(dt)\n",
        "    lam2_num = R(dt0)*D(dt0)*dt0**2 + R(dt1)*D(dt1)*dt1**2\n",
        "    lam2_den = B(dt0)*dt0**2       + B(dt1)*dt1**2\n",
        "    lam2     = lam2_num / lam2_den\n",
        "\n",
        "    return .5 * (lam1+lam2)\n",
        "\n",
        "\n",
        "\n",
        "def power_grid(t0, tT, n, gamma=1.0):  # gamma<1 flattens big steps\n",
        "    frac = (tT / t0)\n",
        "    exps = (np.arange(n+1) / n) ** gamma\n",
        "    return t0 * frac ** exps\n",
        "\n",
        "\n",
        "# --- Get local Gaussian stats (mu_i, var_i+r) used by proxy Tweedie ----\n",
        "def local_gaussian_stats(x_ref, params=None):\n",
        "    \"\"\"\n",
        "    Returns (mu, var_r) from the same local Gaussian fit used in fit_kernel_score.\n",
        "      mu   : (N,D) weighted local means\n",
        "      var_r: (N,D) diagonal variances with ridge added\n",
        "    \"\"\"\n",
        "    if params is None:\n",
        "        params = {}\n",
        "\n",
        "    X = cp.asarray(x_ref)\n",
        "    N, D = X.shape\n",
        "    dtype = params.get(\"dtype\", cp.float32)\n",
        "    X = X.astype(dtype, copy=False)\n",
        "\n",
        "    # kNN with adaptive bandwidths (same as fit_kernel_score)\n",
        "    N0, k0, alpha = 2000, max(4*D, 48), 0.4\n",
        "    k_default = int(min(N-1, max(4*D, round(k0 * (X.shape[0]/N0) ** alpha))))\n",
        "    k = int(params.get(\"k\", k_default))\n",
        "    k = max(1, min(k, N - 1))\n",
        "\n",
        "    x2 = cp.sum(X * X, axis=1, keepdims=True)\n",
        "    dist2 = cp.maximum(x2 + x2.T - 2.0 * (X @ X.T), 0.0)\n",
        "    cp.fill_diagonal(dist2, cp.inf)\n",
        "    idx = cp.argpartition(dist2, kth=k - 1, axis=1)[:, :k]\n",
        "    d2_knn = cp.take_along_axis(dist2, idx, axis=1)\n",
        "    h2 = cp.maximum(cp.max(d2_knn, axis=1), cp.finfo(dtype).tiny)\n",
        "\n",
        "    w   = cp.exp(-d2_knn / (2.0 * h2[:, None]))\n",
        "    Xnb = X[idx]  # (N,k,D)\n",
        "    denom = cp.sum(w, axis=1, keepdims=True) + cp.finfo(dtype).eps\n",
        "    mu   = cp.sum(w[:, :, None] * Xnb, axis=1) / denom  # (N,D)\n",
        "    diff = Xnb - mu[:, None, :]\n",
        "    var  = cp.sum(w[:, :, None] * diff * diff, axis=1) / denom  # (N,D)\n",
        "\n",
        "    ridge_frac = float(params.get(\"ridge_frac\", 1e-3))\n",
        "    tau = ridge_frac * cp.mean(var, axis=1, keepdims=True)       # (N,1)\n",
        "    var_r = var + tau\n",
        "    return mu.astype(cp.float64), var_r.astype(cp.float64)\n",
        "\n",
        "\n",
        "\n",
        "def fit_kernel_score(x_ref, params=None):\n",
        "    \"\"\"\n",
        "    Local Gaussian (diagonal) kernel score fit, with optional GMM/KDE score recomputation.\n",
        "\n",
        "    Inputs:\n",
        "      x_ref   : (N, D) array-like (moved to GPU).\n",
        "      params  : dict (optional)\n",
        "        - k            : int, kNN size for local fit (default ~4*D, clipped to N-1)\n",
        "        - ridge_frac   : float, ridge as fraction of per-point mean var (default: 1e-2)\n",
        "        - dtype        : cp.float32 or cp.float64 (default: float32)\n",
        "        - recompute    : bool, if True return GMM/KDE score at anchors (default: False)\n",
        "        - gmm_batch    : int, component batch size for recompute (default: 1024)\n",
        "\n",
        "    Returns:\n",
        "      s_hat : (N, D) CuPy array.\n",
        "              If recompute=False: local score  s_i = (mu_i - x_i) / (var_i + ridge).\n",
        "              If recompute=True : GMM/KDE score s(x_i) = [∑_j N(x_i|mu_j,Σ_j) Σ_j^{-1}(mu_j-x_i)] / [∑_j N(x_i|mu_j,Σ_j)].\n",
        "    \"\"\"\n",
        "    if params is None:\n",
        "        params = {}\n",
        "\n",
        "    X = cp.asarray(x_ref)\n",
        "    N, D = X.shape\n",
        "    dtype = params.get(\"dtype\", cp.float32)\n",
        "    X = X.astype(dtype, copy=False)\n",
        "\n",
        "    # -------- Local fit (as before) -----------------------------------------\n",
        "    N0, k0, alpha = 2000, max(4*D, 48), 0.4\n",
        "    k_default = int(min(N-1, max(4*D, round(k0 * (X.shape[0]/N0) ** alpha))))\n",
        "    k = int(params.get(\"k\", k_default))\n",
        "    k = max(1, min(k, N - 1))\n",
        "\n",
        "    # Pairwise squared distances (self-excluded by inf on diag)\n",
        "    x2 = cp.sum(X * X, axis=1, keepdims=True)\n",
        "    dist2 = x2 + x2.T - 2.0 * (X @ X.T)\n",
        "    dist2 = cp.maximum(dist2, 0)\n",
        "    cp.fill_diagonal(dist2, cp.inf)\n",
        "\n",
        "    # kNN\n",
        "    idx = cp.argpartition(dist2, kth=k - 1, axis=1)[:, :k]\n",
        "    d2_knn = cp.take_along_axis(dist2, idx, axis=1)\n",
        "\n",
        "    # Adaptive bandwidth per point\n",
        "    h2 = cp.maximum(cp.max(d2_knn, axis=1), cp.finfo(dtype).tiny)\n",
        "\n",
        "    # RBF weights with per-point bandwidth\n",
        "    w = cp.exp(-d2_knn / (2.0 * h2[:, None]))\n",
        "\n",
        "    # Neighbor points\n",
        "    X_nb = X[idx]                            # (N, k, D)\n",
        "\n",
        "    # Weighted mean\n",
        "    denom = cp.sum(w, axis=1, keepdims=True) + cp.finfo(dtype).eps\n",
        "    mu = cp.sum(w[:, :, None] * X_nb, axis=1) / denom   # (N, D)\n",
        "\n",
        "    # Weighted diagonal covariance\n",
        "    diff = X_nb - mu[:, None, :]                        # (N, k, D)\n",
        "    var = cp.sum(w[:, :, None] * diff * diff, axis=1) / denom  # (N, D)\n",
        "\n",
        "    # Ridge\n",
        "    ridge_frac = float(params.get(\"ridge_frac\", 1e-3))\n",
        "    tau = ridge_frac * cp.mean(var, axis=1, keepdims=True)      # (N, 1)\n",
        "    var_r = var + tau                                           # (N, D)\n",
        "\n",
        "    # Local score at anchors (diagonal precision)\n",
        "    s_local = (mu - X) / var_r                                  # (N, D)\n",
        "\n",
        "    # -------- Optional recomputation: KDE/GMM score at each x_i -------------\n",
        "    if not params.get(\"recompute\", False):\n",
        "        return s_local\n",
        "\n",
        "    # We treat {mu_j, var_r_j} as a uniform GMM; compute s(x_i) for all i.\n",
        "    # log N(x | mu_j, diag(var_j)) = -0.5 * [sum_d log(2π var_jd) + sum_d (x_d - mu_jd)^2 / var_jd]\n",
        "    # Use two passes with log-sum-exp for stability and batch over components j.\n",
        "\n",
        "    B = int(params.get(\"gmm_batch\", 1024))\n",
        "    two_pi = dtype(2.0 * np.pi)\n",
        "\n",
        "    # Precompute per-component constants: logdet terms\n",
        "    # (We’ll slice them inside the loop)\n",
        "    logdet = cp.sum(cp.log(two_pi * var_r), axis=1)   # (N,)\n",
        "\n",
        "    # Pass 1: find per-i maxima of log weights over all j\n",
        "    m = cp.full((N,), -cp.inf, dtype=dtype)\n",
        "    for j0 in range(0, N, B):\n",
        "        jb = slice(j0, min(j0 + B, N))\n",
        "        mu_j = mu[jb]          # (B, D)\n",
        "        var_j = var_r[jb]      # (B, D)\n",
        "        logdet_j = logdet[jb]  # (B,)\n",
        "\n",
        "        # (N, B, D)\n",
        "        dX = X[:, None, :] - mu_j[None, :, :]\n",
        "        quad = cp.sum(dX * dX / var_j[None, :, :], axis=2)      # (N, B)\n",
        "        logw = -0.5 * (quad + logdet_j[None, :])                # (N, B)\n",
        "\n",
        "        m = cp.maximum(m, cp.max(logw, axis=1))                 # (N,)\n",
        "\n",
        "    # Pass 2: accumulate numerator and denominator in linear space\n",
        "    den = cp.zeros((N,), dtype=dtype)\n",
        "    num = cp.zeros((N, D), dtype=dtype)\n",
        "    eps = cp.finfo(dtype).eps\n",
        "\n",
        "    for j0 in range(0, N, B):\n",
        "        jb = slice(j0, min(j0 + B, N))\n",
        "        mu_j = mu[jb]             # (B, D)\n",
        "        var_j = var_r[jb]         # (B, D)\n",
        "        logdet_j = logdet[jb]     # (B,)\n",
        "\n",
        "        dX = X[:, None, :] - mu_j[None, :, :]                    # (N, B, D)\n",
        "        quad = cp.sum(dX * dX / var_j[None, :, :], axis=2)       # (N, B)\n",
        "        logw = -0.5 * (quad + logdet_j[None, :])                 # (N, B)\n",
        "        w_nb = cp.exp(logw - m[:, None])                         # (N, B)\n",
        "\n",
        "        # Component scores at x_i: Σ_j^{-1}(μ_j - x_i) = -(x_i - μ_j)/var_j\n",
        "        comp_score = -dX / var_j[None, :, :]                     # (N, B, D)\n",
        "\n",
        "        # Weighted sums over components in this block\n",
        "        num += cp.einsum('nb,nbd->nd', w_nb, comp_score)         # (N, D)\n",
        "        den += cp.sum(w_nb, axis=1)                              # (N,)\n",
        "\n",
        "    s_gmm = num / (den[:, None] + eps)                           # (N, D)\n",
        "\n",
        "    return s_gmm\n",
        "\n",
        "\n",
        "def KSE_diff(X_ref, target_score_func):\n",
        "   proxy_score = fit_kernel_score(X_ref.get())\n",
        "   real_score = target_score_func(X_ref)\n",
        "   diff = cp.square(proxy_score - real_score)\n",
        "   return cp.asnumpy(cp.sqrt(cp.mean(diff)).get())\n",
        "\n",
        "\n",
        "\n",
        "def fit_kernel_score_SVD(x_ref, params=None):\n",
        "    \"\"\"\n",
        "    Option A: Local low-rank-plus-diag precision (rank-r PCA/Woodbury).\n",
        "\n",
        "    params:\n",
        "      - k              : int, kNN size (default ~4*D, clipped to N-1)\n",
        "      - rank           : int, PCA rank r (default: min(8, D))\n",
        "      - dtype          : cp.float32 or cp.float64 (default: float32)\n",
        "      - lam_clip_mult  : float, cap λ_i ≤ lam_clip_mult * τ (default: 1e3)\n",
        "      - # --- new / clarified ridge controls ---\n",
        "      - ridge_mode     : {\"tail_mean\",\"tail_trimmed\",\"tail_median\"} (default: \"tail_mean\")\n",
        "      - tail_trim_q    : float in [0,0.5], trim frac for \"tail_trimmed\" (default: 0.1)\n",
        "      - ridge_scale    : float, scales τ_i from tail (default: 1.0)\n",
        "      - ridge_floor    : float, min τ_i (default: 1e-6)\n",
        "      - # --- recompute controls ---\n",
        "      - recompute      : bool, if True return KDE/GMM score using low-rank Σ_j (default: False)\n",
        "      - gmm_batch      : int, component batch size for recompute (default: 128)\n",
        "    \"\"\"\n",
        "    if params is None:\n",
        "        params = {}\n",
        "\n",
        "    # -------- defaults (add/ensure these lines) --------\n",
        "    params.setdefault(\"recompute\", False)\n",
        "    params.setdefault(\"gmm_batch\", 128)\n",
        "    params.setdefault(\"ridge_mode\", \"tail_mean\")\n",
        "    params.setdefault(\"tail_trim_q\", 0.1)\n",
        "    params.setdefault(\"ridge_scale\", 1.0)\n",
        "    params.setdefault(\"ridge_floor\", 1e-6)\n",
        "    params.setdefault(\"lam_clip_mult\", 1e3)\n",
        "    params.setdefault(\"rank\", min(8, x_ref.shape[1]))\n",
        "    # ---------------------------------------------------\n",
        "\n",
        "    X = cp.asarray(x_ref)\n",
        "    N, D = X.shape\n",
        "    dtype = params.get(\"dtype\", cp.float32)\n",
        "    X = X.astype(dtype, copy=False)\n",
        "\n",
        "    # ---------------- kNN and weights (same as before) ----------------\n",
        "    N0, k0, alpha = 2000, max(4*D, 48), 0.4\n",
        "    k_default = int(min(N-1, max(4*D, round(k0 * (X.shape[0]/N0) ** alpha))))\n",
        "    k = int(params.get(\"k\", k_default))\n",
        "    k = max(1, min(k, N - 1))\n",
        "\n",
        "    # Pairwise squared distances (self-excluded)\n",
        "    x2 = cp.sum(X * X, axis=1, keepdims=True)\n",
        "    dist2 = x2 + x2.T - 2.0 * (X @ X.T)\n",
        "    dist2 = cp.maximum(dist2, 0)\n",
        "    cp.fill_diagonal(dist2, cp.inf)\n",
        "\n",
        "    # kNN indices + distances\n",
        "    idx = cp.argpartition(dist2, kth=k - 1, axis=1)[:, :k]\n",
        "    d2_knn = cp.take_along_axis(dist2, idx, axis=1)\n",
        "\n",
        "    # Adaptive bandwidth and RBF weights\n",
        "    h2 = cp.maximum(cp.max(d2_knn, axis=1), cp.finfo(dtype).tiny)\n",
        "    w = cp.exp(-d2_knn / (2.0 * h2[:, None]))                   # (N, k)\n",
        "    X_nb = X[idx]                                                # (N, k, D)\n",
        "\n",
        "    # Weighted mean μ_i\n",
        "    denom = cp.sum(w, axis=1, keepdims=True) + cp.finfo(dtype).eps\n",
        "    mu = cp.sum(w[:, :, None] * X_nb, axis=1) / denom           # (N, D)\n",
        "\n",
        "    # Diagonal variances (only to set τ robustly)\n",
        "    diff = X_nb - mu[:, None, :]                                 # (N, k, D)\n",
        "    var = cp.sum(w[:, :, None] * diff * diff, axis=1) / denom    # (N, D)\n",
        "\n",
        "    # Ridge floor τ_i (scalar per point)\n",
        "    ridge_frac = float(params.get(\"ridge_frac\", 1e-3))\n",
        "    tau = ridge_frac * cp.mean(var, axis=1, keepdims=True)       # (N, 1)\n",
        "    eps = cp.finfo(dtype).eps\n",
        "\n",
        "    # ---------------- Local score at anchors via rank-r PCA ----------------\n",
        "    r = int(params.get(\"rank\", min(8, D)))\n",
        "    lam_clip_mult = float(params.get(\"lam_clip_mult\", 1e3))\n",
        "\n",
        "    ridge_mode   = params.get(\"ridge_mode\", \"tail_mean\")  # \"tail_mean\" | \"tail_trimmed\" | \"tail_median\"\n",
        "    tail_trim_q  = float(params.get(\"tail_trim_q\", 0.1))  # top fraction of tail to drop (0.0–0.5)\n",
        "    ridge_scale  = float(params.get(\"ridge_scale\", 1.0))  # multiply the tail estimate\n",
        "    ridge_floor  = float(params.get(\"ridge_floor\", 1e-6)) # absolute min tau\n",
        "    eps = cp.finfo(dtype).eps\n",
        "\n",
        "    s_local = cp.empty_like(X)\n",
        "    def _estimate_tau_from_tail(S, r_eff):\n",
        "        \"\"\"Estimate tau_i from singular values S of R_i (so λ = S^2).\"\"\"\n",
        "        if S.shape[0] <= r_eff:\n",
        "            return ridge_floor\n",
        "        lam_tail = (S[r_eff:] * S[r_eff:])  # λ_j for j>r\n",
        "        if ridge_mode == \"tail_trimmed\":\n",
        "            # drop the largest tail_trim_q fraction of the tail (robust to a few structured dirs)\n",
        "            m = lam_tail.shape[0]\n",
        "            if m > 2:\n",
        "                kdrop = int(max(1, min(m-1, round(tail_trim_q * m))))\n",
        "                lam_tail_sorted = cp.sort(lam_tail)\n",
        "                lam_tail = lam_tail_sorted[:m - kdrop]\n",
        "            tau_i = cp.mean(lam_tail)\n",
        "        elif ridge_mode == \"tail_median\":\n",
        "            tau_i = cp.median(lam_tail)\n",
        "        else:  # \"tail_mean\"\n",
        "            tau_i = cp.mean(lam_tail)\n",
        "        tau_i = float(ridge_scale) * float(tau_i)\n",
        "        tau_i = float(max(tau_i, ridge_floor))\n",
        "        return tau_i\n",
        "\n",
        "    def _apply_precision_single(v, R, r_eff, Vh, S, tau_i):\n",
        "        \"\"\"Compute (tau I + U Λ U^T)^(-1) v using top r_eff of Vh,S and scalar tau_i.\"\"\"\n",
        "        if r_eff == 0 or Vh is None or S is None:\n",
        "            return v / (tau_i + eps)\n",
        "        V_r = Vh[:r_eff, :].T\n",
        "        lam = S[:r_eff] * S[:r_eff]\n",
        "        lam = cp.minimum(lam, lam_clip_mult * tau_i)\n",
        "        coef = V_r.T @ v\n",
        "        scale = lam / (tau_i * (tau_i + lam) + eps)\n",
        "        corr = V_r @ (scale * coef)\n",
        "        return (v / (tau_i + eps)) - corr\n",
        "\n",
        "    # -------- Local score with per-point tau_i from SVD tail --------\n",
        "    taus = cp.empty((N,), dtype=dtype)   # keep if you want to inspect τ_i later\n",
        "    for i in range(N):\n",
        "        v_i = (mu[i] - X[i])                              # (D,)\n",
        "        sw = cp.sqrt(w[i] / (denom[i, 0] + eps))          # (k,)\n",
        "        R_i = sw[:, None] * (X_nb[i] - mu[i])             # (k, D)\n",
        "        try:\n",
        "            _, S, Vh = cp.linalg.svd(R_i, full_matrices=False)\n",
        "            r_eff = int(min(r, Vh.shape[0]))\n",
        "        except cp.linalg.LinAlgError:\n",
        "            S, Vh, r_eff = None, None, 0\n",
        "\n",
        "        # --- NEW: τ_i from the discarded spectrum ---\n",
        "        tau_i = _estimate_tau_from_tail(S, r_eff) if S is not None else ridge_floor\n",
        "        taus[i] = tau_i\n",
        "\n",
        "        s_local[i] = _apply_precision_single(v_i, R_i, r_eff, Vh, S, tau_i)\n",
        "\n",
        "    if not params.get(\"recompute\", False):\n",
        "        return s_local\n",
        "\n",
        "    # ---------------- Recompute path: reuse per-component τ_j and (V, λ) --------------\n",
        "    B = int(params.get(\"gmm_batch\", 128))\n",
        "    two_pi = dtype(2.0 * np.pi)\n",
        "    m = cp.full((N,), -cp.inf, dtype=dtype)\n",
        "    den = cp.zeros((N,), dtype=dtype)\n",
        "    num = cp.zeros((N, D), dtype=dtype)\n",
        "\n",
        "    for j0 in range(0, N, B):\n",
        "        jb = slice(j0, min(j0 + B, N))\n",
        "        Bcur = jb.stop - jb.start\n",
        "\n",
        "        mu_j  = mu[jb]                      # (B, D)\n",
        "        tau_j = taus[jb]                    # (B,)\n",
        "\n",
        "        V_batch = cp.zeros((Bcur, D, r), dtype=dtype)\n",
        "        lam_batch = cp.zeros((Bcur, r), dtype=dtype)\n",
        "        r_effs = np.empty(Bcur, dtype=np.int32)\n",
        "        logdet_const = cp.empty((Bcur,), dtype=dtype)\n",
        "\n",
        "        for b in range(Bcur):\n",
        "            j = j0 + b\n",
        "            sw = cp.sqrt(w[j] / (denom[j, 0] + eps))\n",
        "            R_j = sw[:, None] * (X_nb[j] - mu[j])\n",
        "            try:\n",
        "                _, S, Vh = cp.linalg.svd(R_j, full_matrices=False)\n",
        "                r_eff = int(min(r, Vh.shape[0]))\n",
        "            except cp.linalg.LinAlgError:\n",
        "                S, Vh, r_eff = None, None, 0\n",
        "\n",
        "            # IMPORTANT: recompute τ_j from this component’s tail as well\n",
        "            tau_comp = _estimate_tau_from_tail(S, r_eff) if S is not None else float(tau_j[b])\n",
        "            tau_j[b] = tau_comp\n",
        "\n",
        "            r_effs[b] = r_eff\n",
        "            if r_eff > 0:\n",
        "                V_r = Vh[:r_eff, :].T\n",
        "                lam = S[:r_eff] * S[:r_eff]\n",
        "                lam = cp.minimum(lam, lam_clip_mult * tau_comp)\n",
        "                V_batch[b, :, :r_eff] = V_r\n",
        "                lam_batch[b, :r_eff] = lam\n",
        "                logdet_const[b] = D * cp.log(two_pi) + (D - r_eff) * cp.log(tau_comp + eps) \\\n",
        "                                  + cp.sum(cp.log(tau_comp + lam + eps))\n",
        "            else:\n",
        "                logdet_const[b] = D * cp.log(two_pi) + D * cp.log(tau_comp + eps)\n",
        "\n",
        "        dX = X[:, None, :] - mu_j[None, :, :]\n",
        "        T = cp.einsum('nbd,bdr->nbr', dX, V_batch)\n",
        "        scale = lam_batch / (tau_j[:, None] * (tau_j[:, None] + lam_batch) + eps)\n",
        "        term = cp.einsum('nbr,bdr->nbd', T * scale[None, :, :], V_batch)\n",
        "        u = dX / (tau_j[None, :, None] + eps) - term\n",
        "\n",
        "        quad = cp.sum(dX * u, axis=2)\n",
        "        logw = -0.5 * (quad + logdet_const[None, :])\n",
        "        comp_score = -u\n",
        "\n",
        "        new_m = cp.maximum(m, cp.max(logw, axis=1))\n",
        "        scale_old = cp.exp(m - new_m)\n",
        "        W = cp.exp(logw - new_m[:, None])\n",
        "\n",
        "        num = num * scale_old[:, None] + cp.einsum('nb,nbd->nd', W, comp_score)\n",
        "        den = den * scale_old + cp.sum(W, axis=1)\n",
        "        m = new_m\n",
        "\n",
        "    s_gmm = num / (den[:, None] + eps)\n",
        "    return s_gmm\n",
        "\n",
        "\n",
        "def run_comparison(\n",
        "    samplers, steps_list,  true_sampler_func,\n",
        "    prior_sampler_func, prior_score_func=None, prior_score_div_func = None,\n",
        "    likelyhood_func = None, loglik_grad_fn = None,\n",
        "    post_sampler_func = None, post_score_func = None, post_score_div_func = None,\n",
        "    true_score_func=None,               # NEW: oracle score function (y, t) -> score\n",
        "    true_init_score = None,\n",
        "    track_score_rmse=False,             # NEW: whether to compute RMSE vs oracle\n",
        "    N_ref=1000, N_part=2000, batch=1000,\n",
        "    trials=20, nrows=3, trial_name='trial', div='M_KSD',\n",
        "    T_end=1.5, T_target=1e-3, time_split='power', self_normalize_hists = False,\n",
        "    plot_hists=True, csv_path=None, hist_mode='first', display_mode='standard',\n",
        "    ref_seed='rand', plot_res=False, d_pairs=None, pre_process_f=None, plot_post = False,\n",
        "    mean_bins = 80, best_bins = 60, p_prune = 0, plot_prior = True, hist_norm = 1.25,\n",
        "    draw_countours = False, prior_scale = True, save_tag = '', r = 0, post_hist_norm = 1.0,\n",
        "    w_correct_func = None, comparison_samples = None, comparison_label = None,\n",
        "    rand_seed = False, ridge_frac = 3e-3, cov_rank = 3, compare_keys = None):\n",
        "\n",
        "\n",
        "    if len(save_tag):\n",
        "       print_tag = f'{save_tag}, '\n",
        "    else:\n",
        "       print_tag = ''\n",
        "\n",
        "\n",
        "    results = {f'{label}': []  for (_, _, label, *_) in samplers}\n",
        "    samples_dict =  {f'{label}': []  for (_, _, label, *_) in samplers}\n",
        "    divs_dict = {f'{label}': []  for (_, _, label, *_) in samplers}\n",
        "\n",
        "\n",
        "    #results = {f'{steps}_dict': {} for steps in steps_list}\n",
        "\n",
        "    #best_samples_dict = {f'{label}': []  for (_, _, label, *_) in samplers\n",
        "\n",
        "\n",
        "\n",
        "    if loglik_grad_fn is None:                 # Indication we arent doing any posterior\n",
        "        post_sampler_func = true_sampler_func\n",
        "        post_score_func = true_init_score\n",
        "        likelyhood_func = None\n",
        "\n",
        "    if true_init_score == None:\n",
        "      div = 'W2'\n",
        "      true_init_score = prior_score_func\n",
        "\n",
        "    clip_plot = False\n",
        "    floors = []\n",
        "\n",
        "    if track_score_rmse and true_score_func is not None:\n",
        "        score_rmse = {label: [] for  (_, _, label, *_) in samplers}\n",
        "\n",
        "\n",
        "    for steps in steps_list:\n",
        "        delta = T_target**(1/steps)\n",
        "\n",
        "        if time_split == 'exp':\n",
        "            times = T_end * (delta**np.arange(steps+1))\n",
        "\n",
        "\n",
        "        elif time_split == 'power':\n",
        "            gamma = min(1.0, 0.15*steps)   # adaptive γ\n",
        "            times = power_grid(T_end, T_target, steps, gamma)\n",
        "\n",
        "        else:\n",
        "            times = np.linspace(T_end, T_target, steps+1)\n",
        "\n",
        "\n",
        "        h_coeff = min(lambda_two_step(steps, T_end, T_target), .5)\n",
        "\n",
        "        output_dict = {}\n",
        "\n",
        "        for k , (sampler_tuple) in enumerate(samplers):\n",
        "\n",
        "            if len(sampler_tuple) == 3:\n",
        "                mode, score_mode, label = sampler_tuple\n",
        "                custom_callable = None\n",
        "            else:\n",
        "                mode, score_mode, label, custom_callable = sampler_tuple\n",
        "\n",
        "            per_trial_rmse = []\n",
        "            if ('heun' in label.lower()) and steps > steps_list[-1]/2:\n",
        "                #min_div =  results[f'{label}_min'][-1]\n",
        "                avg_div = results[f'{label}'][-1]\n",
        "                print_res = False\n",
        "                clip_plot = True\n",
        "\n",
        "            else:\n",
        "\n",
        "                print_res = True\n",
        "                divs = []\n",
        "                t0 = time.time()\n",
        "\n",
        "                for j in range(trials):\n",
        "                    #test_samples = sampler_func(N_part)\n",
        "                    if rand_seed:\n",
        "                      ref_seed = j\n",
        "                    #test_samples = OU_evolve_samples(post_sampler_func(N_part), T_target)\n",
        "                    test_samples = post_sampler_func(N_part)\n",
        "\n",
        "\n",
        "                    if score_mode == 'tweedie':\n",
        "                        out = tweedie_sampler(N_part, N_ref, times, batch, prior_sampler_func, mode = mode, likelyhood_func = likelyhood_func,\n",
        "                                              true_score_func=true_score_func, h_coeff = h_coeff,\n",
        "                                              seed = ref_seed, w_correct_func = w_correct_func)\n",
        "\n",
        "                    elif score_mode == 'kss':\n",
        "                        out = kss_sampler(N_part, N_ref, times, batch, prior_sampler_func, prior_score_func, mode = mode,\n",
        "                                          likelyhood_func = likelyhood_func, loglik_grad_fn = loglik_grad_fn, true_score_func=true_score_func,\n",
        "                                          h_coeff = h_coeff, seed = ref_seed,  w_correct_func = w_correct_func)\n",
        "\n",
        "\n",
        "                    elif score_mode == 'blend':\n",
        "                        out = blend_sampler(N_part, N_ref, times, batch, prior_sampler_func, true_init_score, mode = mode,\n",
        "                                            likelyhood_func = likelyhood_func, loglik_grad_fn = loglik_grad_fn, weight_mode='snis',\n",
        "                                            seed = ref_seed, h_coeff = h_coeff, true_score_func=true_score_func, w_correct_func = w_correct_func)\n",
        "\n",
        "\n",
        "                    elif score_mode == 'blend_proxy':\n",
        "                        proxy_prior_score = lambda x: fit_kernel_score(x, params = {\"ridge_frac\": ridge_frac, \"recompute\": False} )\n",
        "                        out = blend_sampler(N_part, N_ref, times, batch, prior_sampler_func, proxy_prior_score,  mode = mode,\n",
        "                                            likelyhood_func = likelyhood_func, loglik_grad_fn = loglik_grad_fn, weight_mode='snis',\n",
        "                                            seed = ref_seed, h_coeff = h_coeff, true_score_func=true_score_func, w_correct_func = w_correct_func)\n",
        "\n",
        "\n",
        "                    elif score_mode == 'blend_proxy_SVD':\n",
        "                        params = dict(rank=cov_rank,ridge_mode=\"tail_trimmed\", tail_trim_q=0.1, ridge_scale=1.0,\n",
        "                                      ridge_floor=1e-6, recompute = False, gmm_batch=128)\n",
        "                        proxy_prior_score = lambda x: fit_kernel_score_SVD(x, params = params)\n",
        "                        out = blend_sampler(N_part, N_ref, times, batch, prior_sampler_func, proxy_prior_score,  mode = mode,\n",
        "                                            likelyhood_func = likelyhood_func, loglik_grad_fn = loglik_grad_fn, weight_mode='snis',\n",
        "                                             seed = ref_seed, h_coeff = h_coeff, true_score_func=true_score_func, w_correct_func = w_correct_func)\n",
        "\n",
        "                    elif score_mode == 'blend_proxy_SVD_recomp':\n",
        "                        params = dict(rank=cov_rank,ridge_mode=\"tail_trimmed\", tail_trim_q=0.1, ridge_scale=1.0,\n",
        "                                      ridge_floor=1e-6, recompute = True, gmm_batch=128)\n",
        "                        proxy_prior_score = lambda x: fit_kernel_score_SVD(x, params = params)\n",
        "                        out = blend_sampler(N_part, N_ref, times, batch, prior_sampler_func, proxy_prior_score,  mode = mode,\n",
        "                                            likelyhood_func = likelyhood_func, loglik_grad_fn = loglik_grad_fn, weight_mode='snis',\n",
        "                                             seed = ref_seed, h_coeff = h_coeff, true_score_func=true_score_func, w_correct_func = w_correct_func)\n",
        "\n",
        "\n",
        "                    elif score_mode == 'blend_proxy_recomp':\n",
        "                        proxy_prior_score = lambda x: fit_kernel_score(x, params = {\"ridge_frac\": ridge_frac, \"recompute\": True} )\n",
        "                        out = blend_sampler(N_part, N_ref, times, batch, prior_sampler_func, proxy_prior_score, mode = mode,\n",
        "                                            likelyhood_func = likelyhood_func, loglik_grad_fn = loglik_grad_fn, weight_mode='snis',\n",
        "                                            seed = ref_seed, h_coeff = h_coeff, true_score_func=true_score_func, w_correct_func = w_correct_func)\n",
        "\n",
        "\n",
        "                    elif score_mode == 'custom':\n",
        "                        out = blackbox_sampler( N_part, times, custom_callable, prior_sampler_func, mode = mode,\n",
        "                                               likelyhood_func = likelyhood_func, loglik_grad_fn = loglik_grad_fn,\n",
        "                                               h_coeff=h_coeff, true_score_func=true_score_func, p_prune = p_prune)\n",
        "\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unknown score mode {score_mode}\")\n",
        "\n",
        "\n",
        "\n",
        "                    if track_score_rmse and true_score_func:\n",
        "                        output_samples, rmse_list = out\n",
        "                        per_trial_rmse.append(np.mean(rmse_list))\n",
        "                    else:\n",
        "                        output_samples = out\n",
        "\n",
        "                    purned_output_samples,_ = prune_cp_arr(output_samples, p_percent = p_prune)\n",
        "                    #test_samples, _ = prune_cp_arr(test_samples, p_percent = p_prune)\n",
        "                    if div == 'W2':\n",
        "                        divs.append(sliced_wasserstein2(test_samples,  purned_output_samples, n_proj=2048, max_pts=5000))\n",
        "                    elif div == 'M_KSD':\n",
        "                      divs.append(compute_multiscale_ksd( purned_output_samples, post_score_func))\n",
        "                     # divs.append(compute_multiscale_ksd(cp.concatenate((output_samples, test_samples), axis = 0), post_score_func))\n",
        "                    elif div == 'M_MMD':\n",
        "                        divs.append(M_MMD(test_samples,  purned_output_samples))\n",
        "                    elif div == 'OT_MMD':\n",
        "                        divs.append(OT_MMD(test_samples,  purned_output_samples))\n",
        "                    elif div == 'KSE_diff':\n",
        "                        divs.append(KSE_diff( purned_output_samples, post_score_func))\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unknown div {div}\")\n",
        "\n",
        "\n",
        "                    if label not in output_dict.keys():\n",
        "                      output_dict[label] = output_samples\n",
        "\n",
        "                    else:\n",
        "                      output_dict[label] = np.concatenate((output_dict[label], output_samples))\n",
        "\n",
        "                avg_div = np.mean(divs)\n",
        "                divs_dict[label].append(avg_div)\n",
        "\n",
        "                samples_dict[label].append(output_dict[label])\n",
        "\n",
        "\n",
        "                if track_score_rmse and true_score_func:\n",
        "                    avg_rmse = np.mean(per_trial_rmse)\n",
        "                    score_rmse[label].append(avg_rmse)\n",
        "\n",
        "\n",
        "\n",
        "            floor_samples = post_sampler_func(N_part) #OU_evolve_samples(post_sampler_func(N_part), T_target)\n",
        "            #floor_samples, _ = prune_cp_arr(floor_samples, p_percent = p_prune)\n",
        "\n",
        "            if div == 'W2':\n",
        "                  floors.append(sliced_wasserstein2(floor_samples, test_samples))\n",
        "                  if comparison_samples is not None:\n",
        "                    compare_div = sliced_wasserstein2(comparison_samples,test_samples)\n",
        "            elif div == 'M_KSD':\n",
        "                  floors.append(compute_multiscale_ksd(floor_samples, post_score_func))\n",
        "                  if comparison_samples is not None:\n",
        "                    compare_div = compute_multiscale_ksd(comparison_samples, post_score_func)\n",
        "            elif div == 'M_MMD':\n",
        "                  floors.append(M_MMD(floor_samples, test_samples))\n",
        "                  if comparison_samples is not None:\n",
        "                    compare_div = M_MMD(comparison_samples,test_samples)\n",
        "            elif div == 'OT_MMD':\n",
        "                  floors.append(OT_MMD(floor_samples, test_samples))\n",
        "                  if comparison_samples is not None:\n",
        "                    compare_div = OT_MMD(comparison_samples,test_samples)\n",
        "            elif div == 'KSE_diff':\n",
        "                  floors.append(KSE_diff(floor_samples, post_score_func))\n",
        "                  if comparison_samples is not None:\n",
        "                    compare_div = KSE_diff(comparison_samples, post_score_func)\n",
        "            else:\n",
        "                  raise ValueError(f\"Unknown div {div}\")\n",
        "\n",
        "            if comparison_label is not None:\n",
        "              compare_str = f' {comparison_label}: {div} = {compare_div:.3e},'\n",
        "              output_dict['comparison_label'] = comparison_samples\n",
        "            else:\n",
        "              compare_str = ''\n",
        "\n",
        "\n",
        "            #results[f'{label}'].append(avg_div)\n",
        "\n",
        "            if len(save_tag):\n",
        "              print_tag = f'{save_tag}, '\n",
        "            else:\n",
        "              print_tag = ''\n",
        "\n",
        "            mean_floor = np.mean(floors)\n",
        "            divs_dict['floor'] = mean_floor\n",
        "\n",
        "            if print_res:\n",
        "              if track_score_rmse and true_score_func:\n",
        "                print(f\"{print_tag}{steps:2d} steps, {label}: {div} ={avg_div:.3e}, {div}_floor = {mean_floor:.3e}, score_RMSE={avg_rmse:.3e}, time={time.time()-t0:.1f}s\")\n",
        "                #print(f\"{print_tag}{steps:2d} steps, {label}: {div} ={avg_div:.3e},{compare_str} {div}_floor = {np.mean(floors):.3e}, time={time.time()-t0:.1f}s\")\n",
        "                print(' ')\n",
        "              else:\n",
        "                print(f\"{print_tag}{steps:2d} steps, {label}: {div} ={avg_div:.3e},{compare_str} {div}_floor = {np.mean(floors):.3e}, time={time.time()-t0:.1f}s\")\n",
        "                print(' ')\n",
        "\n",
        "\n",
        "        #true_prior_samples = OU_evolve_samples(true_sampler_func(trials * N_part), T_target)\n",
        "        true_prior_samples = true_sampler_func(trials * N_part)\n",
        "\n",
        "        if plot_hists:\n",
        "          if likelyhood_func is not None:\n",
        "           #post_samples = OU_evolve_samples(post_sampler_func(trials * N_part, debug = True), T_target)\n",
        "            post_samples = post_sampler_func(trials * N_part)\n",
        "            #post_samples, _ = prune_cp_arr(post_samples, p_percent = p_prune)\n",
        "          else:\n",
        "            post_samples = None\n",
        "\n",
        "          #true_prior_samples, _ = prune_cp_arr(true_prior_samples, p_percent = p_prune)\n",
        "          plot_pair_histograms(output_dict, true_prior_samples, post_samples, save_path=f\"{save_tag}_{steps}_steps_mean_hist.png\",\n",
        "                               bins = mean_bins, nrows = nrows,  mode = hist_mode, display_mode = display_mode, hist_norm = hist_norm,\n",
        "                               dim_pairs = d_pairs, pre_process_f = pre_process_f, likelyhood_func = likelyhood_func,\n",
        "                               draw_countours = draw_countours, prior_scale = prior_scale, compare_keys = compare_keys,\n",
        "                               plot_prior = plot_prior, post_hist_norm = post_hist_norm, plot_post = plot_post, self_normalize = self_normalize_hists)\n",
        "\n",
        "        results[f'{steps}_dict'] = (output_dict, divs_dict)\n",
        "\n",
        "    if plot_res:\n",
        "      plt.figure(figsize=(6,4))\n",
        "      colors = ['blue', 'red', 'green', 'black',\n",
        "                'orange', 'purple','pink', 'yellow', 'grey']\n",
        "\n",
        "      floor = np.mean(floors)\n",
        "      print(f\"Floor={floor:.4e}\")\n",
        "\n",
        "\n",
        "      max_avg = -np.inf\n",
        "      for i, (_, _, sampler, *_) in enumerate(samplers[:len(colors)]):\n",
        "        if ('heun' in sampler.lower()):\n",
        "            NFEs = 2 * np.asarray(steps_list)\n",
        "        else:\n",
        "            NFEs =  np.asarray(steps_list)\n",
        "\n",
        "        plt.plot(NFEs , np.log(results[f'{sampler}']), marker='o',\n",
        "                label = f'{sampler}', color = colors[i])\n",
        "\n",
        "        if np.max(np.log(results[f'{sampler}'])) >max_avg:\n",
        "            max_avg = np.max(np.log(results[f'{sampler}']))\n",
        "\n",
        "\n",
        "      plt.plot(NFEs, np.log(np.asarray([floor for NFE in NFEs])),\n",
        "               label = 'floor', color = 'black', linestyle = 'dashed')\n",
        "\n",
        "\n",
        "      plt.xlabel(\"NFE\", fontsize=11)\n",
        "\n",
        "\n",
        "      if clip_plot:\n",
        "        plt.xlim(left = 2 *  steps_list[0], right = steps_list[-1])\n",
        "\n",
        "      if div == 'W2':\n",
        "        plt.ylabel(f\" Mean Log Wasserstein\", fontsize=13)\n",
        "      elif div == 'M_KSD':\n",
        "        plt.ylabel(f\" Mean Log M-KSD\", fontsize=13)\n",
        "\n",
        "      plt.title(f\"Sampler Comparison\", fontsize=13)\n",
        "      plt.legend(loc='upper right')\n",
        "      plt.grid(ls='--', alpha=0.4)\n",
        "      #plt.ylim(bottom = floor - 1,  top = min(max_avg + 2,5))\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(f'{trial_name}.png')\n",
        "      plt.show()\n",
        "\n",
        "    samples_dict['True prior'] = true_prior_samples\n",
        "    try:\n",
        "      samples_dict['True posterior'] = OU_evolve_samples(post_sampler_func(trials * N_part), T_target)\n",
        "    except Exception:\n",
        "      print('There was an issue big dawg')\n",
        "\n",
        "\n",
        "    results['samples_dict'] = samples_dict\n",
        "    results['divs_dict'] = divs_dict\n",
        "    if track_score_rmse and true_score_func:\n",
        "        results['score_rmse'] = score_rmse\n",
        "    return results\n",
        "\n",
        "\n",
        "# ─── 4. MAIN SWEEP ─────────────────────────────────────────────────────────────\n",
        "\n",
        "def main1(N_REF = 500, plot_hists = False, do_inv = False,\n",
        "          div = 'W2',save_tag = '', ridge_frac = 3e-3, trials = 5):\n",
        "    ################################################################################################################################################################################\n",
        "    SEED  = 2\n",
        "    NUM_C, K_DIM, M_DIM, = 6000, 12, 3\n",
        "    VARIANT, STD, SCALE, norm_size = \"helix\", .12, 3.0, 1.0\n",
        "    T_target, T_end = 5e-4, 1.5\n",
        "    EMBED_MODE = 'sine_wiggle'\n",
        "    #N_REF = 500\n",
        "    ################################################################################################################################################################################\n",
        "\n",
        "    # 1) Build your pathological GMM and grab its params for the oracle\n",
        "    params, sampler_func, score_func, density_func, score_div_func = get_gmm_funcs(num_c = NUM_C, variant = VARIANT, embedding_mode=EMBED_MODE,\n",
        "                                                     overall_scale = SCALE, comp_std=STD, k_dim =K_DIM, m_dim = M_DIM, normalize = True)\n",
        "\n",
        "    proxy_score_func = fit_kernel_score\n",
        "\n",
        "\n",
        "    test_sample = sampler_func(1)\n",
        "    test_density = density_func(test_sample)\n",
        "\n",
        "    #params, sampler_func, score_func = make_pathological_gmm(\n",
        "        #C=500, d=39, sep=2, cov_scale=0.01, seed=0)\n",
        "    means0, covars0, w0 = params   # these are NumPy arrays\n",
        "\n",
        "\n",
        "\n",
        "    # **Convert to Cupy once** so all downstream calls get Cupy arrays\n",
        "    means0_cp = cp.asarray(means0, dtype=cp.float64)\n",
        "    covars0_cp  = cp.asarray(covars0,  dtype=cp.float64)\n",
        "    w0_cp  = cp.asarray(w0,dtype=cp.float64)\n",
        "\n",
        "\n",
        "    ################################################################################################################################################################################\n",
        "    if do_inv:\n",
        "      D = means0.shape[1]\n",
        "      rank_k = 2          # e.g., project down to 3 observed dims\n",
        "      obs_sigma = 1/10\n",
        "\n",
        "      #y_obs = sampler_func(1, seed=SEED).reshape(-1)\n",
        "      A, y_obs_k, likelyhood_func, log_likelyhood_func, loglik_grad_fn = make_rank_k_likelihood(\n",
        "        D=D, rank_k=rank_k, obs_sigma=obs_sigma,\n",
        "        sampler_func=sampler_func, seed=5)\n",
        "\n",
        "      def log_density_func(x_ref):\n",
        "        return cp.log(density_func(x_ref))  # unchanged\n",
        "\n",
        "      Rk = (obs_sigma**2) * cp.eye(rank_k, dtype=cp.float64)\n",
        "\n",
        "      w_post, means_post, covars_post = gmm_posterior_params(y_obs_k, w0, means0, covars0, A=A, b=None, R=Rk)\n",
        "\n",
        "      post_params = (means_post, covars_post, w_post)\n",
        "      _, post_sampler_func, post_score_func, post_density_func, post_score_div_func = get_gmm_funcs(num_c = NUM_C, variant = 'custom',\n",
        "                                                                               preset_params = post_params,  normalize = False)\n",
        "\n",
        "      means_post_cp = cp.asarray(means_post, dtype=cp.float64)\n",
        "      covars_post_cp  = cp.asarray(covars_post,  dtype=cp.float64)\n",
        "      w_post_cp     = cp.asarray(w_post,dtype=cp.float64)\n",
        "\n",
        "    else:\n",
        "      post_sampler_func = None\n",
        "      post_score_func = score_func\n",
        "      post_score_div_func = score_div_func\n",
        "      likelyhood_func = None\n",
        "      loglik_grad_fn = None\n",
        "      means_post_cp = means0_cp\n",
        "      covars_post_cp  = covars0_cp\n",
        "      w_post_cp     = w0_cp\n",
        "\n",
        "    ################################################################################################################################################################################\n",
        "\n",
        "\n",
        "    # 2) Wrap the true‐score calculator so it matches run_comparison’s API:\n",
        "    #    true_score_func(y_batch, t_cur) -> [B, d] cupy array\n",
        "    def true_score_func(y_batch, t_cur):\n",
        "        # here batch_size is only used for micro‐batching inside calculate_true_score_at_t,\n",
        "        # but since y_batch is already a batch we just pass len(y_batch)\n",
        "        return calculate_true_score_at_t(\n",
        "            y_batch,\n",
        "            t_cur,\n",
        "            means_post_cp, covars_post_cp, w_post_cp,\n",
        "            batch_size=y_batch.shape[0])\n",
        "\n",
        "\n",
        "    # 3) Define your samplers\n",
        "    samplers = [\n",
        "        ('heun_pc', 'blend',        'Blend score( True ) '),\n",
        "        #('heun_pc', 'blend_proxy',  'Blend score( Proxy)' ),\n",
        "        #('heun_pc', 'blend_proxy_SVD',  'Blend score(Proxy SVD)' ),\n",
        "        #('heun_pc', 'blend_proxy_SVD',  'Blend score(Proxy SVD recomp)' ),\n",
        "        #('heun_pc', 'tweedie',      'Tweedie score'),\n",
        "    ]\n",
        "    # 4) Call run_comparison with the new arguments:\n",
        "\n",
        "\n",
        "    results = run_comparison(\n",
        "        samplers              = samplers,\n",
        "        steps_list            = [8],\n",
        "        true_sampler_func     = sampler_func,\n",
        "        prior_sampler_func    = sampler_func,\n",
        "        prior_score_func      = score_func,\n",
        "        prior_score_div_func  = score_div_func,\n",
        "        likelyhood_func       = likelyhood_func,\n",
        "        loglik_grad_fn        = loglik_grad_fn,\n",
        "        post_sampler_func     = post_sampler_func,\n",
        "        post_score_func       = post_score_func,\n",
        "        post_score_div_func   = post_score_div_func,\n",
        "        true_init_score       = score_func,\n",
        "        ref_seed              = 0,\n",
        "        trials                = trials,\n",
        "        N_part                = 5000,\n",
        "        nrows                 = 3,\n",
        "        time_split            = 'power',\n",
        "        trial_name            = 'test_with_score_rmse',\n",
        "        div                   = div,\n",
        "        plot_hists            = plot_hists,\n",
        "        hist_mode             = 'pca',\n",
        "        display_mode          =  'min_label',\n",
        "        d_pairs               = None,\n",
        "        plot_res              = False,\n",
        "        N_ref = N_REF,\n",
        "        T_end= T_end,\n",
        "        T_target=T_target,\n",
        "        true_score_func       = true_score_func,\n",
        "        track_score_rmse      = True,\n",
        "        mean_bins = 120,\n",
        "        prior_scale =   negate(do_inv),\n",
        "        plot_prior =  negate(do_inv),\n",
        "        plot_post = True,\n",
        "        draw_countours = False,\n",
        "        hist_norm= 1.5,\n",
        "        post_hist_norm = 1.0,\n",
        "        save_tag = save_tag,\n",
        "        ridge_frac = ridge_frac,\n",
        "        rand_seed= False\n",
        "        )\n",
        "\n",
        "    divs_dict = results['divs_dict']\n",
        "    samples_dict = results['samples_dict']\n",
        "\n",
        "    return results\n",
        "\n",
        "    # 5) results will now include keys like\n",
        "    #    'HHop, Tweedie score_score_rmse' and 'HHop, Blend score_score_rmse'\n",
        "    #print(results)\n",
        "\n",
        "\n",
        "# --- sampler roster -----------------------------------------------------\n",
        "\n",
        "def negate(arg):\n",
        "  if arg:\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  pass\n",
        "\n",
        "  main1(500, plot_hists = True, div = 'W2',\n",
        "        ridge_frac = 1e-5, trials = 1, do_inv = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMNB5HaGWubn",
        "outputId": "d0b881ff-81a5-431f-ac57-3bd5f0ad4125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:282: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1000/90000] lr:5.00e-03 | loss(DSM/Tweedie):1.372e-02\n",
            "  TRUE(γ): TNN:5.939e+00\n",
            "\n",
            "run_data/TWDNN_epoch_1000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.089e+04, M_KSD_floor = 4.007e+01, score_RMSE=1.836e+02, time=17.4s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_1000_8_steps_mean_hist.png'\n",
            "\n",
            "[ 2000/90000] lr:4.99e-03 | loss(DSM/Tweedie):1.688e-02\n",
            "  TRUE(γ): TNN:4.567e+00\n",
            "\n",
            "run_data/TWDNN_epoch_2000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.261e+04, M_KSD_floor = 4.047e+01, score_RMSE=9.980e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_2000_8_steps_mean_hist.png'\n",
            "\n",
            "[ 3000/90000] lr:4.99e-03 | loss(DSM/Tweedie):1.573e-02\n",
            "  TRUE(γ): TNN:3.815e+00\n",
            "\n",
            "run_data/TWDNN_epoch_3000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.129e+04, M_KSD_floor = 4.054e+01, score_RMSE=8.379e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_3000_8_steps_mean_hist.png'\n",
            "\n",
            "[ 4000/90000] lr:4.98e-03 | loss(DSM/Tweedie):1.428e-02\n",
            "  TRUE(γ): TNN:3.454e+00\n",
            "\n",
            "run_data/TWDNN_epoch_4000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.013e+04, M_KSD_floor = 3.968e+01, score_RMSE=7.318e+01, time=4.7s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_4000_8_steps_mean_hist.png'\n",
            "\n",
            "[ 5000/90000] lr:4.96e-03 | loss(DSM/Tweedie):1.184e-02\n",
            "  TRUE(γ): TNN:3.262e+00\n",
            "\n",
            "run_data/TWDNN_epoch_5000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =7.759e+03, M_KSD_floor = 4.031e+01, score_RMSE=6.708e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_5000_8_steps_mean_hist.png'\n",
            "\n",
            "[ 6000/90000] lr:4.95e-03 | loss(DSM/Tweedie):9.028e-03\n",
            "  TRUE(γ): TNN:3.129e+00\n",
            "\n",
            "run_data/TWDNN_epoch_6000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =7.054e+03, M_KSD_floor = 3.965e+01, score_RMSE=6.305e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_6000_8_steps_mean_hist.png'\n",
            "\n",
            "[ 7000/90000] lr:4.93e-03 | loss(DSM/Tweedie):1.548e-02\n",
            "  TRUE(γ): TNN:3.046e+00\n",
            "\n",
            "run_data/TWDNN_epoch_7000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =6.507e+03, M_KSD_floor = 4.075e+01, score_RMSE=6.289e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_7000_8_steps_mean_hist.png'\n",
            "\n",
            "[ 8000/90000] lr:4.90e-03 | loss(DSM/Tweedie):1.310e-02\n",
            "  TRUE(γ): TNN:2.971e+00\n",
            "\n",
            "run_data/TWDNN_epoch_8000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =4.566e+03, M_KSD_floor = 4.067e+01, score_RMSE=5.829e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_8000_8_steps_mean_hist.png'\n",
            "\n",
            "[ 9000/90000] lr:4.88e-03 | loss(DSM/Tweedie):1.214e-02\n",
            "  TRUE(γ): TNN:2.916e+00\n",
            "\n",
            "run_data/TWDNN_epoch_9000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =4.302e+03, M_KSD_floor = 3.927e+01, score_RMSE=5.738e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_9000_8_steps_mean_hist.png'\n",
            "\n",
            "[10000/90000] lr:4.85e-03 | loss(DSM/Tweedie):1.328e-02\n",
            "  TRUE(γ): TNN:2.867e+00\n",
            "\n",
            "run_data/TWDNN_epoch_10000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =3.601e+03, M_KSD_floor = 4.062e+01, score_RMSE=5.427e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_10000_8_steps_mean_hist.png'\n",
            "\n",
            "[11000/90000] lr:4.82e-03 | loss(DSM/Tweedie):1.130e-02\n",
            "  TRUE(γ): TNN:2.840e+00\n",
            "\n",
            "run_data/TWDNN_epoch_11000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =3.840e+03, M_KSD_floor = 4.021e+01, score_RMSE=5.370e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_11000_8_steps_mean_hist.png'\n",
            "\n",
            "[12000/90000] lr:4.78e-03 | loss(DSM/Tweedie):1.043e-02\n",
            "  TRUE(γ): TNN:2.800e+00\n",
            "\n",
            "run_data/TWDNN_epoch_12000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =3.031e+03, M_KSD_floor = 4.017e+01, score_RMSE=5.184e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_12000_8_steps_mean_hist.png'\n",
            "\n",
            "[13000/90000] lr:4.75e-03 | loss(DSM/Tweedie):1.030e-02\n",
            "  TRUE(γ): TNN:2.769e+00\n",
            "\n",
            "run_data/TWDNN_epoch_13000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =3.195e+03, M_KSD_floor = 4.040e+01, score_RMSE=4.939e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_13000_8_steps_mean_hist.png'\n",
            "\n",
            "[14000/90000] lr:4.71e-03 | loss(DSM/Tweedie):8.056e-03\n",
            "  TRUE(γ): TNN:2.749e+00\n",
            "\n",
            "run_data/TWDNN_epoch_14000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =3.032e+03, M_KSD_floor = 4.031e+01, score_RMSE=4.948e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_14000_8_steps_mean_hist.png'\n",
            "\n",
            "[15000/90000] lr:4.67e-03 | loss(DSM/Tweedie):7.126e-03\n",
            "  TRUE(γ): TNN:2.725e+00\n",
            "\n",
            "run_data/TWDNN_epoch_15000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =3.102e+03, M_KSD_floor = 3.993e+01, score_RMSE=4.886e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_15000_8_steps_mean_hist.png'\n",
            "\n",
            "[16000/90000] lr:4.62e-03 | loss(DSM/Tweedie):1.077e-02\n",
            "  TRUE(γ): TNN:2.716e+00\n",
            "\n",
            "run_data/TWDNN_epoch_16000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.727e+03, M_KSD_floor = 4.051e+01, score_RMSE=4.693e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_16000_8_steps_mean_hist.png'\n",
            "\n",
            "[17000/90000] lr:4.57e-03 | loss(DSM/Tweedie):8.318e-03\n",
            "  TRUE(γ): TNN:2.693e+00\n",
            "\n",
            "run_data/TWDNN_epoch_17000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.796e+03, M_KSD_floor = 3.954e+01, score_RMSE=4.502e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_17000_8_steps_mean_hist.png'\n",
            "\n",
            "[18000/90000] lr:4.52e-03 | loss(DSM/Tweedie):1.145e-02\n",
            "  TRUE(γ): TNN:2.676e+00\n",
            "\n",
            "run_data/TWDNN_epoch_18000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.582e+03, M_KSD_floor = 4.037e+01, score_RMSE=4.348e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_18000_8_steps_mean_hist.png'\n",
            "\n",
            "[19000/90000] lr:4.47e-03 | loss(DSM/Tweedie):1.220e-02\n",
            "  TRUE(γ): TNN:2.662e+00\n",
            "\n",
            "run_data/TWDNN_epoch_19000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.845e+03, M_KSD_floor = 4.021e+01, score_RMSE=4.331e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_19000_8_steps_mean_hist.png'\n",
            "\n",
            "[20000/90000] lr:4.42e-03 | loss(DSM/Tweedie):8.173e-03\n",
            "  TRUE(γ): TNN:2.656e+00\n",
            "\n",
            "run_data/TWDNN_epoch_20000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.650e+03, M_KSD_floor = 4.004e+01, score_RMSE=4.311e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_20000_8_steps_mean_hist.png'\n",
            "\n",
            "[21000/90000] lr:4.36e-03 | loss(DSM/Tweedie):6.788e-03\n",
            "  TRUE(γ): TNN:2.631e+00\n",
            "\n",
            "run_data/TWDNN_epoch_21000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.593e+03, M_KSD_floor = 4.024e+01, score_RMSE=4.368e+01, time=4.6s\n",
            " \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3103856076.py:2880: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  fig, axes = plt.subplots(n_rows, n_cols, figsize=(fig_w, fig_h), squeeze=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_21000_8_steps_mean_hist.png'\n",
            "\n",
            "[22000/90000] lr:4.30e-03 | loss(DSM/Tweedie):7.804e-03\n",
            "  TRUE(γ): TNN:2.618e+00\n",
            "\n",
            "run_data/TWDNN_epoch_22000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.323e+03, M_KSD_floor = 3.993e+01, score_RMSE=4.168e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_22000_8_steps_mean_hist.png'\n",
            "\n",
            "[23000/90000] lr:4.24e-03 | loss(DSM/Tweedie):6.550e-03\n",
            "  TRUE(γ): TNN:2.595e+00\n",
            "\n",
            "run_data/TWDNN_epoch_23000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.728e+03, M_KSD_floor = 4.006e+01, score_RMSE=4.076e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_23000_8_steps_mean_hist.png'\n",
            "\n",
            "[24000/90000] lr:4.17e-03 | loss(DSM/Tweedie):1.075e-02\n",
            "  TRUE(γ): TNN:2.582e+00\n",
            "\n",
            "run_data/TWDNN_epoch_24000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.388e+03, M_KSD_floor = 4.080e+01, score_RMSE=4.091e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_24000_8_steps_mean_hist.png'\n",
            "\n",
            "[25000/90000] lr:4.11e-03 | loss(DSM/Tweedie):7.297e-03\n",
            "  TRUE(γ): TNN:2.579e+00\n",
            "\n",
            "run_data/TWDNN_epoch_25000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.276e+03, M_KSD_floor = 4.010e+01, score_RMSE=3.903e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_25000_8_steps_mean_hist.png'\n",
            "\n",
            "[26000/90000] lr:4.04e-03 | loss(DSM/Tweedie):1.187e-02\n",
            "  TRUE(γ): TNN:2.552e+00\n",
            "\n",
            "run_data/TWDNN_epoch_26000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.410e+03, M_KSD_floor = 4.052e+01, score_RMSE=4.006e+01, time=4.7s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_26000_8_steps_mean_hist.png'\n",
            "\n",
            "[27000/90000] lr:3.97e-03 | loss(DSM/Tweedie):8.631e-03\n",
            "  TRUE(γ): TNN:2.535e+00\n",
            "\n",
            "run_data/TWDNN_epoch_27000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.370e+03, M_KSD_floor = 4.018e+01, score_RMSE=3.851e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_27000_8_steps_mean_hist.png'\n",
            "\n",
            "[28000/90000] lr:3.90e-03 | loss(DSM/Tweedie):6.804e-03\n",
            "  TRUE(γ): TNN:2.518e+00\n",
            "\n",
            "run_data/TWDNN_epoch_28000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.272e+03, M_KSD_floor = 4.039e+01, score_RMSE=3.823e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_28000_8_steps_mean_hist.png'\n",
            "\n",
            "[29000/90000] lr:3.83e-03 | loss(DSM/Tweedie):8.096e-03\n",
            "  TRUE(γ): TNN:2.490e+00\n",
            "\n",
            "run_data/TWDNN_epoch_29000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.450e+03, M_KSD_floor = 4.014e+01, score_RMSE=3.834e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_29000_8_steps_mean_hist.png'\n",
            "\n",
            "[30000/90000] lr:3.75e-03 | loss(DSM/Tweedie):5.595e-03\n",
            "  TRUE(γ): TNN:2.474e+00\n",
            "\n",
            "run_data/TWDNN_epoch_30000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.078e+03, M_KSD_floor = 4.085e+01, score_RMSE=3.727e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_30000_8_steps_mean_hist.png'\n",
            "\n",
            "[31000/90000] lr:3.68e-03 | loss(DSM/Tweedie):5.924e-03\n",
            "  TRUE(γ): TNN:2.449e+00\n",
            "\n",
            "run_data/TWDNN_epoch_31000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.140e+03, M_KSD_floor = 4.081e+01, score_RMSE=3.693e+01, time=4.7s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_31000_8_steps_mean_hist.png'\n",
            "\n",
            "[32000/90000] lr:3.60e-03 | loss(DSM/Tweedie):5.235e-03\n",
            "  TRUE(γ): TNN:2.421e+00\n",
            "\n",
            "run_data/TWDNN_epoch_32000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.991e+03, M_KSD_floor = 4.047e+01, score_RMSE=3.769e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_32000_8_steps_mean_hist.png'\n",
            "\n",
            "[33000/90000] lr:3.52e-03 | loss(DSM/Tweedie):7.903e-03\n",
            "  TRUE(γ): TNN:2.392e+00\n",
            "\n",
            "run_data/TWDNN_epoch_33000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.079e+03, M_KSD_floor = 4.022e+01, score_RMSE=3.509e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_33000_8_steps_mean_hist.png'\n",
            "\n",
            "[34000/90000] lr:3.44e-03 | loss(DSM/Tweedie):7.948e-03\n",
            "  TRUE(γ): TNN:2.364e+00\n",
            "\n",
            "run_data/TWDNN_epoch_34000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.072e+03, M_KSD_floor = 4.054e+01, score_RMSE=3.570e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_34000_8_steps_mean_hist.png'\n",
            "\n",
            "[35000/90000] lr:3.36e-03 | loss(DSM/Tweedie):5.642e-03\n",
            "  TRUE(γ): TNN:2.336e+00\n",
            "\n",
            "run_data/TWDNN_epoch_35000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.107e+03, M_KSD_floor = 4.003e+01, score_RMSE=3.658e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_35000_8_steps_mean_hist.png'\n",
            "\n",
            "[36000/90000] lr:3.28e-03 | loss(DSM/Tweedie):6.897e-03\n",
            "  TRUE(γ): TNN:2.314e+00\n",
            "\n",
            "run_data/TWDNN_epoch_36000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.948e+03, M_KSD_floor = 3.967e+01, score_RMSE=3.568e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_36000_8_steps_mean_hist.png'\n",
            "\n",
            "[37000/90000] lr:3.19e-03 | loss(DSM/Tweedie):5.804e-03\n",
            "  TRUE(γ): TNN:2.293e+00\n",
            "\n",
            "run_data/TWDNN_epoch_37000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.999e+03, M_KSD_floor = 4.038e+01, score_RMSE=3.616e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_37000_8_steps_mean_hist.png'\n",
            "\n",
            "[38000/90000] lr:3.11e-03 | loss(DSM/Tweedie):6.534e-03\n",
            "  TRUE(γ): TNN:2.271e+00\n",
            "\n",
            "run_data/TWDNN_epoch_38000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.858e+03, M_KSD_floor = 3.999e+01, score_RMSE=3.313e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_38000_8_steps_mean_hist.png'\n",
            "\n",
            "[39000/90000] lr:3.02e-03 | loss(DSM/Tweedie):7.016e-03\n",
            "  TRUE(γ): TNN:2.245e+00\n",
            "\n",
            "run_data/TWDNN_epoch_39000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.095e+03, M_KSD_floor = 4.085e+01, score_RMSE=3.474e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_39000_8_steps_mean_hist.png'\n",
            "\n",
            "[40000/90000] lr:2.94e-03 | loss(DSM/Tweedie):5.609e-03\n",
            "  TRUE(γ): TNN:2.218e+00\n",
            "\n",
            "run_data/TWDNN_epoch_40000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.067e+03, M_KSD_floor = 4.023e+01, score_RMSE=3.460e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_40000_8_steps_mean_hist.png'\n",
            "\n",
            "[41000/90000] lr:2.85e-03 | loss(DSM/Tweedie):7.052e-03\n",
            "  TRUE(γ): TNN:2.198e+00\n",
            "\n",
            "run_data/TWDNN_epoch_41000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.978e+03, M_KSD_floor = 4.063e+01, score_RMSE=3.511e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_41000_8_steps_mean_hist.png'\n",
            "\n",
            "[42000/90000] lr:2.77e-03 | loss(DSM/Tweedie):4.477e-03\n",
            "  TRUE(γ): TNN:2.185e+00\n",
            "\n",
            "run_data/TWDNN_epoch_42000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.826e+03, M_KSD_floor = 3.992e+01, score_RMSE=3.414e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_42000_8_steps_mean_hist.png'\n",
            "\n",
            "[43000/90000] lr:2.68e-03 | loss(DSM/Tweedie):5.970e-03\n",
            "  TRUE(γ): TNN:2.160e+00\n",
            "\n",
            "run_data/TWDNN_epoch_43000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.262e+03, M_KSD_floor = 4.011e+01, score_RMSE=3.383e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_43000_8_steps_mean_hist.png'\n",
            "\n",
            "[44000/90000] lr:2.59e-03 | loss(DSM/Tweedie):6.238e-03\n",
            "  TRUE(γ): TNN:2.145e+00\n",
            "\n",
            "run_data/TWDNN_epoch_44000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.082e+03, M_KSD_floor = 3.994e+01, score_RMSE=3.544e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_44000_8_steps_mean_hist.png'\n",
            "\n",
            "[45000/90000] lr:2.50e-03 | loss(DSM/Tweedie):3.579e-03\n",
            "  TRUE(γ): TNN:2.116e+00\n",
            "\n",
            "run_data/TWDNN_epoch_45000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.015e+03, M_KSD_floor = 4.044e+01, score_RMSE=3.336e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_45000_8_steps_mean_hist.png'\n",
            "\n",
            "[46000/90000] lr:2.42e-03 | loss(DSM/Tweedie):4.995e-03\n",
            "  TRUE(γ): TNN:2.100e+00\n",
            "\n",
            "run_data/TWDNN_epoch_46000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.955e+03, M_KSD_floor = 4.089e+01, score_RMSE=3.416e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_46000_8_steps_mean_hist.png'\n",
            "\n",
            "[47000/90000] lr:2.33e-03 | loss(DSM/Tweedie):3.679e-03\n",
            "  TRUE(γ): TNN:2.094e+00\n",
            "\n",
            "run_data/TWDNN_epoch_47000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.960e+03, M_KSD_floor = 4.036e+01, score_RMSE=3.455e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_47000_8_steps_mean_hist.png'\n",
            "\n",
            "[48000/90000] lr:2.24e-03 | loss(DSM/Tweedie):3.692e-03\n",
            "  TRUE(γ): TNN:2.082e+00\n",
            "\n",
            "run_data/TWDNN_epoch_48000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.960e+03, M_KSD_floor = 3.931e+01, score_RMSE=3.323e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_48000_8_steps_mean_hist.png'\n",
            "\n",
            "[49000/90000] lr:2.16e-03 | loss(DSM/Tweedie):4.407e-03\n",
            "  TRUE(γ): TNN:2.059e+00\n",
            "\n",
            "run_data/TWDNN_epoch_49000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.847e+03, M_KSD_floor = 4.073e+01, score_RMSE=3.200e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_49000_8_steps_mean_hist.png'\n",
            "\n",
            "[50000/90000] lr:2.07e-03 | loss(DSM/Tweedie):3.353e-03\n",
            "  TRUE(γ): TNN:2.052e+00\n",
            "\n",
            "run_data/TWDNN_epoch_50000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.921e+03, M_KSD_floor = 3.994e+01, score_RMSE=3.409e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_50000_8_steps_mean_hist.png'\n",
            "\n",
            "[51000/90000] lr:1.99e-03 | loss(DSM/Tweedie):4.092e-03\n",
            "  TRUE(γ): TNN:2.028e+00\n",
            "\n",
            "run_data/TWDNN_epoch_51000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.897e+03, M_KSD_floor = 4.007e+01, score_RMSE=3.196e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_51000_8_steps_mean_hist.png'\n",
            "\n",
            "[52000/90000] lr:1.90e-03 | loss(DSM/Tweedie):5.094e-03\n",
            "  TRUE(γ): TNN:2.026e+00\n",
            "\n",
            "run_data/TWDNN_epoch_52000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.855e+03, M_KSD_floor = 4.008e+01, score_RMSE=3.248e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_52000_8_steps_mean_hist.png'\n",
            "\n",
            "[53000/90000] lr:1.82e-03 | loss(DSM/Tweedie):3.194e-03\n",
            "  TRUE(γ): TNN:2.011e+00\n",
            "\n",
            "run_data/TWDNN_epoch_53000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.963e+03, M_KSD_floor = 4.050e+01, score_RMSE=3.201e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_53000_8_steps_mean_hist.png'\n",
            "\n",
            "[54000/90000] lr:1.73e-03 | loss(DSM/Tweedie):3.116e-03\n",
            "  TRUE(γ): TNN:1.990e+00\n",
            "\n",
            "run_data/TWDNN_epoch_54000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.885e+03, M_KSD_floor = 4.012e+01, score_RMSE=3.195e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_54000_8_steps_mean_hist.png'\n",
            "\n",
            "[55000/90000] lr:1.65e-03 | loss(DSM/Tweedie):5.204e-03\n",
            "  TRUE(γ): TNN:2.000e+00\n",
            "\n",
            "run_data/TWDNN_epoch_55000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.792e+03, M_KSD_floor = 4.000e+01, score_RMSE=3.155e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_55000_8_steps_mean_hist.png'\n",
            "\n",
            "[56000/90000] lr:1.57e-03 | loss(DSM/Tweedie):4.934e-03\n",
            "  TRUE(γ): TNN:1.977e+00\n",
            "\n",
            "run_data/TWDNN_epoch_56000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.710e+03, M_KSD_floor = 4.004e+01, score_RMSE=3.115e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_56000_8_steps_mean_hist.png'\n",
            "\n",
            "[57000/90000] lr:1.49e-03 | loss(DSM/Tweedie):6.859e-03\n",
            "  TRUE(γ): TNN:1.957e+00\n",
            "\n",
            "run_data/TWDNN_epoch_57000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.858e+03, M_KSD_floor = 3.987e+01, score_RMSE=3.147e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_57000_8_steps_mean_hist.png'\n",
            "\n",
            "[58000/90000] lr:1.41e-03 | loss(DSM/Tweedie):5.347e-03\n",
            "  TRUE(γ): TNN:1.951e+00\n",
            "\n",
            "run_data/TWDNN_epoch_58000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.752e+03, M_KSD_floor = 4.062e+01, score_RMSE=3.122e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_58000_8_steps_mean_hist.png'\n",
            "\n",
            "[59000/90000] lr:1.33e-03 | loss(DSM/Tweedie):3.243e-03\n",
            "  TRUE(γ): TNN:1.948e+00\n",
            "\n",
            "run_data/TWDNN_epoch_59000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.745e+03, M_KSD_floor = 4.031e+01, score_RMSE=3.073e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_59000_8_steps_mean_hist.png'\n",
            "\n",
            "[60000/90000] lr:1.26e-03 | loss(DSM/Tweedie):4.233e-03\n",
            "  TRUE(γ): TNN:1.941e+00\n",
            "\n",
            "run_data/TWDNN_epoch_60000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.733e+03, M_KSD_floor = 4.010e+01, score_RMSE=3.165e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_60000_8_steps_mean_hist.png'\n",
            "\n",
            "[61000/90000] lr:1.18e-03 | loss(DSM/Tweedie):4.493e-03\n",
            "  TRUE(γ): TNN:1.923e+00\n",
            "\n",
            "run_data/TWDNN_epoch_61000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.697e+03, M_KSD_floor = 4.090e+01, score_RMSE=3.115e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_61000_8_steps_mean_hist.png'\n",
            "\n",
            "[62000/90000] lr:1.11e-03 | loss(DSM/Tweedie):4.379e-03\n",
            "  TRUE(γ): TNN:1.911e+00\n",
            "\n",
            "run_data/TWDNN_epoch_62000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.858e+03, M_KSD_floor = 4.020e+01, score_RMSE=3.098e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_62000_8_steps_mean_hist.png'\n",
            "\n",
            "[63000/90000] lr:1.04e-03 | loss(DSM/Tweedie):4.577e-03\n",
            "  TRUE(γ): TNN:1.919e+00\n",
            "\n",
            "run_data/TWDNN_epoch_63000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.788e+03, M_KSD_floor = 4.066e+01, score_RMSE=3.005e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_63000_8_steps_mean_hist.png'\n",
            "\n",
            "[64000/90000] lr:9.69e-04 | loss(DSM/Tweedie):3.055e-03\n",
            "  TRUE(γ): TNN:1.914e+00\n",
            "\n",
            "run_data/TWDNN_epoch_64000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.897e+03, M_KSD_floor = 4.050e+01, score_RMSE=3.118e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_64000_8_steps_mean_hist.png'\n",
            "\n",
            "[65000/90000] lr:9.01e-04 | loss(DSM/Tweedie):4.159e-03\n",
            "  TRUE(γ): TNN:1.895e+00\n",
            "\n",
            "run_data/TWDNN_epoch_65000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.798e+03, M_KSD_floor = 4.113e+01, score_RMSE=3.172e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_65000_8_steps_mean_hist.png'\n",
            "\n",
            "[66000/90000] lr:8.36e-04 | loss(DSM/Tweedie):4.192e-03\n",
            "  TRUE(γ): TNN:1.898e+00\n",
            "\n",
            "run_data/TWDNN_epoch_66000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.955e+03, M_KSD_floor = 4.030e+01, score_RMSE=3.168e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_66000_8_steps_mean_hist.png'\n",
            "\n",
            "[67000/90000] lr:7.72e-04 | loss(DSM/Tweedie):2.697e-03\n",
            "  TRUE(γ): TNN:1.892e+00\n",
            "\n",
            "run_data/TWDNN_epoch_67000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.715e+03, M_KSD_floor = 4.058e+01, score_RMSE=3.100e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_67000_8_steps_mean_hist.png'\n",
            "\n",
            "[68000/90000] lr:7.10e-04 | loss(DSM/Tweedie):2.777e-03\n",
            "  TRUE(γ): TNN:1.886e+00\n",
            "\n",
            "run_data/TWDNN_epoch_68000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =2.132e+03, M_KSD_floor = 4.012e+01, score_RMSE=3.165e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_68000_8_steps_mean_hist.png'\n",
            "\n",
            "[69000/90000] lr:6.51e-04 | loss(DSM/Tweedie):3.915e-03\n",
            "  TRUE(γ): TNN:1.871e+00\n",
            "\n",
            "run_data/TWDNN_epoch_69000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.820e+03, M_KSD_floor = 4.059e+01, score_RMSE=3.046e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_69000_8_steps_mean_hist.png'\n",
            "\n",
            "[70000/90000] lr:5.94e-04 | loss(DSM/Tweedie):4.547e-03\n",
            "  TRUE(γ): TNN:1.864e+00\n",
            "\n",
            "run_data/TWDNN_epoch_70000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.625e+03, M_KSD_floor = 4.089e+01, score_RMSE=2.974e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_70000_8_steps_mean_hist.png'\n",
            "\n",
            "[71000/90000] lr:5.39e-04 | loss(DSM/Tweedie):5.617e-03\n",
            "  TRUE(γ): TNN:1.877e+00\n",
            "\n",
            "run_data/TWDNN_epoch_71000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.705e+03, M_KSD_floor = 4.064e+01, score_RMSE=2.969e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_71000_8_steps_mean_hist.png'\n",
            "\n",
            "[72000/90000] lr:4.87e-04 | loss(DSM/Tweedie):3.582e-03\n",
            "  TRUE(γ): TNN:1.862e+00\n",
            "\n",
            "run_data/TWDNN_epoch_72000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.571e+03, M_KSD_floor = 4.027e+01, score_RMSE=2.947e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_72000_8_steps_mean_hist.png'\n",
            "\n",
            "[73000/90000] lr:4.37e-04 | loss(DSM/Tweedie):2.726e-03\n",
            "  TRUE(γ): TNN:1.863e+00\n",
            "\n",
            "run_data/TWDNN_epoch_73000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.739e+03, M_KSD_floor = 4.024e+01, score_RMSE=3.058e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_73000_8_steps_mean_hist.png'\n",
            "\n",
            "[74000/90000] lr:3.89e-04 | loss(DSM/Tweedie):2.765e-03\n",
            "  TRUE(γ): TNN:1.862e+00\n",
            "\n",
            "run_data/TWDNN_epoch_74000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.444e+03, M_KSD_floor = 4.045e+01, score_RMSE=3.096e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_74000_8_steps_mean_hist.png'\n",
            "\n",
            "[75000/90000] lr:3.44e-04 | loss(DSM/Tweedie):4.125e-03\n",
            "  TRUE(γ): TNN:1.852e+00\n",
            "\n",
            "run_data/TWDNN_epoch_75000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.651e+03, M_KSD_floor = 4.057e+01, score_RMSE=2.975e+01, time=4.6s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_75000_8_steps_mean_hist.png'\n",
            "\n",
            "[76000/90000] lr:3.02e-04 | loss(DSM/Tweedie):4.993e-03\n",
            "  TRUE(γ): TNN:1.859e+00\n",
            "\n",
            "run_data/TWDNN_epoch_76000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.591e+03, M_KSD_floor = 4.036e+01, score_RMSE=2.851e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_76000_8_steps_mean_hist.png'\n",
            "\n",
            "[77000/90000] lr:2.63e-04 | loss(DSM/Tweedie):3.643e-03\n",
            "  TRUE(γ): TNN:1.858e+00\n",
            "\n",
            "run_data/TWDNN_epoch_77000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.634e+03, M_KSD_floor = 3.969e+01, score_RMSE=2.843e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_77000_8_steps_mean_hist.png'\n",
            "\n",
            "[78000/90000] lr:2.26e-04 | loss(DSM/Tweedie):3.932e-03\n",
            "  TRUE(γ): TNN:1.858e+00\n",
            "\n",
            "run_data/TWDNN_epoch_78000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.649e+03, M_KSD_floor = 4.014e+01, score_RMSE=3.004e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_78000_8_steps_mean_hist.png'\n",
            "\n",
            "[79000/90000] lr:1.92e-04 | loss(DSM/Tweedie):3.936e-03\n",
            "  TRUE(γ): TNN:1.845e+00\n",
            "\n",
            "run_data/TWDNN_epoch_79000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.700e+03, M_KSD_floor = 4.028e+01, score_RMSE=2.861e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_79000_8_steps_mean_hist.png'\n",
            "\n",
            "[80000/90000] lr:1.60e-04 | loss(DSM/Tweedie):2.648e-03\n",
            "  TRUE(γ): TNN:1.845e+00\n",
            "\n",
            "run_data/TWDNN_epoch_80000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.569e+03, M_KSD_floor = 4.018e+01, score_RMSE=3.007e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_80000_8_steps_mean_hist.png'\n",
            "\n",
            "[81000/90000] lr:1.32e-04 | loss(DSM/Tweedie):2.569e-03\n",
            "  TRUE(γ): TNN:1.853e+00\n",
            "\n",
            "run_data/TWDNN_epoch_81000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.781e+03, M_KSD_floor = 4.037e+01, score_RMSE=3.047e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_81000_8_steps_mean_hist.png'\n",
            "\n",
            "[82000/90000] lr:1.07e-04 | loss(DSM/Tweedie):2.055e-03\n",
            "  TRUE(γ): TNN:1.849e+00\n",
            "\n",
            "run_data/TWDNN_epoch_82000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.658e+03, M_KSD_floor = 3.981e+01, score_RMSE=2.881e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_82000_8_steps_mean_hist.png'\n",
            "\n",
            "[83000/90000] lr:8.41e-05 | loss(DSM/Tweedie):2.969e-03\n",
            "  TRUE(γ): TNN:1.844e+00\n",
            "\n",
            "run_data/TWDNN_epoch_83000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.783e+03, M_KSD_floor = 4.119e+01, score_RMSE=2.996e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_83000_8_steps_mean_hist.png'\n",
            "\n",
            "[84000/90000] lr:6.45e-05 | loss(DSM/Tweedie):4.840e-03\n",
            "  TRUE(γ): TNN:1.847e+00\n",
            "\n",
            "run_data/TWDNN_epoch_84000,  8 steps, NN-EMA (DSM/Tweedie): M_KSD =1.691e+03, M_KSD_floor = 3.968e+01, score_RMSE=2.846e+01, time=4.5s\n",
            " \n",
            "[WARN] run_comparison probe failed: [Errno 2] No such file or directory: 'run_data/TWDNN_epoch_84000_8_steps_mean_hist.png'\n",
            "\n",
            "[85000/90000] lr:4.79e-05 | loss(DSM/Tweedie):2.630e-03\n",
            "  TRUE(γ): TNN:1.843e+00\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# Tweedie-only Neural Distillation (OU score) + PROBE\n",
        "#  - log- or uniform-t sampling\n",
        "#  - cosine LR, EMA, AMP, grad clipping\n",
        "#  - training target from Tweedie (NP) via streamed refs\n",
        "#  - eval: RMSE(net_ema vs TRUE) across γ-time grid\n",
        "#  - NEW: probe_with_run_comparison() during training (Heun-PC path)\n",
        "# ==========================================\n",
        "\n",
        "import math, os, time\n",
        "import numpy as np\n",
        "import cupy as cp  # <--- NEW\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# -------------------- Config --------------------\n",
        "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "USE_AMP  = (DEVICE == \"cuda\")\n",
        "\n",
        "########################################################################################\n",
        "SEED  = 0\n",
        "NUM_C, K_DIM = 6000, 32\n",
        "VARIANT, STD, SCALE = \"crossing_torus\", .02, 3.0\n",
        "EMBED_MODE = 'sine_wiggle'\n",
        "T_target, T_end = 5e-4, 1.5\n",
        "N_Train = 500000\n",
        "N_REF = 3000\n",
        "HIDDEN = 512\n",
        "STEPS = 90000\n",
        "########################################################################################\n",
        "\n",
        "BATCH = 512\n",
        "LR_INIT, LR_MIN = 5e-3, 1e-5\n",
        "T_MIN, T_MAX = T_target, T_end\n",
        "PRINT_EVERY = 1000\n",
        "EMA_DECAY = 0.999\n",
        "REF_CHUNK  = BATCH\n",
        "MAX_REF_CALL = 50_000\n",
        "REFRESH_DATA = False\n",
        "\n",
        "# PROBE controls (matched to your snippet)\n",
        "PROBE_STEPS      = 8\n",
        "PROBE_TRIALS     = 1\n",
        "RUN_PROBE_EVERY  = PRINT_EVERY   # call probe on the same cadence as prints\n",
        "N_TEST = 5000\n",
        "\n",
        "# time sampling mode for training\n",
        "T_MODE = 'uniform'\n",
        "SAVE_PATH = \"tweedie_distill_ema.pt\"\n",
        "\n",
        "\n",
        "torch.manual_seed(SEED); np.random.seed(SEED)\n",
        "if DEVICE == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "# ===== Gamma-prior eval helpers =====\n",
        "GAMMA_PRIOR   = 1\n",
        "EVAL_N_T      = 24     # number of midpoint times for training-time eval\n",
        "EVAL_PER_T    = 2048   # eval samples per time\n",
        "T_END, T_TGT  = T_MAX, T_MIN\n",
        "\n",
        "@torch.no_grad()\n",
        "def make_gamma_times(n_t: int = EVAL_N_T, gamma: float = GAMMA_PRIOR,\n",
        "                     T_end: float = T_END, T_tgt: float = T_TGT, device: str = DEVICE):\n",
        "    u = (torch.arange(n_t, device=device, dtype=torch.float32) + 0.5) / n_t  # midpoints\n",
        "    return T_end * ((T_tgt / T_end) ** (u ** (1.0 / gamma)))                # [n_t], decreasing\n",
        "\n",
        "def rmse_t(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.sqrt(torch.mean((a - b) ** 2))\n",
        "\n",
        "# ----------------- Basic GMM helpers -----------------\n",
        "@torch.no_grad()\n",
        "def torch_sample_gmm(n: int, means: torch.Tensor, stds: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:\n",
        "    K, D = means.shape\n",
        "    idx = torch.multinomial(weights, num_samples=n, replacement=True)\n",
        "    noise = torch.randn(n, D, device=means.device) * stds[idx].unsqueeze(1)\n",
        "    return means[idx] + noise\n",
        "\n",
        "@torch.no_grad()\n",
        "def ou_params_at_t(means0: torch.Tensor, stds0: torch.Tensor, t: torch.Tensor):\n",
        "    et   = torch.exp(-t)                 # [B,1]\n",
        "    et2  = torch.exp(-2.0 * t)\n",
        "    means_t = et.view(-1,1,1) * means0.unsqueeze(0)       # [B,K,D]\n",
        "    var_t   = stds0.pow(2).unsqueeze(0) * et2 + (1.0 - et2)\n",
        "    stds_t  = torch.sqrt(var_t + 1e-12)\n",
        "    return means_t, stds_t\n",
        "\n",
        "@torch.no_grad()\n",
        "def true_score_xt_torch(x: torch.Tensor, t: torch.Tensor, means0: torch.Tensor, stds0: torch.Tensor, weights0: torch.Tensor) -> torch.Tensor:\n",
        "    B, D = x.shape\n",
        "    K, _ = means0.shape\n",
        "    means_t, stds_t = ou_params_at_t(means0, stds0, t)\n",
        "    inv_vars = 1.0 / (stds_t ** 2)\n",
        "    log_w    = torch.log(weights0 + 1e-40).view(1, K).expand(B, K)\n",
        "    norm_const = -0.5 * D * (math.log(2*math.pi) + torch.log(stds_t**2))\n",
        "    diff = means_t - x.unsqueeze(1)                               # [B,K,D]\n",
        "    d2   = (diff**2).sum(-1)\n",
        "    logp = log_w + norm_const - 0.5 * d2 * inv_vars\n",
        "    w    = torch.softmax(logp, dim=1)\n",
        "    return (w.unsqueeze(-1) * diff * inv_vars.unsqueeze(-1)).sum(1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_evolved_gmm(n: int, t_scalar: float,\n",
        "                       means0: torch.Tensor, stds0: torch.Tensor, w0: torch.Tensor) -> torch.Tensor:\n",
        "    t  = torch.tensor(t_scalar, device=means0.device, dtype=means0.dtype)\n",
        "    et = torch.exp(-t); et2 = torch.exp(-2.0*t)\n",
        "    means_t = means0 * et\n",
        "    var_t   = stds0**2 * et2 + (1.0 - et2)\n",
        "    stds_t  = torch.sqrt(var_t.clamp_min(1e-12))\n",
        "    return torch_sample_gmm(n, means_t, stds_t, w0)\n",
        "\n",
        "# ----------------- Tweedie (NP) target -----------------\n",
        "@torch.no_grad()\n",
        "def tweedie_score_np(y: torch.Tensor, t: torch.Tensor, x_ref: torch.Tensor,\n",
        "                     ref_chunk: int = 4096, max_ref: int | None = 10_000) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Streaming Tweedie target s_twd(y,t) using references x_ref:\n",
        "      s_twd = - (y - E[e^{-t} X_0 | y]) / Var(y|x0) ,\n",
        "      with OU variance Var = 1 - e^{-2t}.\n",
        "    Returns [B,D].\n",
        "    \"\"\"\n",
        "    B, D = y.shape\n",
        "    N = x_ref.shape[0]\n",
        "    if (max_ref is not None) and (N > max_ref):\n",
        "        idx = torch.randperm(N, device=x_ref.device)[:max_ref]\n",
        "        x_ref = x_ref[idx]; N = x_ref.shape[0]\n",
        "\n",
        "    et   = torch.exp(-t)                     # [B,1]\n",
        "    et2  = torch.exp(-2.0 * t)\n",
        "    var  = (1.0 - et2).clamp_min(1e-12)      # [B,1]\n",
        "    invv = 1.0 / var\n",
        "\n",
        "    y2     = (y*y).sum(-1)                   # [B]\n",
        "    m      = torch.full((B,), -float(\"inf\"), device=y.device, dtype=y.dtype)\n",
        "    s      = torch.zeros(B, device=y.device, dtype=y.dtype)\n",
        "    num_x  = torch.zeros(B, D, device=y.device, dtype=y.dtype)\n",
        "\n",
        "    for start in range(0, N, ref_chunk):\n",
        "        end = min(start + ref_chunk, N)\n",
        "        xr  = x_ref[start:end].to(y.device, non_blocking=True).contiguous()   # [Nc,D]\n",
        "        xr2   = (xr*xr).sum(-1)                                # [Nc]\n",
        "        cross = y @ xr.T                                       # [B,Nc]\n",
        "        d2    = y2[:,None] + (et*et) * xr2[None,:] - 2.0 * et * cross\n",
        "        L     = -0.5 * invv * d2                               # [B,Nc]\n",
        "\n",
        "        m_new = torch.maximum(m, L.max(dim=1).values)          # [B]\n",
        "        scale = torch.exp(m - m_new)                           # [B]\n",
        "        wtil  = torch.exp(L - m_new[:,None])                   # [B,Nc]\n",
        "\n",
        "        s      = s * scale + wtil.sum(1)\n",
        "        num_x  = num_x  * scale[:,None] + torch.einsum('bn,nd->bd', wtil, xr)\n",
        "        m      = m_new\n",
        "\n",
        "    x_bar  = num_x  / (s[:,None] + 1e-30)                     # E[X0|y] proxy (scaled later)\n",
        "    mu_bar = et * x_bar                                       # E[e^{-t} X0 | y]\n",
        "    return -invv * (y - mu_bar)\n",
        "\n",
        "# ----------------- Net ----------------------\n",
        "class ScoreNet(nn.Module):\n",
        "    def __init__(self, dim: int, hidden: int = 512):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim + 1, hidden), nn.SiLU(),\n",
        "            nn.Linear(hidden, hidden),   nn.SiLU(),\n",
        "            nn.Linear(hidden, dim),\n",
        "        )\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(torch.cat([x, torch.log(t)], dim=1))  # predicts sigma * s(y,t)\n",
        "\n",
        "def update_ema(target: nn.Module, src: nn.Module, decay: float):\n",
        "    with torch.no_grad():\n",
        "        for p_t, p_s in zip(target.parameters(), src.parameters()):\n",
        "            p_t.data.mul_((decay)).add_(p_s.data, alpha=(1.0 - decay))\n",
        "\n",
        "# ----------------- Build targets (GMM params only) -----------------\n",
        "# Expectation: get_gmm_funcs is available in your codebase\n",
        "params, sampler_func, score_func = get_gmm_funcs(\n",
        "    NUM_C, k_dim=K_DIM, variant=VARIANT, comp_std=STD, overall_scale=SCALE,\n",
        "    seed=0, embedding_mode=EMBED_MODE\n",
        ")[:3]\n",
        "means0, stds0, w0 = params\n",
        "\n",
        "# convert to torch tensors directly\n",
        "means0 = torch.as_tensor(means0, device=DEVICE, dtype=torch.float32).contiguous()\n",
        "stds0  = torch.as_tensor(stds0,  device=DEVICE, dtype=torch.float32).contiguous()\n",
        "w0     = torch.as_tensor(w0,     device=DEVICE, dtype=torch.float32).contiguous()\n",
        "w0    /= w0.sum()\n",
        "\n",
        "# Fixed reference set for Tweedie targets (CPU pinned for streaming)\n",
        "with torch.no_grad():\n",
        "    x_ref = torch_sample_gmm(N_REF, means0, stds0, w0).detach().cpu().float().pin_memory()\n",
        "\n",
        "D = means0.shape[1]\n",
        "net_twd     = ScoreNet(D, hidden=HIDDEN).to(DEVICE)\n",
        "net_twd_ema = ScoreNet(D, hidden=HIDDEN).to(DEVICE)\n",
        "net_twd_ema.load_state_dict(net_twd.state_dict())\n",
        "\n",
        "# optional compile\n",
        "try:\n",
        "    if DEVICE == \"cuda\":\n",
        "        net_twd     = torch.compile(net_twd)\n",
        "        net_twd_ema = torch.compile(net_twd_ema)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "opt_twd   = optim.Adam(net_twd.parameters(), lr=LR_INIT)\n",
        "sched_twd = torch.optim.lr_scheduler.CosineAnnealingLR(opt_twd, T_max=STEPS, eta_min=LR_MIN)\n",
        "scaler_twd= torch.amp.GradScaler('cuda', enabled=USE_AMP)\n",
        "mse = nn.MSELoss()\n",
        "\n",
        "def update_ema_all():\n",
        "    update_ema(net_twd_ema, net_twd, EMA_DECAY)\n",
        "\n",
        "# ----------------- Fixed training dataset (CPU pinned, deterministic) -----------------\n",
        "TRAIN_SEED  = int(globals().get(\"TRAIN_SEED\", globals().get(\"SEED\", 0)))\n",
        "_gen_cpu    = torch.Generator(device='cpu').manual_seed(TRAIN_SEED)\n",
        "\n",
        "def _sample_gmm_cpu_with_gen(n, means_cpu, stds_cpu, w_cpu, gen):\n",
        "    K, D = means_cpu.shape\n",
        "    idx   = torch.multinomial(w_cpu, num_samples=n, replacement=True, generator=gen)\n",
        "    noise = torch.randn(n, D, generator=gen, dtype=means_cpu.dtype)\n",
        "    return means_cpu[idx] + noise * stds_cpu[idx].unsqueeze(1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _means_cpu = means0.detach().cpu().float()\n",
        "    _stds_cpu  = stds0.detach().cpu().float()\n",
        "    _w_cpu     = (w0 / w0.sum()).detach().cpu().float()\n",
        "\n",
        "    x0_train_cpu  = _sample_gmm_cpu_with_gen(N_Train, _means_cpu, _stds_cpu, _w_cpu, _gen_cpu).pin_memory()\n",
        "    u_train_cpu   = torch.rand(N_Train, 1, generator=_gen_cpu, dtype=torch.float32).pin_memory()\n",
        "    eps_train_cpu = torch.randn(N_Train, _means_cpu.shape[1], generator=_gen_cpu, dtype=torch.float32).pin_memory()\n",
        "\n",
        "_train_ptr  = 0\n",
        "_train_perm = torch.randperm(N_Train, generator=_gen_cpu)\n",
        "\n",
        "def _next_batch_indices(B):\n",
        "    global _train_ptr, _train_perm\n",
        "    if _train_ptr + B <= N_Train:\n",
        "        idx = _train_perm[_train_ptr:_train_ptr+B]; _train_ptr += B; return idx\n",
        "    tail = _train_perm[_train_ptr:]\n",
        "    _train_perm = torch.randperm(N_Train, generator=_gen_cpu)  # <--- FIXED the tiny typo from your paste\n",
        "    _train_ptr  = 0\n",
        "    need = B - tail.numel()\n",
        "    head = _train_perm[_train_ptr:_train_ptr+need]; _train_ptr += need\n",
        "    return torch.cat([tail, head], dim=0)\n",
        "\n",
        "# ----------------- Batch maker (Tweedie NP target) -----------------\n",
        "@torch.no_grad()\n",
        "def make_batch(B, *, ref_chunk=None, max_ref=None, t_mode=T_MODE):\n",
        "    if ref_chunk is None:\n",
        "        ref_chunk = min(4096, int(globals().get(\"N_REF\", 4096)))\n",
        "    if max_ref is None:\n",
        "        max_ref = int(globals().get(\"N_REF\", 10_000))\n",
        "\n",
        "    if globals().get(\"REFRESH_DATA\", False):\n",
        "        gen = torch.Generator(device='cpu').manual_seed(time.time_ns() % (2**63))\n",
        "        x0  = _sample_gmm_cpu_with_gen(B, _means_cpu, _stds_cpu, _w_cpu, gen).to(DEVICE, non_blocking=True)\n",
        "        u   = torch.rand(B, 1, generator=gen, dtype=torch.float32).to(DEVICE, non_blocking=True)\n",
        "        eps = torch.randn(B, _means_cpu.shape[1], generator=gen, dtype=torch.float32).to(DEVICE, non_blocking=True)\n",
        "    else:\n",
        "        idx = _next_batch_indices(B)\n",
        "        x0  = x0_train_cpu[idx].to(DEVICE, non_blocking=True)\n",
        "        u   = u_train_cpu[idx].to(DEVICE, non_blocking=True)\n",
        "        eps = eps_train_cpu[idx].to(DEVICE, non_blocking=True)\n",
        "\n",
        "    if t_mode == 'log':\n",
        "        logTmin = math.log(T_MIN); logTmax = math.log(T_MAX)\n",
        "        t = torch.exp(torch.tensor(logTmin, device=DEVICE) + u * (logTmax - logTmin))\n",
        "    elif t_mode == 'uniform':\n",
        "        t = torch.tensor(T_MIN, device=DEVICE) + u * (T_MAX - T_MIN)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown t_mode: {t_mode}\")\n",
        "\n",
        "    et    = torch.exp(-t)\n",
        "    sigma = torch.sqrt((1.0 - torch.exp(-2.0 * t)).clamp_min(1e-12))\n",
        "    x_t   = et * x0 + sigma * eps\n",
        "\n",
        "    # streamed Tweedie NP target\n",
        "    x_ref_src = globals().get(\"x_ref_cpu\", globals().get(\"x_ref\", None))\n",
        "    if x_ref_src is None:\n",
        "        raise RuntimeError(\"x_ref (or x_ref_cpu) must be defined.\")\n",
        "    if isinstance(x_ref_src, torch.Tensor) and x_ref_src.is_cuda:\n",
        "        # prefer CPU pinned for streaming\n",
        "        x_ref_src = x_ref_src.detach().cpu().pin_memory()\n",
        "    s_twd_np = tweedie_score_np(x_t, t, x_ref_src, ref_chunk=ref_chunk, max_ref=max_ref)\n",
        "\n",
        "    # true score for eval only\n",
        "    s_true = true_score_xt_torch(x_t, t, means0, stds0, w0)\n",
        "\n",
        "    return x_t, t, sigma, s_twd_np, s_true\n",
        "\n",
        "# ----------------- Eval (γ-grid RMSE, EMA) -----------------\n",
        "@torch.no_grad()\n",
        "def eval_gamma_grid_rmse(n_t: int = EVAL_N_T, per_t: int = EVAL_PER_T):\n",
        "    t_grid = make_gamma_times(n_t=n_t)\n",
        "    ssum = 0.0\n",
        "    for t in t_grid.tolist():\n",
        "        x_eval = sample_evolved_gmm(per_t, t, means0, stds0, w0)     # [B,D]\n",
        "        tcol   = torch.full((per_t,1), t, device=DEVICE, dtype=x_eval.dtype)\n",
        "        sigma  = torch.sqrt((1.0 - torch.exp(-2.0*tcol)).clamp_min(1e-12))\n",
        "        s_true = true_score_xt_torch(x_eval, tcol, means0, stds0, w0)\n",
        "        pred   = net_twd_ema(x_eval, tcol) / sigma\n",
        "        ssum  += rmse_t(pred, s_true).item()\n",
        "    return {\"TWDNN_TRUE\": ssum / len(t_grid)}\n",
        "\n",
        "# ===================== NEW: PROBE INTEGRATION =====================\n",
        "\n",
        "# 1) build CuPy copies of params for the run_comparison utilities\n",
        "means0_cp, stds0_cp, w0_cp = map(cp.asarray, (means0.detach().cpu().numpy(),\n",
        "                                              stds0.detach().cpu().numpy(),\n",
        "                                              (w0 / w0.sum()).detach().cpu().numpy()))\n",
        "\n",
        "# 2) true score in CuPy space, using your existing helper from the run_comparison stack\n",
        "def true_score_func_cp(y_cp, t_scalar: float):\n",
        "    # relies on your project-defined function\n",
        "    return calculate_true_score_at_t(\n",
        "        y_cp, t_scalar, means0_cp, stds0_cp, w0_cp, batch_size=4096\n",
        "    )\n",
        "\n",
        "# 3) wrap EMA torch model → CuPy score(y,t)\n",
        "@torch.no_grad()\n",
        "def ema_score_xt_torch(x_t: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
        "    if t.dim() == 1: t = t[:, None]\n",
        "    sigma_t  = torch.sqrt((1.0 - torch.exp(-2.0 * t)).clamp_min(1e-12))\n",
        "    q_scaled = net_twd_ema(x_t, t)                      # predicts σ_t * s\n",
        "    s = q_scaled / sigma_t\n",
        "    return torch.nan_to_num(s, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "def make_custom_score_from_torch(score_xt_torch, device):\n",
        "    def score_cp(y_cp, t_scalar):\n",
        "        y_np = cp.asnumpy(y_cp)\n",
        "        x_t  = torch.as_tensor(y_np, device=device, dtype=torch.float32)\n",
        "        t    = torch.full((x_t.shape[0], 1), float(t_scalar), device=device, dtype=x_t.dtype)\n",
        "        s    = score_xt_torch(x_t, t).detach().cpu().numpy().astype(np.float64)\n",
        "        return cp.asarray(s)\n",
        "    return score_cp\n",
        "\n",
        "# 4) EXACT blackbox sampler (Heun-PC path included)\n",
        "def blackbox_sampler(\n",
        "    N_part, time_pts, custom_score, sampler_func, *,\n",
        "    mode=\"heun_pc\", h_coeff=0.5, true_score_func=None,\n",
        "    p_prune=0, likelyhood_func=None, loglik_grad_fn=None\n",
        "):\n",
        "    xp  = cp\n",
        "    dim = int(sampler_func(5).shape[1])           # match model/data dim\n",
        "    y   = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "    rmse_list = []                                # collect RMSE vs oracle\n",
        "\n",
        "    for k in range(len(time_pts)-1):\n",
        "        t_cur, t_prev = time_pts[k], time_pts[k+1]\n",
        "        dt    = t_cur - t_prev\n",
        "        noise = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "        S_cur  = custom_score(y, t_cur)           # [N_part, dim]\n",
        "        y_hat  = y + dt*(y + 2*S_cur) + xp.sqrt(2.0*dt)*noise\n",
        "        S_prev = custom_score(y_hat, t_prev)\n",
        "\n",
        "        if mode == \"heun_pc\":\n",
        "            # predictor-corrector Heun step for the OU-like forward ODE/SDE drift\n",
        "            drift_avg = 0.5*(y + 2*S_cur) + 0.5*(y_hat + 2*S_prev)\n",
        "            y = y + dt*drift_avg + xp.sqrt(2.0 * dt) * noise\n",
        "        elif mode == \"ou_sde\":\n",
        "            y = y_hat\n",
        "        elif mode == \"pf_ode\":\n",
        "            y = y + dt*(y + S_cur)\n",
        "        else:\n",
        "            raise ValueError(mode)\n",
        "\n",
        "    if true_score_func is not None:\n",
        "        S_cur_true  = true_score_func(y, t_cur)\n",
        "        S_prev_true = true_score_func(y_hat, t_prev)\n",
        "\n",
        "        # These helpers are part of your run_comparison utilities\n",
        "        S_cur_safe,  cur_idx_safe  = prune_cp_arr(S_cur,  p_percent=p_prune)\n",
        "        S_prev_safe, prev_idx_safe = prune_cp_arr(S_prev, p_percent=p_prune)\n",
        "\n",
        "        RMSE_cur  = float(xp_rmse(S_cur_safe,  S_cur_true[cur_idx_safe]))\n",
        "        RMSE_prev = float(xp_rmse(S_prev_safe, S_prev_true[prev_idx_safe]))\n",
        "\n",
        "        rmse_list.append(RMSE_cur)\n",
        "        rmse_list.append(RMSE_prev)\n",
        "        return y, rmse_list\n",
        "    else:\n",
        "        return y\n",
        "\n",
        "# 5) bind EMA score to CuPy callback\n",
        "_CUSTOM_SCORE = make_custom_score_from_torch(ema_score_xt_torch, DEVICE)\n",
        "\n",
        "# 6) the probe wrapper (uses your run_comparison)\n",
        "def probe_with_run_comparison(epoch_or_step: int, nsteps: int = 8, trials: int = 3,\n",
        "                              p_prune: int = 50, N_ref = 2000, N_part = 2000):\n",
        "    if 'run_comparison' not in globals():\n",
        "        raise RuntimeError(\"run_comparison is not defined in this environment.\")\n",
        "    # ensure blackbox path override is used by run_comparison internals\n",
        "    run_comparison.__globals__['blackbox_sampler'] = blackbox_sampler\n",
        "\n",
        "    samplers = [\n",
        "        ('heun_pc', 'custom', 'NN-EMA (DSM/Tweedie)', _CUSTOM_SCORE),\n",
        "    ]\n",
        "    _ = run_comparison(\n",
        "        samplers           = samplers,\n",
        "        steps_list         = [int(nsteps)],   # use requested nsteps\n",
        "        true_sampler_func  = sampler_func,\n",
        "        prior_sampler_func = sampler_func,\n",
        "        prior_score_func   = score_func,\n",
        "        true_score_func    = true_score_func_cp,\n",
        "        true_init_score    = score_func,\n",
        "        track_score_rmse   = True,\n",
        "        N_ref              = N_ref,\n",
        "        N_part             = N_part,\n",
        "        trials             = int(trials),\n",
        "        nrows              = 3,\n",
        "        mean_bins          = 120,\n",
        "        time_split         = 'power',\n",
        "        T_end              = T_end,\n",
        "        T_target           = T_target,\n",
        "        div                = 'M_KSD',\n",
        "        plot_hists         = True,\n",
        "        hist_mode          = 'pca',\n",
        "        display_mode       = 'min_label',\n",
        "        trial_name         = f'TWDNN_hist_compare_e{epoch_or_step}',\n",
        "        p_prune            = p_prune,\n",
        "        ref_seed           = 1,\n",
        "        plot_res           = False,\n",
        "        hist_norm          = 2.0,\n",
        "        plot_prior         = True,\n",
        "        save_tag           = f\"run_data/TWDNN_epoch_{epoch_or_step}\",\n",
        "    )\n",
        "\n",
        "# =================== END PROBE INTEGRATION ===================\n",
        "\n",
        "# ----------------- Training -------------------------\n",
        "SUBSTEPS = int(globals().get(\"SUBSTEPS\", 1))\n",
        "REF_CHUNK_TRAIN = min(4096, int(globals().get(\"N_REF\", 4096)))\n",
        "MAX_REF_TRAIN   = int(globals().get(\"N_REF\", 10_000))\n",
        "\n",
        "for step in range(1, STEPS+1):\n",
        "    for _ in range(SUBSTEPS):\n",
        "        x_t, t, sigma_t, s_twd_np, s_true = make_batch(\n",
        "            BATCH, ref_chunk=REF_CHUNK_TRAIN, max_ref=MAX_REF_TRAIN, t_mode=T_MODE\n",
        "        )\n",
        "        tgt_twd = sigma_t * s_twd_np  # train on sigma * score\n",
        "\n",
        "        opt_twd.zero_grad(set_to_none=True)\n",
        "        with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
        "            pred_twd_scaled = net_twd(x_t, t)\n",
        "            loss_twd = mse(pred_twd_scaled, tgt_twd)\n",
        "        scaler_twd.scale(loss_twd).backward()\n",
        "        scaler_twd.unscale_(opt_twd)\n",
        "        torch.nn.utils.clip_grad_norm_(net_twd.parameters(), 1.0)\n",
        "        scaler_twd.step(opt_twd); scaler_twd.update(); sched_twd.step()\n",
        "\n",
        "        update_ema_all()\n",
        "\n",
        "    if step % PRINT_EVERY == 0:\n",
        "        metrics = eval_gamma_grid_rmse(n_t=EVAL_N_T, per_t=EVAL_PER_T)\n",
        "        lr_now = sched_twd.get_last_lr()[0]\n",
        "        print(f\"[{step:5d}/{STEPS}] lr:{lr_now:.2e} | loss(DSM/Tweedie):{loss_twd.item():.3e}\")\n",
        "        print(f\"  TRUE(γ): TNN:{metrics['TWDNN_TRUE']:.3e}\")\n",
        "        print()\n",
        "\n",
        "    # ---- PROBE on cadence ----\n",
        "    if (RUN_PROBE_EVERY > 0) and (step % RUN_PROBE_EVERY == 0):\n",
        "        try:\n",
        "            # --- end-to-end probe through EXACT blackbox path (Heun-PC) ---\n",
        "            probe_with_run_comparison(step, nsteps=PROBE_STEPS, trials=PROBE_TRIALS)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] run_comparison probe failed: {e}\\n\")\n",
        "\n",
        "# === Save the EMA net =========\n",
        "def unwrap_module(m):\n",
        "    if hasattr(m, \"module\"):    m = m.module\n",
        "    if hasattr(m, \"_orig_mod\"): m = m._orig_mod\n",
        "    return m\n",
        "\n",
        "torch.save(\n",
        "    {\n",
        "        \"net_twd\": unwrap_module(net_twd_ema).state_dict(),\n",
        "        \"dim\":     D,\n",
        "        \"hidden\":  HIDDEN,\n",
        "    },\n",
        "    SAVE_PATH,\n",
        ")\n",
        "print(f\"[INFO] EMA network written to {SAVE_PATH}\")\n",
        "\n",
        "# Optional final quick eval batch\n",
        "do_final_eval = False\n",
        "if do_final_eval:\n",
        "    with torch.no_grad():\n",
        "        x_t, t, sigma_t, s_twd_np, s_true = make_batch(N_TEST)\n",
        "        pred_twd = net_twd_ema(x_t, t) / sigma_t\n",
        "        print(\"\\n========== FINAL (EMA) RMSE ==========\")\n",
        "        print(f\"TWDNN_to_true: {rmse_t(pred_twd, s_true).item():.4e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juroty679XOr"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# Critic + Gate only (GPU/fast, stabilized)\n",
        "#  - log-uniform t, σ_t-scaled critic target\n",
        "#  - cosine LR, EMA, AMP, grad clipping\n",
        "#  - γ-grid eval (RMSE to true), λ agreement\n",
        "#  - NP targets only for metrics; no NN KSS/TWD heads\n",
        "#  - *** s0 is now a learned proxy via local Gaussian kernel (PyTorch) ***\n",
        "\n",
        "import math, os, time\n",
        "import numpy as nptr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# -----------trrrreedee--------- Config & safe defaults --------------------\n",
        "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "USE_AMP  = (DEVICE == \"cuda\")\n",
        "\n",
        "# NEW: toggle to choose KSS/CSEM teacher: proxy s0 vs TRUE s0\n",
        "#SAVE_PATH = \"learned_score_critic_gate_ema_recomp.pt\"  # <-- renamed\n",
        "SAVE_PATHS =  [\"learned_score_critic_gate_recomp_ema.pt\"]\n",
        "#[\"critic_gate_ema.pt\", \"learned_score_critic_gate_ema.pt\", \"learned_score_critic_gate_recomp_ema.pt\"]\n",
        "for SAVE_PATH in SAVE_PATHS:\n",
        "    if SAVE_PATH == \"critic_gate_ema.pt\":\n",
        "      USE_PROXY_SCORES = False\n",
        "    else:\n",
        "      USE_PROXY_SCORES = True\n",
        "\n",
        "    if 'recomp' in SAVE_PATH:\n",
        "      PROXY_RECOMPUTE = True\n",
        "    else:\n",
        "      PROXY_RECOMPUTE = False\n",
        "\n",
        "    #USE_PROXY_SCORES = bool(globals().get(\"USE_PROXY_SCORES\", True))\n",
        "    print(f\"[INFO] Using {'PROXY' if USE_PROXY_SCORES else 'TRUE'} initial scores for KSS/CSEM teacher.\")\n",
        "\n",
        "    #SAVE_PATH = \"learned_score_critic_gate_ema.pt\"  # <-- renamed\n",
        "    #SAVE_PATH = \"critic_gate_ema.pt\"  # <-- renamed\n",
        "\n",
        "    # Fallbacks if not defined upstream\n",
        "    SEED        = globals().get(\"SEED\", 0)\n",
        "    NUM_C       = globals().get(\"NUM_C\", 2000)\n",
        "    K_DIM       = globals().get(\"K_DIM\", 12)\n",
        "    VARIANT     = globals().get(\"VARIANT\", \"helix\")\n",
        "    STD         = globals().get(\"STD\", 0.12)\n",
        "    SCALE       = globals().get(\"SCALE\", 3.0)\n",
        "    T_target    = globals().get(\"T_target\", 5e-4)\n",
        "    T_end       = globals().get(\"T_end\", 1.5)\n",
        "    N_Train     = globals().get(\"N_Train\", globals().get(\"N_TRAIN\", 500000))\n",
        "    N_REF       = globals().get(\"N_REF\", 5000)\n",
        "    HIDDEN      = globals().get(\"HIDDEN\", 512)\n",
        "    HIDDEM_LAM  = globals().get(\"HIDDEM_LAM\", 128)\n",
        "    EVAL_N_T    = globals().get(\"EVAL_N_T\", 24)\n",
        "    EVAL_PER_T  = globals().get(\"EVAL_PER_T\", 2048)\n",
        "    GAMMA_PRIOR = globals().get(\"GAMMA_PRIOR\", 1)\n",
        "    EMBED_MODE = globals().get(\"EMBED_MODE\", 'sine_wiggle')\n",
        "\n",
        "    # ---- NEW: proxy-score controls ----\n",
        "    PROXY_N           = int(globals().get(\"N_train\", 500_000))\n",
        "    PROXY_K_MIX       = 400 #int(globals().get(\"PROXY_K_MIX\", 100))   # how many anchor comps to mix per query\n",
        "    PROXY_COMP_K      = globals().get(\"PROXY_COMP_K\", None)\n",
        "\n",
        "\n",
        "    PROXY_RIDGE_FRAC        = 1e-6\n",
        "    PROXY_ALPHA             = float(globals().get(\"PROXY_ALPHA\", 0.4))\n",
        "    PROXY_REF_CHUNK         = int(globals().get(\"PROXY_REF_CHUNK\", 8192))\n",
        "    PRECOMPUTE_TRAIN_SCORES = bool(globals().get(\"PRECOMPUTE_TRAIN_SCORES\", True))\n",
        "    PRECOMP_BATCH           = int(globals().get(\"PRECOMP_BATCH\", 8192))\n",
        "    SHOW_LAMBDA_STATS       = bool(globals().get(\"SHOW_LAMBDA_STATS\", True))\n",
        "\n",
        "\n",
        "    # ---- NEW: toggle & limits for KDE/GMM recomputation mode ----\n",
        "    PROXY_GMM_BATCH                 = int(globals().get(\"PROXY_GMM_BATCH\", 1024))\n",
        "    PROXY_RECOMPUTE_MAX_ANCHORS     = 30000 #int(globals().get(\"N_train\", 500_000))  # guardrail\n",
        "\n",
        "    # ---------------- Base training knobs ----------------\n",
        "    # (keep your existing knob defaults)\n",
        "    STEPS = 29500 #int(globals().get(\"STEPS\", 17000))\n",
        "    do_final_eval = bool(globals().get(\"do_final_eval\", False))\n",
        "\n",
        "    ############## critic gate specific ###################\n",
        "    CRIT_STEPS_PER_LAM = 3 #int(globals().get(\"CRIT_STEPS_PER_LAM\", 4))\n",
        "    WARMUP_LAM_STEPS   = 500\n",
        "    LAM_EPS            = float(globals().get(\"LAM_EPS\", 1e-4))\n",
        "    LR_LAM_SCALE       = .66\n",
        "    WD_LAM             = float(globals().get(\"WD_LAM\", 1e-4))\n",
        "    STEPS += WARMUP_LAM_STEPS\n",
        "    #######################################################\n",
        "\n",
        "\n",
        "    # ===== Helper: γ-prior time grid =====\n",
        "    @torch.no_grad()\n",
        "    def make_gamma_times(n_t: int = EVAL_N_T, gamma: float = GAMMA_PRIOR,\n",
        "                        T_end: float = T_MAX, T_tgt: float = T_MIN, device: str = DEVICE):\n",
        "        u = (torch.arange(n_t, device=device, dtype=torch.float32) + 0.5) / n_t\n",
        "        return T_end * ((T_tgt / T_end) ** (u ** (1.0 / gamma)))\n",
        "\n",
        "    def rmse_t(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "        return torch.sqrt(torch.mean((a - b) ** 2))\n",
        "\n",
        "    # ===== GMM helpers (you already have get_gmm_funcs) =====\n",
        "    @torch.no_grad()\n",
        "    def torch_sample_gmm(n: int, means: torch.Tensor, stds: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:\n",
        "        K, D = means.shape\n",
        "        idx = torch.multinomial(weights, num_samples=n, replacement=True)\n",
        "        noise = torch.randn(n, D, device=means.device) * stds[idx].unsqueeze(1)\n",
        "        return means[idx] + noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def torch_score_gmm_batch(x: torch.Tensor, means: torch.Tensor, stds: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"(Kept for 'true score' checks; no longer used as the teacher s0.)\"\"\"\n",
        "        B, D = x.shape\n",
        "        K = means.shape[0]\n",
        "        inv_vars = 1.0 / (stds ** 2)\n",
        "        log_w    = torch.log(weights + 1e-40)\n",
        "        norm_const = -0.5 * D * (math.log(2 * math.pi) + torch.log(stds**2))\n",
        "        diff = means.unsqueeze(0) - x.unsqueeze(1)                      # [B,K,D]\n",
        "        d2   = (diff ** 2).sum(dim=-1)                                  # [B,K]\n",
        "        logp = log_w + norm_const - 0.5 * d2 * inv_vars.unsqueeze(0)    # [B,K]\n",
        "        w = torch.softmax(logp, dim=1)\n",
        "        return (w.unsqueeze(-1) * diff * inv_vars.view(1, K, 1)).sum(dim=1)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def ou_params_at_t(means0: torch.Tensor, stds0: torch.Tensor, t: torch.Tensor):\n",
        "        et   = torch.exp(-t)\n",
        "        et2  = torch.exp(-2.0 * t)\n",
        "        means_t = et.view(-1,1,1) * means0.unsqueeze(0)       # [B,K,D]\n",
        "        var_t   = stds0.pow(2).unsqueeze(0) * et2 + (1.0 - et2)\n",
        "        stds_t  = torch.sqrt(var_t + 1e-12)\n",
        "        return means_t, stds_t\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def true_score_xt_torch(x: torch.Tensor, t: torch.Tensor, means0: torch.Tensor, stds0: torch.Tensor, weights0: torch.Tensor) -> torch.Tensor:\n",
        "        B, D = x.shape\n",
        "        K, _ = means0.shape\n",
        "        means_t, stds_t = ou_params_at_t(means0, stds0, t)\n",
        "        inv_vars = 1.0 / (stds_t ** 2)\n",
        "        log_w    = torch.log(weights0 + 1e-40).view(1, K).expand(B, K)\n",
        "        norm_const = -0.5 * D * (math.log(2*math.pi) + torch.log(stds_t**2))\n",
        "        diff = means_t - x.unsqueeze(1)                               # [B,K,D]\n",
        "        d2   = (diff**2).sum(-1)\n",
        "        logp = log_w + norm_const - 0.5 * d2 * inv_vars\n",
        "        w    = torch.softmax(logp, dim=1)\n",
        "        return (w.unsqueeze(-1) * diff * inv_vars.unsqueeze(-1)).sum(1)\n",
        "\n",
        "    # ======= NEW: Learned proxy for initial score s0 (PyTorch) =======\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _adaptive_k_default(n_ref: int, D: int, alpha: float = PROXY_ALPHA) -> int:\n",
        "        N0, k0 = 2000, max(4*D, 48)\n",
        "        k_default = int(min(n_ref-1, max(1, round(k0 * (n_ref / N0) ** alpha))))\n",
        "        return k_default\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _knn_topk_across_chunks(Q: torch.Tensor,\n",
        "                                R: torch.Tensor,\n",
        "                                k: int,\n",
        "                                ref_chunk: int = PROXY_REF_CHUNK):\n",
        "        \"\"\"\n",
        "        Maintain top-k smallest squared distances from each query in Q to rows in R\n",
        "        without materializing the full distance matrix. Returns (best_d2, best_idx).\n",
        "        \"\"\"\n",
        "        device = Q.device\n",
        "        Nq, D = Q.shape\n",
        "        Nr = R.shape[0]\n",
        "\n",
        "        # Precompute squared norms\n",
        "        q2 = (Q*Q).sum(dim=1, keepdim=True)           # [Nq,1]\n",
        "        r2_all = (R*R).sum(dim=1)                     # [Nr]\n",
        "\n",
        "        best_d2  = torch.full((Nq, k), float('inf'), device=device, dtype=Q.dtype)\n",
        "        best_idx = torch.full((Nq, k), -1, device=device, dtype=torch.long)\n",
        "\n",
        "        # Process reference in chunks\n",
        "        arng = torch.arange(Nq, device=device)\n",
        "        for start in range(0, Nr, ref_chunk):\n",
        "            end = min(start + ref_chunk, Nr)\n",
        "            Rc = R[start:end]                         # [Br,D]\n",
        "            r2 = r2_all[start:end]                    # [Br]\n",
        "            # d2 = ||q||^2 + ||r||^2 - 2 q r^T\n",
        "            d2 = q2 + r2.view(1,-1) - 2.0 * (Q @ Rc.T)   # [Nq,Br]\n",
        "            d2.clamp_(min=0)\n",
        "\n",
        "            # Merge with running top-k\n",
        "            d2_cat   = torch.cat([best_d2, d2], dim=1)   # [Nq, k+Br]\n",
        "            idx_new  = start + torch.arange(Rc.shape[0], device=device)\n",
        "            idx_cat  = torch.cat([best_idx, idx_new.view(1,-1).expand(Nq,-1)], dim=1)\n",
        "            # pick smallest k\n",
        "            d2_k, ord_k = torch.topk(d2_cat, k=k, dim=1, largest=False, sorted=False)\n",
        "            idx_k = torch.gather(idx_cat, dim=1, index=ord_k)\n",
        "            best_d2, best_idx = d2_k, idx_k\n",
        "\n",
        "        return best_d2, best_idx\n",
        "\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def kernel_proxy_score_torch(x_query: torch.Tensor,\n",
        "                                x_anchor: torch.Tensor,\n",
        "                                *,\n",
        "                                k: int | None = None,\n",
        "                                ridge_frac: float = PROXY_RIDGE_FRAC,\n",
        "                                ref_chunk: int = PROXY_REF_CHUNK,\n",
        "                                recompute: bool = False,\n",
        "                                gmm_batch: int = PROXY_GMM_BATCH,\n",
        "                                recompute_max_anchors: int = PROXY_RECOMPUTE_MAX_ANCHORS,\n",
        "                                k_mix: int | None = None,  #New\n",
        "                                comp_k: int | None = None,  #New\n",
        "                                return_params: bool = False\n",
        "                                ) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        assert x_query.dim() == 2 and x_anchor.dim() == 2, \"Inputs must be [N, D]\"\n",
        "        device = x_query.device\n",
        "        dtype  = x_query.dtype\n",
        "        Nq, D  = x_query.shape\n",
        "        Nr, Da = x_anchor.shape\n",
        "        assert Da == D, f\"Dim mismatch: x_anchor has D={Da}, x_query has D={D}\"\n",
        "\n",
        "        if k is None:\n",
        "            k = _adaptive_k_default(Nr, D)\n",
        "\n",
        "\n",
        "        # defaults for recompute mode\n",
        "        if k_mix is None:\n",
        "            k_mix = min(k, Nr)          # how many comps we mix per query\n",
        "        if comp_k is None:\n",
        "            comp_k = min(k, Nr-1)       # how many neighbors per anchor for μ,var estimation\n",
        "        comp_k = max(1, int(comp_k))\n",
        "        k_mix  = max(1, int(k_mix))\n",
        "\n",
        "\n",
        "        # ---------- Fast path: standard local estimator ----------\n",
        "        if not recompute:\n",
        "            # kNN from query -> anchors\n",
        "            d2_knn, idx = _knn_topk_across_chunks(x_query, x_anchor, k=k, ref_chunk=ref_chunk)  # [Nq,k], [Nq,k]\n",
        "            # adaptive bandwidth from max d2 among the k-NN (per query)\n",
        "            tiny = torch.finfo(dtype).tiny\n",
        "            h2   = torch.maximum(d2_knn.max(dim=1).values, torch.tensor(tiny, device=device, dtype=dtype))  # [Nq]\n",
        "            w    = torch.exp(-d2_knn / (2.0 * h2[:, None]))                                                 # [Nq,k]\n",
        "            denom= w.sum(dim=1, keepdim=True).clamp_min(torch.finfo(dtype).eps)                             # [Nq,1]\n",
        "            X_nb = x_anchor[idx]                                                                            # [Nq,k,D]\n",
        "\n",
        "            mu   = (w.unsqueeze(-1) * X_nb).sum(dim=1) / denom                                              # [Nq,D]\n",
        "            diff = X_nb - mu.unsqueeze(1)                                                                    # [Nq,k,D]\n",
        "            var  = (w.unsqueeze(-1) * (diff * diff)).sum(dim=1) / denom                                      # [Nq,D]\n",
        "\n",
        "            # ridge proportional to mean variance per point\n",
        "            tau   = ridge_frac * var.mean(dim=1, keepdim=True)                                              # [Nq,1]\n",
        "            var_r = (var + tau).clamp_min(torch.finfo(dtype).eps)                                           # [Nq,D]\n",
        "\n",
        "            score = (mu - x_query) / var_r                                                                   # [Nq,D]\n",
        "            return (score, mu, var_r) if return_params else score\n",
        "\n",
        "                    # ---------- Recompute path: gated mixture over compact anchor subset ----------\n",
        "        if return_params:\n",
        "            raise NotImplementedError(\"return_params=True is only supported for recompute=False (fast path).\")\n",
        "\n",
        "        # A) Gate: per-query k_mix nearest anchors (from FULL pool)\n",
        "        d2_qk, idx_qk = _knn_topk_across_chunks(x_query, x_anchor, k=k_mix, ref_chunk=ref_chunk)  # [Nq,k_mix]\n",
        "\n",
        "        # Build compact unique set by global frequency\n",
        "        flat_idx = idx_qk.reshape(-1)\n",
        "        counts   = torch.bincount(flat_idx, minlength=Nr)\n",
        "        keep_n   = min(recompute_max_anchors, int((counts > 0).sum().item()))\n",
        "        top_idx  = torch.topk(counts, k=keep_n, largest=True).indices\n",
        "        uniq_idx = top_idx[counts[top_idx] > 0]      # [Nu]\n",
        "        Nu       = uniq_idx.numel()\n",
        "\n",
        "        # Map global anchor id -> position in compact set\n",
        "        pos_map = torch.full((Nr,), -1, device=x_query.device, dtype=torch.long)\n",
        "        pos_map[uniq_idx] = torch.arange(Nu, device=x_query.device)\n",
        "\n",
        "        # Per-query validity after capping\n",
        "        pos         = pos_map[idx_qk]                 # [Nq,k_mix] in [-1,Nu-1]\n",
        "        has_valid   = pos.ge(0).any(dim=1)            # [Nq]\n",
        "        no_valid    = ~has_valid                      # [Nq]\n",
        "\n",
        "        # If some queries lost all anchors, compute their scores with the fast (non-recompute) path\n",
        "        out = torch.empty(Nq, D, device=device, dtype=dtype)\n",
        "\n",
        "        # B) Learn per-anchor diagonal params ONLY for kept anchors, using neighbors in FULL pool\n",
        "        if Nu > 0:\n",
        "            Q_comp = x_anchor[uniq_idx]  # [Nu,D]\n",
        "            d2_all, idx_all = _knn_topk_across_chunks(Q_comp, x_anchor, k=min(comp_k+1, Nr), ref_chunk=ref_chunk)\n",
        "\n",
        "            # remove self (if present); else drop farthest to keep size=comp_k\n",
        "            ar        = uniq_idx.view(-1,1).expand(-1, idx_all.shape[1])\n",
        "            is_self   = (idx_all == ar)\n",
        "            has_self  = is_self.any(dim=1)\n",
        "\n",
        "            keep_mask = torch.ones_like(idx_all, dtype=torch.bool)\n",
        "            if has_self.any():\n",
        "                self_pos = torch.argmax(is_self.int(), dim=1)\n",
        "                keep_mask[torch.arange(Nu, device=device), self_pos] = False\n",
        "            need_drop = ~has_self\n",
        "            if need_drop.any():\n",
        "                far_pos = torch.argmax(d2_all, dim=1)\n",
        "                keep_mask[torch.arange(Nu, device=device)[need_drop], far_pos[need_drop]] = False\n",
        "\n",
        "            idx_comp = idx_all[keep_mask].view(Nu, -1)  # [Nu,comp_k]\n",
        "            d2_comp  = d2_all[keep_mask].view(Nu, -1)   # [Nu,comp_k]\n",
        "\n",
        "            tiny   = torch.finfo(dtype).tiny\n",
        "            eps    = torch.finfo(dtype).eps\n",
        "            h2_c   = torch.maximum(d2_comp.max(dim=1).values, torch.tensor(tiny, device=device, dtype=dtype))\n",
        "            w_c    = torch.exp(-d2_comp / (2.0 * h2_c[:, None]))                           # [Nu,comp_k]\n",
        "            denomc = w_c.sum(dim=1, keepdim=True).clamp_min(eps)\n",
        "            Xnb_c  = x_anchor[idx_comp]                                                    # [Nu,comp_k,D]\n",
        "\n",
        "            mu_c   = (w_c.unsqueeze(-1) * Xnb_c).sum(dim=1) / denomc                       # [Nu,D]\n",
        "            diffc  = Xnb_c - mu_c.unsqueeze(1)                                             # [Nu,comp_k,D]\n",
        "            var_c  = (w_c.unsqueeze(-1) * (diffc*diffc)).sum(dim=1) / denomc               # [Nu,D]\n",
        "            tau_c  = ridge_frac * var_c.mean(dim=1, keepdim=True)                          # [Nu,1]\n",
        "            varr_c = (var_c + tau_c).clamp_min(eps)                                        # [Nu,D]\n",
        "\n",
        "            two_pi   = torch.tensor(2.0 * math.pi, device=device, dtype=dtype)\n",
        "            logdet_c = torch.sum(torch.log(two_pi * varr_c), dim=1)                        # [Nu]\n",
        "\n",
        "            # C) Evaluate mixture for queries that HAVE at least one valid anchor\n",
        "            if has_valid.any():\n",
        "                sel      = has_valid.nonzero(as_tuple=True)[0]\n",
        "                pos_sel  = pos[sel].clamp_min(0)             # [Nv,k_mix]\n",
        "                mask_sel = pos[sel].ge(0)                    # [Nv,k_mix]\n",
        "                x_sel    = x_query[sel]                      # [Nv,D]\n",
        "\n",
        "                mu_sel   = mu_c[pos_sel]                     # [Nv,k_mix,D]\n",
        "                var_sel  = varr_c[pos_sel]                   # [Nv,k_mix,D]\n",
        "                logd_sel = logdet_c[pos_sel]                 # [Nv,k_mix]\n",
        "\n",
        "                dX    = x_sel[:, None, :] - mu_sel\n",
        "                quad  = torch.sum(dX * (dX / var_sel), dim=2)           # [Nv,k_mix]\n",
        "                logw  = -0.5 * (quad + logd_sel)\n",
        "                logw  = logw.masked_fill(~mask_sel, -float(\"inf\"))\n",
        "\n",
        "                m     = torch.max(logw, dim=1, keepdim=True).values     # [Nv,1], finite because at least one valid\n",
        "                W     = torch.exp(logw - m)                              # [Nv,k_mix]\n",
        "                compS = -(dX / var_sel)                                  # [Nv,k_mix,D]\n",
        "\n",
        "                num   = torch.einsum(\"nk,nkd->nd\", W, compS)             # [Nv,D]\n",
        "                den   = W.sum(dim=1).clamp_min(eps)                      # [Nv]\n",
        "                out[sel] = num / den[:, None]\n",
        "\n",
        "        # D) Fast-path fallback for queries with no valid anchors\n",
        "        if no_valid.any():\n",
        "            sel_nv = no_valid.nonzero(as_tuple=True)[0]\n",
        "            out[sel_nv] = kernel_proxy_score_torch(\n",
        "                x_query[sel_nv], x_anchor,\n",
        "                k=k, ridge_frac=ridge_frac, ref_chunk=ref_chunk,\n",
        "                recompute=False,  # <— fast local estimator\n",
        "                gmm_batch=gmm_batch\n",
        "            )\n",
        "\n",
        "        # Final numerical hygiene\n",
        "        return torch.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        '''\n",
        "        # ---------- Recompute path: gated mixture over compact anchor subset ----------\n",
        "        if return_params:\n",
        "            raise NotImplementedError(\"return_params=True is only supported for recompute=False (fast path).\")\n",
        "\n",
        "        # A) Gate: for each query, take its k_mix nearest anchors from the FULL pool\n",
        "        d2_qk, idx_qk = _knn_topk_across_chunks(x_query, x_anchor, k=k_mix, ref_chunk=ref_chunk)  # [Nq,k_mix]\n",
        "\n",
        "        # Make a compact unique set of anchors actually needed by this batch\n",
        "        flat_idx = idx_qk.reshape(-1)\n",
        "        # Count frequency to keep the most-used anchors if we exceed the cap\n",
        "        counts = torch.bincount(flat_idx, minlength=Nr)\n",
        "        # Keep at most recompute_max_anchors anchors; prefer most frequent\n",
        "        keep_n = min(recompute_max_anchors, (counts > 0).sum().item())\n",
        "        top_idx = torch.topk(counts, k=keep_n, largest=True).indices\n",
        "        uniq_idx = top_idx[counts[top_idx] > 0]     # drop zeros if any\n",
        "        Nu = uniq_idx.numel()\n",
        "        # Map global anchor id -> position in compact set\n",
        "        pos_map = torch.full((Nr,), -1, device=x_query.device, dtype=torch.long)\n",
        "        pos_map[uniq_idx] = torch.arange(Nu, device=x_query.device)\n",
        "\n",
        "        # B) Learn per-anchor diagonal components for ONLY the kept anchors,\n",
        "        #    BUT using neighbors drawn from the FULL pool (global params).\n",
        "        #    Q = x_anchor[uniq_idx], R = x_anchor\n",
        "        Q_comp = x_anchor[uniq_idx]                       # [Nu,D]\n",
        "        # comp_k+1 to allow removing self if it appears\n",
        "        d2_all, idx_all = _knn_topk_across_chunks(Q_comp, x_anchor, k=min(comp_k+1, Nr), ref_chunk=ref_chunk)  # [Nu,comp_k+1]\n",
        "\n",
        "        # Remove self if present; if not present, drop farthest to keep size = comp_k\n",
        "        ar = uniq_idx.view(-1,1).expand(-1, idx_all.shape[1])        # [Nu,comp_k+1] (global ids)\n",
        "        is_self = (idx_all == ar)\n",
        "        has_self = is_self.any(dim=1)\n",
        "\n",
        "        keep_mask = torch.ones_like(idx_all, dtype=torch.bool)\n",
        "        if has_self.any():\n",
        "            self_pos = torch.argmax(is_self.int(), dim=1)\n",
        "            keep_mask[torch.arange(Nu, device=x_query.device), self_pos] = False\n",
        "        need_drop_farthest = ~has_self\n",
        "        if need_drop_farthest.any():\n",
        "            far_pos = torch.argmax(d2_all, dim=1)\n",
        "            keep_mask[torch.arange(Nu, device=x_query.device)[need_drop_farthest], far_pos[need_drop_farthest]] = False\n",
        "\n",
        "        idx_comp = idx_all[keep_mask].view(Nu, -1)  # [Nu, comp_k]\n",
        "        d2_comp  = d2_all[keep_mask].view(Nu, -1)   # [Nu, comp_k]\n",
        "\n",
        "        # Local RBF weights with bandwidth h^2 = max d2 among neighbors (per anchor)\n",
        "        tiny = torch.finfo(dtype).tiny\n",
        "        h2_c = torch.maximum(d2_comp.max(dim=1).values, torch.tensor(tiny, device=device, dtype=dtype))  # [Nu]\n",
        "        w_c  = torch.exp(-d2_comp / (2.0 * h2_c[:, None]))                                               # [Nu,comp_k]\n",
        "        denom_c = w_c.sum(dim=1, keepdim=True).clamp_min(torch.finfo(dtype).eps)\n",
        "        X_nb_c = x_anchor[idx_comp]                                                                       # [Nu,comp_k,D]\n",
        "\n",
        "        mu_c  = (w_c.unsqueeze(-1) * X_nb_c).sum(dim=1) / denom_c                                        # [Nu,D]\n",
        "        diffc = X_nb_c - mu_c.unsqueeze(1)                                                                # [Nu,comp_k,D]\n",
        "        var_c = (w_c.unsqueeze(-1) * (diffc * diffc)).sum(dim=1) / denom_c                                # [Nu,D]\n",
        "        tau_c = ridge_frac * var_c.mean(dim=1, keepdim=True)                                              # [Nu,1]\n",
        "        varr_c = (var_c + tau_c).clamp_min(torch.finfo(dtype).eps)                                        # [Nu,D]\n",
        "\n",
        "        # Precompute log det for diagonal Gaussians\n",
        "        two_pi = torch.tensor(2.0 * math.pi, device=device, dtype=dtype)\n",
        "        logdet_c = torch.sum(torch.log(two_pi * varr_c), dim=1)                                           # [Nu]\n",
        "\n",
        "        # C) Evaluate mixture only over the gated anchors per query\n",
        "        # Remap per-query indices to compact positions and mask dropped anchors\n",
        "        pos = pos_map[idx_qk]                         # [Nq,k_mix] in [-1,Nu-1]\n",
        "        mask_valid = pos.ge(0)\n",
        "        pos_clamped = pos.clamp_min(0)\n",
        "\n",
        "        mu_sel   = mu_c[pos_clamped]                  # [Nq,k_mix,D]\n",
        "        var_sel  = varr_c[pos_clamped]                # [Nq,k_mix,D]\n",
        "        logd_sel = logdet_c[pos_clamped]              # [Nq,k_mix]\n",
        "\n",
        "        dX   = x_query[:, None, :] - mu_sel\n",
        "        quad = torch.sum(dX * (dX / var_sel), dim=2)  # [Nq,k_mix]\n",
        "        logw = -0.5 * (quad + logd_sel)\n",
        "        logw.masked_fill_(~mask_valid, -float(\"inf\"))\n",
        "\n",
        "        m = torch.max(logw, dim=1, keepdim=True).values\n",
        "        W = torch.exp(logw - m)\n",
        "        comp_score = -(dX / var_sel)                  # [Nq,k_mix,D]\n",
        "        num = torch.einsum(\"nk,nkd->nd\", W, comp_score)\n",
        "        den = W.sum(dim=1).clamp_min(torch.finfo(dtype).eps)\n",
        "        return num / den[:, None]\n",
        "        '''\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def precompute_proxy_scores(dataset_cpu: torch.Tensor,\n",
        "                                anchor_pool: torch.Tensor,\n",
        "                                *,\n",
        "                                batch: int = PRECOMP_BATCH,\n",
        "                                k: int | None = None,\n",
        "                                ridge_frac: float = PROXY_RIDGE_FRAC,\n",
        "                                ref_chunk: int = PROXY_REF_CHUNK,\n",
        "                                recompute: bool = PROXY_RECOMPUTE,\n",
        "                                gmm_batch: int = PROXY_GMM_BATCH,\n",
        "                                recompute_max_anchors: int = PROXY_RECOMPUTE_MAX_ANCHORS) -> torch.Tensor:\n",
        "        N, D = dataset_cpu.shape\n",
        "        out = torch.empty(N, D, dtype=torch.float32)\n",
        "        for start in range(0, N, batch):\n",
        "            end = min(start + batch, N)\n",
        "            q = dataset_cpu[start:end].to(DEVICE, non_blocking=True)\n",
        "            s = kernel_proxy_score_torch(\n",
        "                q, anchor_pool, k=k, ridge_frac=ridge_frac, ref_chunk=ref_chunk,\n",
        "                recompute=recompute, gmm_batch=gmm_batch, recompute_max_anchors=recompute_max_anchors,\n",
        "                k_mix=PROXY_K_MIX, comp_k=PROXY_COMP_K)   #NEW\n",
        "            # Repair any non-finite rows by falling back to the fast estimator\n",
        "            bad = ~torch.isfinite(s).all(dim=1)\n",
        "            if bad.any():\n",
        "                s[bad] = kernel_proxy_score_torch(\n",
        "                    q[bad], anchor_pool, k=k, ridge_frac=ridge_frac, ref_chunk=ref_chunk,\n",
        "                    recompute=False, gmm_batch=gmm_batch)\n",
        "            out[start:end] = s.detach().to('cpu')\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    # ----------------- Nets ----------------------\n",
        "    class LambdaNet(nn.Module):\n",
        "        \"\"\"Small MLP → scalar λ∈(0,1).\"\"\"\n",
        "        def __init__(self, dim: int, hidden: int = 128):\n",
        "            super().__init__()\n",
        "            self.f = nn.Sequential(\n",
        "                nn.Linear(dim + 1, hidden), nn.SiLU(),\n",
        "                nn.Linear(hidden, hidden),  nn.SiLU(),\n",
        "                nn.Linear(hidden, 1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "        def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
        "            return self.f(torch.cat([x, torch.log(t)], dim=1)).squeeze(-1)\n",
        "\n",
        "    class CriticNet(nn.Module):\n",
        "        \"\"\"Outputs σ_t-scaled score estimate (we unscale at eval).\"\"\"\n",
        "        def __init__(self, dim: int, hidden: int = 512):\n",
        "            super().__init__()\n",
        "            self.f = nn.Sequential(\n",
        "                nn.Linear(dim + 1, hidden), nn.SiLU(),\n",
        "                nn.Linear(hidden, hidden),  nn.SiLU(),\n",
        "                nn.Linear(hidden, dim),\n",
        "            )\n",
        "        def forward(self, y: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
        "            return self.f(torch.cat([y, torch.log(t)], dim=1))\n",
        "\n",
        "    def update_ema(target: nn.Module, src: nn.Module, decay: float):\n",
        "        with torch.no_grad():\n",
        "            for p_t, p_s in zip(target.parameters(), src.parameters()):\n",
        "                p_t.data.mul_(decay).add_(p_s.data, alpha=(1.0 - decay))\n",
        "\n",
        "    # ----------------- Build targets -----------------\n",
        "    params, sampler_func, score_func = get_gmm_funcs(NUM_C, k_dim=K_DIM, variant=VARIANT, comp_std=STD,\n",
        "                                                    overall_scale=SCALE, seed=0, embedding_mode = EMBED_MODE)[:3]\n",
        "\n",
        "    means0_cp, stds0_cp, w0_cp = map(cp.asarray, params)\n",
        "    means0_np, stds0_np, w0_np = params\n",
        "    means0 = torch.as_tensor(means0_np, device=DEVICE).float().contiguous()\n",
        "    stds0  = torch.as_tensor(stds0_np,  device=DEVICE).float().contiguous()\n",
        "    w0     = torch.as_tensor(w0_np,     device=DEVICE).float().contiguous(); w0 /= w0.sum()\n",
        "\n",
        "\n",
        "    def true_score_func_cp(y_cp, t_scalar):\n",
        "        return calculate_true_score_at_t(\n",
        "            y_cp, t_scalar, means0_cp, stds0_cp, w0_cp, batch_size=4096\n",
        "        )\n",
        "\n",
        "\n",
        "    # --- Build a large anchor pool (prior samples) for proxy s0 ---\n",
        "    with torch.no_grad():\n",
        "        proxy_pool = torch_sample_gmm(PROXY_N, means0, stds0, w0)  # [PROXY_N, D]\n",
        "\n",
        "    # --- Reference anchors used in KSS/TWD teachers (always needed) ---\n",
        "    with torch.no_grad():\n",
        "        x_ref  = torch_sample_gmm(N_REF, means0, stds0, w0)  # [N,D]\n",
        "\n",
        "    # --- Build s0_ref depending on USE_PROXY_SCORES ---\n",
        "    if USE_PROXY_SCORES:\n",
        "        # Build a large anchor pool (prior samples) for proxy s0\n",
        "        with torch.no_grad():\n",
        "            proxy_pool = torch_sample_gmm(PROXY_N, means0, stds0, w0)                              # [PROXY_N, D]\n",
        "            s0_ref = kernel_proxy_score_torch(x_ref, proxy_pool,\n",
        "                                              recompute_max_anchors=PROXY_RECOMPUTE_MAX_ANCHORS,   # NEW\n",
        "                                              k_mix=PROXY_K_MIX, comp_k=PROXY_COMP_K,)             # PROXY teacher on refs\n",
        "    else:\n",
        "        # TRUE initial scores (no proxy pool or KDE/GMM work)\n",
        "        with torch.no_grad():\n",
        "            s0_ref = torch_score_gmm_batch(x_ref, means0, stds0, w0)\n",
        "\n",
        "    D = means0.shape[1]\n",
        "\n",
        "    # ------- Gate (λ-net) -------\n",
        "    lam_net     = LambdaNet(D, hidden=HIDDEM_LAM).to(DEVICE)\n",
        "    lam_net_ema = LambdaNet(D, hidden=HIDDEM_LAM).to(DEVICE)\n",
        "    lam_net_ema.load_state_dict(lam_net.state_dict())\n",
        "    print(f\"[INFO] Gate init: random (D={D}).\")\n",
        "\n",
        "    # ------- Critic -------\n",
        "    critic      = CriticNet(D, hidden=HIDDEN).to(DEVICE)\n",
        "    critic_ema  = CriticNet(D, hidden=HIDDEN).to(DEVICE)\n",
        "    critic_ema.load_state_dict(critic.state_dict())\n",
        "\n",
        "    # Unscaled EMA score from the critic.\n",
        "    # During training the critic target was:\n",
        "    #   q_scaled = sigma_t * ((1-λ) * a + λ * b)  ≈  sigma_t * s_theta(x_t, t)\n",
        "    # so at inference we simply undo the scale:\n",
        "    #   s_theta(x_t, t) ≈ q_scaled / sigma_t\n",
        "\n",
        "\n",
        "    # optional compile\n",
        "    try:\n",
        "        if DEVICE == \"cuda\":\n",
        "            lam_net     = torch.compile(lam_net)\n",
        "            critic      = torch.compile(critic)\n",
        "            lam_net_ema = torch.compile(lam_net_ema)\n",
        "            critic_ema  = torch.compile(critic_ema)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    opt_lam    = optim.Adam(lam_net.parameters(), lr=LR_INIT*LR_LAM_SCALE, weight_decay=WD_LAM)\n",
        "    opt_crit   = optim.Adam(critic.parameters(),  lr=LR_INIT)\n",
        "    sched_lam  = torch.optim.lr_scheduler.CosineAnnealingLR(opt_lam,  T_max=STEPS, eta_min=LR_MIN*LR_LAM_SCALE)\n",
        "    sched_crit = torch.optim.lr_scheduler.CosineAnnealingLR(opt_crit, T_max=STEPS, eta_min=LR_MIN)\n",
        "    scaler_crit = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n",
        "    scaler_lam  = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n",
        "    mse        = nn.MSELoss()\n",
        "\n",
        "    def update_all_ema():\n",
        "        update_ema(lam_net_ema, lam_net, EMA_DECAY)\n",
        "        update_ema(critic_ema, critic, EMA_DECAY)\n",
        "\n",
        "    # ----------------- Fixed training dataset (CPU pinned, deterministic) -----------------\n",
        "    TRAIN_SEED  = int(globals().get(\"TRAIN_SEED\", globals().get(\"SEED\", 0)))\n",
        "    _gen_cpu    = torch.Generator(device='cpu').manual_seed(TRAIN_SEED)\n",
        "\n",
        "    def _sample_gmm_cpu_with_gen(n, means_cpu, stds_cpu, w_cpu, gen):\n",
        "        K, D = means_cpu.shape\n",
        "        idx   = torch.multinomial(w_cpu, num_samples=n, replacement=True, generator=gen)\n",
        "        noise = torch.randn(n, D, generator=gen, dtype=means_cpu.dtype)\n",
        "        return means_cpu[idx] + noise * stds_cpu[idx].unsqueeze(1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _means_cpu = means0.detach().cpu().float()\n",
        "        _stds_cpu  = stds0.detach().cpu().float()\n",
        "        _w_cpu     = (w0 / w0.sum()).detach().cpu().float()\n",
        "\n",
        "        x0_train_cpu  = _sample_gmm_cpu_with_gen(N_Train, _means_cpu, _stds_cpu, _w_cpu, _gen_cpu).pin_memory()\n",
        "        u_train_cpu   = torch.rand(N_Train, 1, generator=_gen_cpu, dtype=torch.float32).pin_memory()\n",
        "        eps_train_cpu = torch.randn(N_Train, _means_cpu.shape[1], generator=_gen_cpu, dtype=torch.float32).pin_memory()\n",
        "\n",
        "    # --- Optional precompute of s0 for training pool (only for PROXY mode) ---\n",
        "    S0_TRAIN_CPU = None\n",
        "    if USE_PROXY_SCORES and PRECOMPUTE_TRAIN_SCORES:\n",
        "        print(f\"[INFO] Precomputing learned s0 proxies for training pool of size {N_Train}...\"\n",
        "              + (\" (recompute/KDE mode)\" if PROXY_RECOMPUTE else \"\"))\n",
        "        S0_TRAIN_CPU = precompute_proxy_scores(\n",
        "            x0_train_cpu, proxy_pool, batch=PRECOMP_BATCH\n",
        "        ).pin_memory()\n",
        "        print(\"[INFO] Done precomputing s0 proxies.\")\n",
        "\n",
        "    _train_ptr  = 0\n",
        "    _train_perm = torch.randperm(N_Train, generator=_gen_cpu)\n",
        "\n",
        "    def _next_batch_indices(B):\n",
        "        global _train_ptr, _train_perm\n",
        "        if _train_ptr + B <= N_Train:\n",
        "            idx = _train_perm[_train_ptr:_train_ptr+B]\n",
        "            _train_ptr += B\n",
        "            return idx\n",
        "        tail = _train_perm[_train_ptr:]\n",
        "        _train_perm = torch.randperm(N_Train, generator=_gen_cpu)\n",
        "        _train_ptr  = 0\n",
        "        need = B - tail.numel()\n",
        "        head = _train_perm[_train_ptr:_train_ptr+need]\n",
        "        _train_ptr += need\n",
        "        return torch.cat([tail, head], dim=0)\n",
        "\n",
        "    # ----------------- NP teachers using proxy s0_ref -----------------\n",
        "    @torch.no_grad()\n",
        "    def kss_score_torch(y: torch.Tensor, t: torch.Tensor, x_ref: torch.Tensor, s0_ref: torch.Tensor) -> torch.Tensor:\n",
        "        B, D = y.shape\n",
        "        N    = x_ref.shape[0]\n",
        "        et   = torch.exp(-t); et2 = torch.exp(-2.0 * t)\n",
        "        var  = (1.0 - et2).clamp_min(1e-12); invv = 1.0 / var\n",
        "        mu   = et.view(B,1,1) * x_ref.view(1,N,D)\n",
        "        diff = y.view(B,1,D) - mu\n",
        "        d2   = (diff**2).sum(-1)\n",
        "        logw = -0.5 * invv.view(B,1) * d2\n",
        "        w    = torch.softmax(logw, dim=1)\n",
        "        return torch.exp(t).view(B,1) * (w @ s0_ref)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def tweedie_score_torch(y: torch.Tensor, t: torch.Tensor, x_ref: torch.Tensor) -> torch.Tensor:\n",
        "        B, D = y.shape\n",
        "        N    = x_ref.shape[0]\n",
        "        et   = torch.exp(-t); et2 = torch.exp(-2.0 * t)\n",
        "        var  = (1.0 - et2).clamp_min(1e-12); invv = 1.0 / var\n",
        "        mu   = et.view(B,1,1) * x_ref.view(1,N,D)\n",
        "        diff = y.view(B,1,D) - mu\n",
        "        d2   = (diff**2).sum(-1)\n",
        "        logw = -0.5 * invv.view(B,1) * d2\n",
        "        w    = torch.softmax(logw, dim=1)\n",
        "        mu_bar = (w.unsqueeze(-1) * mu).sum(dim=1)\n",
        "        return -invv.view(B,1) * (y - mu_bar)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def snis_lambda_torch(\n",
        "        y, t, x_ref, s0_ref,\n",
        "        s_kss=None, s_twd=None, eps: float = 1e-12,\n",
        "        max_ref: int | None = 10_000, chunk: int = 4096\n",
        "    ):\n",
        "        \"\"\"Streaming SNIS λ*: (Var[â]-Cov[â,b̂])/(Var[â]+Var[b̂]-2Cov[â,b̂]).\"\"\"\n",
        "        B, D = y.shape\n",
        "        N = x_ref.shape[0]\n",
        "        if (max_ref is not None) and (N > max_ref):\n",
        "            idx = torch.randperm(N, device=x_ref.device)[:max_ref]\n",
        "            x_ref  = x_ref[idx]\n",
        "            s0_ref = s0_ref[idx]\n",
        "            N = x_ref.shape[0]\n",
        "\n",
        "        et   = torch.exp(-t)                     # [B,1]\n",
        "        et2  = torch.exp(-2.0 * t)\n",
        "        var  = (1.0 - et2).clamp_min(1e-12)      # [B,1]\n",
        "        invv = 1.0 / var\n",
        "\n",
        "        # Pass 1\n",
        "        m = torch.full((B,), -float(\"inf\"), device=y.device, dtype=y.dtype)\n",
        "        s = torch.zeros(B, device=y.device, dtype=y.dtype)\n",
        "        acc_a = torch.zeros(B, D, device=y.device, dtype=y.dtype)\n",
        "        acc_b = torch.zeros(B, D, device=y.device, dtype=y.dtype)\n",
        "\n",
        "        for start in range(0, N, chunk):\n",
        "            end = min(start + chunk, N)\n",
        "            xr = x_ref[start:end]\n",
        "            s0 = s0_ref[start:end]\n",
        "            mu   = et.view(B,1,1) * xr.view(1,-1,D)\n",
        "            diff = y.view(B,1,D) - mu\n",
        "            d2   = (diff**2).sum(-1)\n",
        "            L    = -0.5 * invv.view(B,1) * d2\n",
        "            m_new = torch.maximum(m, L.max(dim=1).values)\n",
        "            scale = torch.exp(m - m_new)\n",
        "            wtil  = torch.exp(L - m_new[:,None])\n",
        "\n",
        "            a_i   = torch.exp(t).view(B,1,1) * s0.view(1,-1,D)\n",
        "            b_i   = -(invv.view(B,1,1)) * (y.view(B,1,D) - mu)\n",
        "            s     = s * scale + wtil.sum(1)\n",
        "            acc_a = acc_a * scale[:,None] + torch.einsum('bn,bnd->bd', wtil, a_i)\n",
        "            acc_b = acc_b * scale[:,None] + torch.einsum('bn,bnd->bd', wtil, b_i)\n",
        "            m = m_new\n",
        "\n",
        "        mu_a = acc_a / s[:,None]\n",
        "        mu_b = acc_b / s[:,None]\n",
        "\n",
        "        # Pass 2\n",
        "        S0    = torch.zeros(B, device=y.device, dtype=y.dtype)\n",
        "        Vknum = torch.zeros(B, device=y.device, dtype=y.dtype)\n",
        "        Vtnum = torch.zeros(B, device=y.device, dtype=y.dtype)\n",
        "        Cnum  = torch.zeros(B, device=y.device, dtype=y.dtype)\n",
        "        logZ  = torch.log(s) + m\n",
        "\n",
        "        for start in range(0, N, chunk):\n",
        "            end = min(start + chunk, N)\n",
        "            xr = x_ref[start:end]\n",
        "            s0 = s0_ref[start:end]\n",
        "            mu   = et.view(B,1,1) * xr.view(1,-1,D)\n",
        "            diff = y.view(B,1,D) - mu\n",
        "            d2   = (diff**2).sum(-1)\n",
        "            L    = -0.5 * invv.view(B,1) * d2\n",
        "            w    = torch.exp(L - logZ[:,None])\n",
        "            w2   = w * w\n",
        "            S0  += w2.sum(1)\n",
        "\n",
        "            a_i  = torch.exp(t).view(B,1,1) * s0.view(1,-1,D)\n",
        "            b_i  = -(invv.view(B,1,1)) * (y.view(B,1,D) - mu)\n",
        "            ac   = a_i - mu_a[:,None,:]\n",
        "            bc   = b_i - mu_b[:,None,:]\n",
        "\n",
        "            Vknum += (w2[:,:,None] * (ac*ac)).sum((1,2))\n",
        "            Vtnum += (w2[:,:,None] * (bc*bc)).sum((1,2))\n",
        "            Cnum  += (w2[:,:,None] * (ac*bc)).sum((1,2))\n",
        "\n",
        "        den = (1.0 - S0).clamp_min(1e-10)\n",
        "        Vk  = Vknum / den\n",
        "        Vt  = Vtnum / den\n",
        "        C   =  Cnum / den\n",
        "        lam = ((Vk - C) / (Vk + Vt - 2.0*C + eps)).clamp(0.0, 1.0)\n",
        "        return lam\n",
        "\n",
        "    # ----------------- Batch maker (log-uniform t) -----------------\n",
        "    _train_indices_last = None  # for exposing the last indices to caller\n",
        "    @torch.no_grad()\n",
        "    def make_batch(B, *, ref_chunk=None, max_ref=None, t_mode='log', return_idx=True):\n",
        "        global _train_indices_last\n",
        "        if ref_chunk is None:\n",
        "            ref_chunk = min(4096, int(globals().get(\"N_REF\", 4096)))\n",
        "        if max_ref is None:\n",
        "            max_ref = int(globals().get(\"N_REF\", 10_000))\n",
        "\n",
        "        # fixed pool slicing\n",
        "        idx = _next_batch_indices(B)\n",
        "        _train_indices_last = idx  # store\n",
        "        x0  = x0_train_cpu[idx].to(DEVICE, non_blocking=True)\n",
        "        u   = u_train_cpu[idx].to(DEVICE, non_blocking=True)\n",
        "        eps = eps_train_cpu[idx].to(DEVICE, non_blocking=True)\n",
        "\n",
        "        if t_mode == 'log':\n",
        "            logTmin = math.log(T_MIN); logTmax = math.log(T_MAX)\n",
        "            t = torch.exp(torch.tensor(logTmin, device=DEVICE) + u * (logTmax - logTmin))\n",
        "        elif t_mode == 'uniform':\n",
        "            t = torch.tensor(T_MIN, device=DEVICE) + u * (T_MAX - T_MIN)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown t_mode: {t_mode}\")\n",
        "\n",
        "        et    = torch.exp(-t)\n",
        "        sigma = torch.sqrt((1.0 - torch.exp(-2.0 * t)).clamp_min(1e-12))\n",
        "        x_t   = et * x0 + sigma * eps\n",
        "\n",
        "        # streamed NP teachers (for metrics and SNIS λ)\n",
        "        x_ref_src, s0_ref_src = x_ref, s0_ref\n",
        "        N_total = x_ref_src.shape[0]\n",
        "        if (max_ref is not None) and (N_total > max_ref):\n",
        "            sel = torch.randperm(N_total, generator=_gen_cpu, device='cpu')[:max_ref]\n",
        "            x_ref_src  = x_ref_src[sel]\n",
        "            s0_ref_src = s0_ref_src[sel]\n",
        "            N_total    = max_ref\n",
        "\n",
        "        Bsz, D = x_t.shape\n",
        "        y      = x_t\n",
        "        invv   = 1.0 / (1.0 - torch.exp(-2.0 * t)).clamp_min(1e-12)\n",
        "        y2     = (y*y).sum(-1)\n",
        "\n",
        "        m      = torch.full((Bsz,), -float(\"inf\"), device=DEVICE, dtype=y.dtype)\n",
        "        s      = torch.zeros(Bsz, device=DEVICE, dtype=y.dtype)\n",
        "        num_x  = torch.zeros(Bsz, D, device=DEVICE, dtype=y.dtype)\n",
        "        num_s0 = torch.zeros(Bsz, D, device=DEVICE, dtype=y.dtype)\n",
        "\n",
        "        for start in range(0, N_total, ref_chunk):\n",
        "            end = min(start + ref_chunk, N_total)\n",
        "            xr  = x_ref_src[start:end].to(DEVICE, non_blocking=True).contiguous()\n",
        "            s0  = s0_ref_src[start:end].to(DEVICE, non_blocking=True).contiguous()\n",
        "\n",
        "            xr2   = (xr*xr).sum(-1)\n",
        "            cross = y @ xr.T\n",
        "            d2    = y2[:,None] + (et*et) * xr2[None,:] - 2.0 * et * cross\n",
        "            L     = -0.5 * invv * d2\n",
        "\n",
        "            m_new = torch.maximum(m, L.max(dim=1).values)\n",
        "            scale = torch.exp(m - m_new)\n",
        "            wtil  = torch.exp(L - m_new[:,None])\n",
        "\n",
        "            s      = s * scale + wtil.sum(1)\n",
        "            num_x  = num_x  * scale[:,None] + torch.einsum('bn,nd->bd', wtil, xr)\n",
        "            num_s0 = num_s0 * scale[:,None] + torch.einsum('bn,nd->bd', wtil, s0)\n",
        "            m      = m_new\n",
        "\n",
        "        x_bar  = num_x  / (s[:,None] + 1e-30)\n",
        "        s0_bar = num_s0 / (s[:,None] + 1e-30)\n",
        "\n",
        "        s_kss  = torch.exp(t) * s0_bar\n",
        "        s_twd  = -invv * (y - torch.exp(-t) * x_bar)\n",
        "        s_true = true_score_xt_torch(x_t, t, means0, stds0, w0)\n",
        "\n",
        "        if return_idx:\n",
        "            return x_t, t, sigma, s_kss, s_twd, s_true, x0, idx\n",
        "        else:\n",
        "            return x_t, t, sigma, s_kss, s_twd, s_true, x0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample_evolved_gmm(n: int, t_scalar: float,\n",
        "                          means0: torch.Tensor, stds0: torch.Tensor, w0: torch.Tensor) -> torch.Tensor:\n",
        "        # Draw x_t directly from the OU-evolved mixture (means_t, stds_t).\n",
        "        t  = torch.tensor(t_scalar, device=means0.device, dtype=means0.dtype)\n",
        "        et = torch.exp(-t); et2 = torch.exp(-2.0 * t)\n",
        "        means_t = means0 * et                               # [K,D]\n",
        "        var_t   = stds0**2 * et2 + (1.0 - et2)             # [K,D]\n",
        "        stds_t  = torch.sqrt(var_t.clamp_min(1e-12))       # [K,D]\n",
        "        return torch_sample_gmm(n, means_t, stds_t, w0)\n",
        "\n",
        "    # ----------------- Eval (γ-grid) -----------------\n",
        "    @torch.no_grad()\n",
        "    def eval_gamma_grid_rmse(n_t: int = EVAL_N_T, per_t: int = EVAL_PER_T):\n",
        "        t_grid = make_gamma_times(n_t=n_t)\n",
        "        sums = {k: 0.0 for k in [\n",
        "            \"KSSNP_TRUE\",\"TWDNP_TRUE\",\"BLNDNP_TRUE_NP\",\"BLNDNP_TRUE_NN\",\n",
        "            \"LAM_ABS_DIFF\",\"LAM_PEARSON\",\"Critic_RMSE\"\n",
        "        ]}\n",
        "        cnt_total = 0\n",
        "        sum_np = 0.0; sum_nn = 0.0\n",
        "        sum_np2 = 0.0; sum_nn2 = 0.0; sum_prod = 0.0\n",
        "\n",
        "        for t in t_grid.tolist():\n",
        "            x_eval = sample_evolved_gmm(per_t, t, means0, stds0, w0)\n",
        "            tcol   = torch.full((per_t,1), t, device=DEVICE, dtype=x_eval.dtype)\n",
        "            sigma  = torch.sqrt((1.0 - torch.exp(-2.0*tcol)).clamp_min(1e-12))\n",
        "\n",
        "            s_true = true_score_xt_torch(x_eval, tcol, means0, stds0, w0)\n",
        "            s_kss  = kss_score_torch(x_eval, tcol, x_ref, s0_ref)       # uses proxy s0_ref\n",
        "            s_twd  = tweedie_score_torch(x_eval, tcol, x_ref)\n",
        "\n",
        "            pred_crit = critic_ema(x_eval, tcol) / sigma\n",
        "\n",
        "            lam_np = snis_lambda_torch(x_eval, tcol, x_ref, s0_ref, s_kss, s_twd).view(-1,1)\n",
        "            lam_nn = lam_net_ema(x_eval, tcol).view(-1,1)\n",
        "\n",
        "            s_bl_np_np = (1 - lam_np)*s_kss + lam_np*s_twd\n",
        "            s_bl_np_nn = (1 - lam_nn)*s_kss + lam_nn*s_twd\n",
        "\n",
        "            sums[\"KSSNP_TRUE\"]     += rmse_t(s_kss,   s_true).item()\n",
        "            sums[\"TWDNP_TRUE\"]     += rmse_t(s_twd,   s_true).item()\n",
        "            sums[\"BLNDNP_TRUE_NP\"] += rmse_t(s_bl_np_np, s_true).item()\n",
        "            sums[\"BLNDNP_TRUE_NN\"] += rmse_t(s_bl_np_nn, s_true).item()\n",
        "            sums[\"Critic_RMSE\"]    += rmse_t(pred_crit, s_true).item()\n",
        "\n",
        "            d = (lam_nn - lam_np).abs().mean().item()\n",
        "            sums[\"LAM_ABS_DIFF\"]   += d\n",
        "\n",
        "            ln = lam_nn.view(-1); lp = lam_np.view(-1)\n",
        "            cnt_total += ln.numel()\n",
        "            sum_nn  += ln.sum().item(); sum_np  += lp.sum().item()\n",
        "            sum_nn2 += (ln*ln).sum().item(); sum_np2 += (lp*lp).sum().item()\n",
        "            sum_prod+= (ln*lp).sum().item()\n",
        "\n",
        "        for k in sums:\n",
        "            if k != \"LAM_PEARSON\":\n",
        "                sums[k] /= len(t_grid)\n",
        "\n",
        "        mean_nn = sum_nn / cnt_total; mean_np = sum_np / cnt_total\n",
        "        var_nn  = max(0.0, sum_nn2 / cnt_total - mean_nn**2)\n",
        "        var_np  = max(0.0, sum_np2 / cnt_total - mean_np**2)\n",
        "        cov     = (sum_prod / cnt_total) - mean_nn*mean_np\n",
        "        den = max(1e-12, math.sqrt(max(var_nn, 0.0) * max(var_np, 0.0)))\n",
        "        sums[\"LAM_PEARSON\"] = (cov / den) if den > 0 else 0.0\n",
        "        return sums\n",
        "\n",
        "\n",
        "    # ===== add once, above your training loop =====\n",
        "    import numpy as np, cupy as cp, torch\n",
        "    @torch.no_grad()\n",
        "    def ema_score_xt_torch(x_t: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
        "        if t.dim() == 1: t = t[:, None]\n",
        "        sigma_t  = torch.sqrt((1.0 - torch.exp(-2.0 * t)).clamp_min(1e-12))\n",
        "        q_scaled = critic_ema(x_t, t)\n",
        "        s = q_scaled / sigma_t\n",
        "        return torch.nan_to_num(s, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    def make_custom_score_from_torch(score_xt_torch, device):\n",
        "        def score_cp(y_cp, t_scalar):\n",
        "            y_np = cp.asnumpy(y_cp)\n",
        "            x_t  = torch.as_tensor(y_np, device=device, dtype=torch.float32)\n",
        "            t    = torch.full((x_t.shape[0], 1), float(t_scalar), device=device, dtype=x_t.dtype)\n",
        "            s    = score_xt_torch(x_t, t).detach().cpu().numpy().astype(np.float64)\n",
        "            return cp.asarray(s)\n",
        "        return score_cp\n",
        "\n",
        "    # --- your EXACT blackbox sampler ---\n",
        "    def blackbox_sampler(\n",
        "        N_part, time_pts, custom_score, sampler_func, *,\n",
        "        mode=\"heun_hop\", h_coeff=0.5, true_score_func=None,\n",
        "        p_prune = 0, likelyhood_func = None, loglik_grad_fn = None):\n",
        "        xp  = cp\n",
        "        dim = int(sampler_func(5).shape[1])           # match model/data dim\n",
        "        y   = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "        rmse_list = []                                # collect RMSE vs oracle\n",
        "\n",
        "        for k in range(len(time_pts)-1):\n",
        "            t_cur, t_prev = time_pts[k], time_pts[k+1]\n",
        "            dt    = t_cur - t_prev\n",
        "            noise = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "            S_cur  = custom_score(y, t_cur)           # [N_part, dim]\n",
        "            y_hat  = y + dt*(y + 2*S_cur) + xp.sqrt(2.0*dt)*noise\n",
        "            S_prev = custom_score(y_hat, t_prev)\n",
        "\n",
        "            if mode == \"heun_hop\":\n",
        "                A = xp.exp(dt); B = xp.exp(2*dt) - 1.0; C = xp.sqrt(B); D = B / A\n",
        "                y = A*y + h_coeff*B*S_prev + (1-h_coeff)*D*S_cur + C*noise\n",
        "            elif mode == \"ou_sde\":\n",
        "                y = y_hat\n",
        "            elif mode == \"pf_ode\":\n",
        "                y = y + dt*(y + S_cur)\n",
        "            elif mode == \"heun_pc\":\n",
        "                drift_avg = 0.5*(y + 2*S_cur) + 0.5*(y_hat + 2*S_prev)\n",
        "                y = y + dt*drift_avg + xp.sqrt(2.0 * dt) * noise\n",
        "            else:\n",
        "                raise ValueError(mode)\n",
        "\n",
        "        if true_score_func is not None:\n",
        "            S_cur_true  = true_score_func(y, t_cur)\n",
        "            S_prev_true = true_score_func(y_hat, t_prev)\n",
        "\n",
        "            S_cur_safe,  cur_idx_safe  = prune_cp_arr(S_cur,  p_percent = p_prune)\n",
        "            S_prev_safe, prev_idx_safe = prune_cp_arr(S_prev, p_percent = p_prune)\n",
        "\n",
        "            RMSE_cur  = float(xp_rmse(S_cur_safe,  S_cur_true[cur_idx_safe]))\n",
        "            RMSE_prev = float(xp_rmse(S_prev_safe, S_prev_true[prev_idx_safe]))\n",
        "\n",
        "            rmse_list.append(RMSE_cur)\n",
        "            rmse_list.append(RMSE_prev)\n",
        "            return y, rmse_list\n",
        "        else:\n",
        "            return y\n",
        "\n",
        "    # build once and reuse\n",
        "    _CUSTOM_SCORE = make_custom_score_from_torch(ema_score_xt_torch, DEVICE)\n",
        "\n",
        "    def probe_with_run_comparison(epoch_or_step: int, N_ref = 2000, N_part = 2000,\n",
        "                                  nsteps: int = 8 , trials = 3, p_prune = 50, plot_hists = False):\n",
        "        # force run_comparison to use THIS blackbox sampler\n",
        "        run_comparison.__globals__['blackbox_sampler'] = blackbox_sampler\n",
        "\n",
        "        samplers = [\n",
        "            ('heun_pc', 'custom', 'NN-Critic-EMA', _CUSTOM_SCORE),\n",
        "        ]\n",
        "        _ = run_comparison(\n",
        "            samplers           = samplers,\n",
        "            steps_list         = [16],\n",
        "            true_sampler_func  = sampler_func,\n",
        "            prior_sampler_func = sampler_func,\n",
        "            prior_score_func   = score_func,\n",
        "            true_score_func    = true_score_func_cp,\n",
        "            true_init_score    = score_func,\n",
        "            track_score_rmse   = True,\n",
        "            N_ref              = N_ref,\n",
        "            N_part             = N_part,\n",
        "            trials             = trials,\n",
        "            nrows              = 3,\n",
        "            mean_bins          = 80,\n",
        "            time_split         = 'power',\n",
        "            T_end              = T_end,\n",
        "            T_target           = T_target,\n",
        "            div                = 'M_KSD',\n",
        "            plot_hists         = plot_hists,\n",
        "            hist_mode          = 'pca',\n",
        "            display_mode       = 'min_label',\n",
        "            trial_name         = f'NN_hist_compare_e{epoch_or_step}',\n",
        "            p_prune            = p_prune,\n",
        "            ref_seed           = 1,\n",
        "            plot_res           = False,\n",
        "            hist_norm          = 2.0,\n",
        "            plot_prior         = True,\n",
        "            save_tag           = f\"run_data/BLEND_epoch_{epoch_or_step}\",\n",
        "        )\n",
        "\n",
        "    # ----------------- Training -------------------------\n",
        "    SUBSTEPS = int(globals().get(\"SUBSTEPS\", 1))\n",
        "    REF_CHUNK_TRAIN = min(4096, int(globals().get(\"N_REF\", 4096)))\n",
        "    MAX_REF_TRAIN   = int(globals().get(\"N_REF\", 10_000))\n",
        "\n",
        "    def _s0_teacher_for_batch(x0_batch: torch.Tensor, idx_in_train: torch.Tensor | None):\n",
        "        \"\"\"\n",
        "        Returns s0(x0_batch) from either the PROXY estimator (if enabled) or the TRUE teacher.\n",
        "        Skips all proxy work when USE_PROXY_SCORES=False.\n",
        "        \"\"\"\n",
        "        if USE_PROXY_SCORES:\n",
        "            # Use precomputed or on-the-fly proxy scores\n",
        "            if (S0_TRAIN_CPU is not None) and (idx_in_train is not None):\n",
        "                return S0_TRAIN_CPU[idx_in_train].to(DEVICE, non_blocking=True)\n",
        "            return kernel_proxy_score_torch(\n",
        "                x0_batch, proxy_pool,\n",
        "                recompute=PROXY_RECOMPUTE, gmm_batch=PROXY_GMM_BATCH,\n",
        "                ref_chunk=PROXY_REF_CHUNK, ridge_frac=PROXY_RIDGE_FRAC,\n",
        "                recompute_max_anchors=PROXY_RECOMPUTE_MAX_ANCHORS,\n",
        "                k_mix=PROXY_K_MIX, comp_k=PROXY_COMP_K,\n",
        "            )\n",
        "        else:\n",
        "            # TRUE t=0 GMM score (fast, no proxy deps)\n",
        "            return torch_score_gmm_batch(x0_batch, means0, stds0, w0)\n",
        "\n",
        "\n",
        "    def _s0_proxy_for_batch(x0_batch: torch.Tensor, idx_in_train: torch.Tensor | None):\n",
        "        if (S0_TRAIN_CPU is not None) and (idx_in_train is not None):\n",
        "            return S0_TRAIN_CPU[idx_in_train].to(DEVICE, non_blocking=True)\n",
        "        return kernel_proxy_score_torch(\n",
        "            x0_batch, proxy_pool,\n",
        "            recompute=PROXY_RECOMPUTE, gmm_batch=PROXY_GMM_BATCH,\n",
        "            ref_chunk=PROXY_REF_CHUNK, ridge_frac=PROXY_RIDGE_FRAC,\n",
        "            recompute_max_anchors=PROXY_RECOMPUTE_MAX_ANCHORS,\n",
        "            k_mix=PROXY_K_MIX, comp_k=PROXY_COMP_K,\n",
        "        )\n",
        "\n",
        "    for step in range(1, STEPS+1):\n",
        "        for _ in range(SUBSTEPS):\n",
        "            for _ in range(CRIT_STEPS_PER_LAM):\n",
        "                  x_t, t, sigma_t, s_kss_np, s_twd_np, s_true, x0, idx_train = make_batch(\n",
        "                      BATCH, ref_chunk=REF_CHUNK_TRAIN, max_ref=MAX_REF_TRAIN, t_mode='log', return_idx=True\n",
        "                  )\n",
        "                  #######\n",
        "                  with torch.no_grad():\n",
        "                      et   = torch.exp(-t)\n",
        "                      invv = 1.0 / (1.0 - torch.exp(-2.0 * t)).clamp_min(1e-12)\n",
        "\n",
        "\n",
        "                      s0_base  = _s0_teacher_for_batch(x0, idx_train)\n",
        "                      z_base_twd = x0\n",
        "\n",
        "                      a = torch.exp(t) * s0_base\n",
        "                      b = -invv * (x_t - et * z_base_twd)\n",
        "\n",
        "                      lam_frozen = lam_net_ema(x_t, t).unsqueeze(-1).clamp(LAM_EPS, 1.0 - LAM_EPS)\n",
        "                      z_scaled   = sigma_t * ((1 - lam_frozen) * a + lam_frozen * b)\n",
        "\n",
        "                  opt_crit.zero_grad(set_to_none=True)\n",
        "                  with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
        "                      q_scaled = critic(x_t, t)\n",
        "                      loss_crit = mse(q_scaled, z_scaled)\n",
        "\n",
        "                  scaler_crit.scale(loss_crit).backward()\n",
        "                  scaler_crit.unscale_(opt_crit)\n",
        "                  torch.nn.utils.clip_grad_norm_(critic.parameters(), 1.0)\n",
        "                  scaler_crit.step(opt_crit)\n",
        "                  scaler_crit.update()\n",
        "                  sched_crit.step()\n",
        "                  update_ema(critic_ema, critic, EMA_DECAY)\n",
        "\n",
        "        # ---------------- Gate: 1 step against frozen critic from EMA ---------------\n",
        "        x_t, t, sigma_t, s_kss_np, s_twd_np, s_true, x0, idx_train = make_batch(\n",
        "            BATCH, ref_chunk=REF_CHUNK_TRAIN, max_ref=MAX_REF_TRAIN, t_mode='log', return_idx=True\n",
        "        )\n",
        "        #####\n",
        "        with torch.no_grad():\n",
        "            et   = torch.exp(-t)\n",
        "            invv = 1.0 / (1.0 - torch.exp(-2.0 * t)).clamp_min(1e-12)\n",
        "\n",
        "            s0_base  = _s0_teacher_for_batch(x0, idx_train)\n",
        "            z_base_twd = x0\n",
        "\n",
        "            a = torch.exp(t) * s0_base\n",
        "            b = -invv * (x_t - et * z_base_twd)\n",
        "            q_scaled_frozen = critic_ema(x_t, t)\n",
        "\n",
        "        opt_lam.zero_grad(set_to_none=True)\n",
        "        with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
        "            lam_pred = lam_net(x_t, t).unsqueeze(-1).clamp(LAM_EPS, 1.0 - LAM_EPS)\n",
        "            z_scaled = sigma_t * ((1 - lam_pred) * a + lam_pred * b)\n",
        "            loss_lam = mse(z_scaled, q_scaled_frozen)\n",
        "\n",
        "        if step > WARMUP_LAM_STEPS:\n",
        "            scaler_lam.scale(loss_lam).backward()\n",
        "            scaler_lam.unscale_(opt_lam)\n",
        "            torch.nn.utils.clip_grad_norm_(lam_net.parameters(), 1.0)\n",
        "            scaler_lam.step(opt_lam)\n",
        "            scaler_lam.update()\n",
        "            sched_lam.step()\n",
        "            update_ema(lam_net_ema, lam_net, EMA_DECAY)\n",
        "        else:\n",
        "            sched_lam.step()\n",
        "\n",
        "    # ---------------- Logging every PRINT_EVERY steps ----------------\n",
        "        if step % PRINT_EVERY == 0:\n",
        "            metrics = eval_gamma_grid_rmse(n_t=EVAL_N_T, per_t=EVAL_PER_T)\n",
        "            lr_c = sched_crit.get_last_lr()[0]; lr_l = sched_lam.get_last_lr()[0]\n",
        "            lam_mean = lam_net_ema(x_t, t).mean().item()\n",
        "            lam_std  = lam_net_ema(x_t, t).std(unbiased=False).item()\n",
        "            print(f\"[{step:5d}/{STEPS}] lr(crit):{lr_c:.2e} lr(λ):{lr_l:.2e} | \"\n",
        "                  f\"loss(crit):{loss_crit.item():.3e}\"\n",
        "                  + (f\" loss(λ):{loss_lam.item():.3e}\" if step>WARMUP_LAM_STEPS else \" (λ warmup)\"))\n",
        "            print(f\"  λ(EMA) mean={lam_mean:.3f} std={lam_std:.3f}\")\n",
        "            print(\"  TRUE(γ): \"\n",
        "                  f\"Critic RMSE:{metrics['Critic_RMSE']:.3e}  \"\n",
        "                  f\"KNP:{metrics['KSSNP_TRUE']:.3e}  \"\n",
        "                  f\"TNP:{metrics['TWDNP_TRUE']:.3e}  \"\n",
        "                  f\"BNP[SNIS-λ]:{metrics['BLNDNP_TRUE_NP']:.3e}  \"\n",
        "                  f\"BNP[NN-λ]: {metrics['BLNDNP_TRUE_NN']:.3e}  |  \")\n",
        "            if SHOW_LAMBDA_STATS:\n",
        "                print(f\"  λ agreement (net vs SNIS): mean|Δ|={metrics['LAM_ABS_DIFF']:.3e}  \"\n",
        "                      f\"pearson={metrics['LAM_PEARSON']:.3f}\\n\")\n",
        "\n",
        "            # --- end-to-end probe through EXACT blackbox path (Heun-PC, 10 steps) ---\n",
        "            try:\n",
        "                probe_with_run_comparison(step, nsteps=8, trials = 1)\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] run_comparison probe failed: {e}\\n\")\n",
        "\n",
        "    # === Save EMA versions ===\n",
        "    def unwrap_module(m):\n",
        "        if hasattr(m, \"module\"):   m = m.module\n",
        "        if hasattr(m, \"_orig_mod\"): m = m._orig_mod\n",
        "        return m\n",
        "\n",
        "    torch.save(\n",
        "        {\n",
        "            \"lam_net\": unwrap_module(lam_net_ema).state_dict(),\n",
        "            \"critic\":  unwrap_module(critic_ema).state_dict(),\n",
        "            \"dim\":     D,\n",
        "            \"hidden\":  HIDDEN,\n",
        "            \"proxy_meta\": {\n",
        "                \"USE_PROXY_SCORES\": USE_PROXY_SCORES,                    # <--- NEW\n",
        "                \"PROXY_N\": PROXY_N,\n",
        "                \"PROXY_RIDGE_FRAC\": PROXY_RIDGE_FRAC,\n",
        "                \"PROXY_ALPHA\": PROXY_ALPHA,\n",
        "                \"PROXY_REF_CHUNK\": PROXY_REF_CHUNK,\n",
        "                \"PRECOMPUTE_TRAIN_SCORES\": PRECOMPUTE_TRAIN_SCORES,\n",
        "                \"PROXY_RECOMPUTE\": PROXY_RECOMPUTE,\n",
        "                \"PROXY_GMM_BATCH\": PROXY_GMM_BATCH,\n",
        "                \"PROXY_RECOMPUTE_MAX_ANCHORS\": PROXY_RECOMPUTE_MAX_ANCHORS,\n",
        "            }\n",
        "        },\n",
        "        SAVE_PATH,\n",
        "    )\n",
        "    print(f\"[INFO] EMA networks written to {SAVE_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Critic + Gate only (GPU/fast, stabilized)\n",
        "#  - log-uniform t, σ_t-scaled critic target\n",
        "#  - cosine LR, EMA, AMP, grad clipping\n",
        "#  - γ-grid eval (RMSE to true), λ agreement\n",
        "#  - NP targets only for metrics; no NN KSS/TWD heads\n",
        "#  - *** s0 is now a learned proxy via local Gaussian kernel (PyTorch) ***\n",
        "# ===========================\n",
        "\n",
        "import torch, cupy as cp, math, gc\n",
        "\n",
        "\n",
        "# -----------trrrreedee--------- Config & safe defaults --------------------\n",
        "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "USE_AMP  = (DEVICE == \"cuda\")\n",
        "DTORCH  = torch.float32\n",
        "\n",
        "# -------------------- VRAM hygiene --------------------\n",
        "def clear_gpu_mem():\n",
        "    try:\n",
        "        import cupy as cp\n",
        "        cp.get_default_memory_pool().free_all_blocks()\n",
        "        cp.get_default_pinned_memory_pool().free_all_blocks()\n",
        "    except Exception:\n",
        "        pass\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        for dev in range(torch.cuda.device_count()):\n",
        "            torch.cuda.set_device(dev)\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.ipc_collect()\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "clear_gpu_mem()\n",
        "\n",
        "import math, os, time\n",
        "import numpy as np         # <- use np everywhere\n",
        "import cupy as cp          # <- needed before build targets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "SAVE_PATH = \"learned_lr_score_critic_gate_ema.pt\"  # <-- renamed\n",
        "\n",
        "# Fallbacks if not defined upstream\n",
        "SEED        = globals().get(\"SEED\", 0)\n",
        "NUM_C       = globals().get(\"NUM_C\", 2000)\n",
        "K_DIM       = globals().get(\"K_DIM\", 12)\n",
        "VARIANT     = globals().get(\"VARIANT\", \"helix\")\n",
        "STD         = globals().get(\"STD\", 0.12)\n",
        "SCALE       = globals().get(\"SCALE\", 3.0)\n",
        "T_target    = globals().get(\"T_target\", 5e-4)\n",
        "T_end       = globals().get(\"T_end\", 1.5)\n",
        "N_Train     = globals().get(\"N_Train\", globals().get(\"N_TRAIN\", 500000))\n",
        "N_REF       = globals().get(\"N_REF\", 5000)\n",
        "HIDDEN      = globals().get(\"HIDDEN\", 512)\n",
        "HIDDEM_LAM  = globals().get(\"HIDDEM_LAM\", 128)\n",
        "EVAL_N_T    = globals().get(\"EVAL_N_T\", 24)\n",
        "EVAL_PER_T  = globals().get(\"EVAL_PER_T\", 2048)\n",
        "GAMMA_PRIOR = globals().get(\"GAMMA_PRIOR\", 1)\n",
        "EMBED_MODE = globals().get(\"EMBED_MODE\", 'sine_wiggle')\n",
        "\n",
        "# ---- NEW: proxy-score controls ----\n",
        "PROXY_N                 = int(globals().get(\"N_Train\", 500_000))\n",
        "PROXY_RIDGE_FRAC        = 1e-5 #4e-3 #float(globals().get(\"PROXY_RIDGE_FRAC\", 1e-2))\n",
        "PROXY_RIDGE_MODE       = globals().get(\"PROXY_RIDGE_MODE\", \"tail_mean\")  # or tail_mean/median\n",
        "PROXY_ALPHA             = float(globals().get(\"PROXY_ALPHA\", 0.4))\n",
        "PROXY_REF_CHUNK = 2048 #int(globals().get(\"PROXY_REF_CHUNK\", 2048))\n",
        "PRECOMPUTE_TRAIN_SCORES = True #bool(globals().get(\"PRECOMPUTE_TRAIN_SCORES\", True))\n",
        "PRECOMP_BATCH = 1012 #int(globals().get(\"PRECOMP_BATCH\", 2048))\n",
        "\n",
        "# ---- New cache/scaling knobs ----\n",
        "PROXY_RANK           = 3 #int(globals().get(\"PROXY_RANK\", 0))\n",
        "PROXY_V_DTYPE        = torch.float16  # settable to bfloat16 on A100/H100\n",
        "PROXY_TAU_MODE       = globals().get(\"PROXY_TAU_MODE\", \"tail_mean\")  # or tail_mean/median\n",
        "PROXY_TAU_TRIM_Q     = float(globals().get(\"PROXY_TAU_TRIM_Q\", 0.1))\n",
        "PROXY_TAU_SCALE      = float(globals().get(\"PROXY_TAU_SCALE\", 1.0))\n",
        "PROXY_TAU_FLOOR      = float(globals().get(\"PROXY_TAU_FLOOR\", 1e-6))\n",
        "PROXY_LAM_CLIP_MULT  = float(globals().get(\"PROXY_LAM_CLIP_MULT\", 1e3))\n",
        "\n",
        "# ---- LR+diag-tail + k-mix recompute toggles ----\n",
        "PROXY_DIAG_TAIL   = bool(globals().get(\"PROXY_DIAG_TAIL\", True))   # use diagonal tail τ⃗ (LR+D)\n",
        "PROXY_K_MIX   = 20 #int(globals().get(\"PROXY_K_MIX\", 64))              # mix over 64 KNN anchors / query\n",
        "PROXY_COMP_K      = globals().get(\"PROXY_COMP_K\", None)\n",
        "PROXY_RECOMPUTE_MAX_ANCHORS = int(globals().get(\"PROXY_RECOMPUTE_MAX_ANCHORS\", 10_000))\n",
        "\n",
        "USE_LOCAL_LRSVD_FOR_S0_BATCH = False\n",
        "# ---- NEW: toggle & limits for KDE/GMM recomputation mode ----\n",
        "PROXY_RECOMPUTE                 = True #bool(globals().get(\"PROXY_RECOMPUTE\", True))\n",
        "PROXY_GMM_BATCH                 = int(globals().get(\"PROXY_GMM_BATCH\", 1024))\n",
        "\n",
        "\n",
        "if DEVICE == \"cuda\":\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# ---------------- Base training knobs ----------------\n",
        "# (keep your existing knob defaults)\n",
        "STEPS = 30000 #int(globals().get(\"STEPS\", 17000))\n",
        "\n",
        "############## critic gate specific ###################\n",
        "CRIT_STEPS_PER_LAM = 3 #int(globals().get(\"CRIT_STEPS_PER_LAM\", 4))\n",
        "WARMUP_LAM_STEPS   = 500\n",
        "LAM_EPS            = float(globals().get(\"LAM_EPS\", 1e-4))\n",
        "LR_LAM_SCALE       = 0.666\n",
        "WD_LAM             = float(globals().get(\"WD_LAM\", 1e-4))\n",
        "STEPS += WARMUP_LAM_STEPS\n",
        "#######################################################\n",
        "\n",
        "\n",
        "BATCH   = int(globals().get(\"BATCH\", 512))\n",
        "LR_INIT, LR_MIN = 5e-3, 1e-5\n",
        "T_MIN, T_MAX = T_target, T_end\n",
        "PRINT_EVERY = 1000 #int(globals().get(\"PRINT_EVERY\", 200))\n",
        "EMA_DECAY = float(globals().get(\"EMA_DECAY\", 0.999))\n",
        "REF_CHUNK  = BATCH\n",
        "MAX_REF_CALL = 50_000\n",
        "REFRESH_DATA15 = bool(globals().get(\"REFRESH_DATA\", False))\n",
        "\n",
        "\n",
        "torch.manual_seed(SEED); np.random.seed(SEED)\n",
        "if DEVICE == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Human-readable labels\n",
        "NP_LAMBDA_LABEL = \"NPλ(SNIS)\"\n",
        "NN_LAMBDA_LABEL = \"NNλ(net)\"\n",
        "SHOW_LAMBDA_STATS = True\n",
        "\n",
        "# ===== Helper: γ-prior time grid =====\n",
        "@torch.no_grad()\n",
        "def make_gamma_times(n_t: int = EVAL_N_T, gamma: float = GAMMA_PRIOR,\n",
        "                     T_end: float = T_MAX, T_tgt: float = T_MIN, device: str = DEVICE):\n",
        "    u = (torch.arange(n_t, device=device, dtype=torch.float32) + 0.5) / n_t\n",
        "    return T_end * ((T_tgt / T_end) ** (u ** (1.0 / gamma)))\n",
        "\n",
        "def rmse_t(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.sqrt(torch.mean((a - b) ** 2))\n",
        "\n",
        "# ===== GMM helpers (you already have get_gmm_funcs) =====\n",
        "@torch.no_grad()\n",
        "def torch_sample_gmm(n: int, means: torch.Tensor, stds: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:\n",
        "    K, D = means.shape\n",
        "    idx = torch.multinomial(weights, num_samples=n, replacement=True)\n",
        "    noise = torch.randn(n, D, device=means.device) * stds[idx].unsqueeze(1)\n",
        "    return means[idx] + noise\n",
        "\n",
        "@torch.no_grad()\n",
        "def torch_score_gmm_batch(x: torch.Tensor, means: torch.Tensor, stds: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"(Kept for 'true score' checks; no longer used as the teacher s0.)\"\"\"\n",
        "    B, D = x.shape\n",
        "    K = means.shape[0]\n",
        "    inv_vars = 1.0 / (stds ** 2)\n",
        "    log_w    = torch.log(weights + 1e-40)\n",
        "    norm_const = -0.5 * D * (math.log(2 * math.pi) + torch.log(stds**2))\n",
        "    diff = means.unsqueeze(0) - x.unsqueeze(1)                      # [B,K,D]\n",
        "    d2   = (diff ** 2).sum(dim=-1)                                  # [B,K]\n",
        "    logp = log_w + norm_const - 0.5 * d2 * inv_vars.unsqueeze(0)    # [B,K]\n",
        "    w = torch.softmax(logp, dim=1)\n",
        "    return (w.unsqueeze(-1) * diff * inv_vars.view(1, K, 1)).sum(dim=1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def ou_params_at_t(means0: torch.Tensor, stds0: torch.Tensor, t: torch.Tensor):\n",
        "    et   = torch.exp(-t)\n",
        "    et2  = torch.exp(-2.0 * t)\n",
        "    means_t = et.view(-1,1,1) * means0.unsqueeze(0)       # [B,K,D]\n",
        "    var_t   = stds0.pow(2).unsqueeze(0) * et2 + (1.0 - et2)\n",
        "    stds_t  = torch.sqrt(var_t + 1e-12)\n",
        "    return means_t, stds_t\n",
        "\n",
        "@torch.no_grad()\n",
        "def true_score_xt_torch(x: torch.Tensor, t: torch.Tensor, means0: torch.Tensor, stds0: torch.Tensor, weights0: torch.Tensor) -> torch.Tensor:\n",
        "    B, D = x.shape\n",
        "    K, _ = means0.shape\n",
        "    means_t, stds_t = ou_params_at_t(means0, stds0, t)\n",
        "    inv_vars = 1.0 / (stds_t ** 2)\n",
        "    log_w    = torch.log(weights0 + 1e-40).view(1, K).expand(B, K)\n",
        "    norm_const = -0.5 * D * (math.log(2*math.pi) + torch.log(stds_t**2))\n",
        "    diff = means_t - x.unsqueeze(1)                               # [B,K,D]\n",
        "    d2   = (diff**2).sum(-1)\n",
        "    logp = log_w + norm_const - 0.5 * d2 * inv_vars\n",
        "    w    = torch.softmax(logp, dim=1)\n",
        "    return (w.unsqueeze(-1) * diff * inv_vars.unsqueeze(-1)).sum(1)\n",
        "\n",
        "# ======= NEW: Learned proxy for initial score s0 (PyTorch) =======\n",
        "\n",
        "@torch.no_grad()\n",
        "def _adaptive_k_default(n_ref: int, D: int, alpha: float = PROXY_ALPHA) -> int:\n",
        "    N0, k0 = 2000, max(4*D, 48)\n",
        "    k_default = int(min(n_ref-1, max(1, round(k0 * (n_ref / N0) ** alpha))))\n",
        "    return k_default\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _knn_topk_across_chunks(Q: torch.Tensor,\n",
        "                            R: torch.Tensor,\n",
        "                            k: int,\n",
        "                            ref_chunk: int = PROXY_REF_CHUNK):\n",
        "    \"\"\"\n",
        "    Maintain top-k smallest squared distances from each query in Q to rows in R\n",
        "    without materializing the full distance matrix. Returns (best_d2, best_idx).\n",
        "    \"\"\"\n",
        "    device = Q.device\n",
        "    Nq, D = Q.shape\n",
        "    Nr = R.shape[0]\n",
        "\n",
        "    # Precompute squared norms\n",
        "    q2 = (Q*Q).sum(dim=1, keepdim=True)           # [Nq,1]\n",
        "    r2_all = (R*R).sum(dim=1)                     # [Nr]\n",
        "\n",
        "    best_d2  = torch.full((Nq, k), float('inf'), device=device, dtype=Q.dtype)\n",
        "    best_idx = torch.full((Nq, k), -1, device=device, dtype=torch.long)\n",
        "\n",
        "    # Process reference in chunks\n",
        "    arng = torch.arange(Nq, device=device)\n",
        "    for start in range(0, Nr, ref_chunk):\n",
        "        end = min(start + ref_chunk, Nr)\n",
        "        Rc = R[start:end]                         # [Br,D]\n",
        "        r2 = r2_all[start:end]                    # [Br]\n",
        "        # d2 = ||q||^2 + ||r||^2 - 2 q r^T\n",
        "        d2 = q2 + r2.view(1,-1) - 2.0 * (Q @ Rc.T)   # [Nq,Br]\n",
        "        d2.clamp_(min=0)\n",
        "\n",
        "        # Merge with running top-k\n",
        "        d2_cat   = torch.cat([best_d2, d2], dim=1)   # [Nq, k+Br]\n",
        "        idx_new  = start + torch.arange(Rc.shape[0], device=device)\n",
        "        idx_cat  = torch.cat([best_idx, idx_new.view(1,-1).expand(Nq,-1)], dim=1)\n",
        "        # pick smallest k\n",
        "        d2_k, ord_k = torch.topk(d2_cat, k=k, dim=1, largest=False, sorted=False)\n",
        "        idx_k = torch.gather(idx_cat, dim=1, index=ord_k)\n",
        "        best_d2, best_idx = d2_k, idx_k\n",
        "\n",
        "    return best_d2, best_idx\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def kernel_proxy_score_torch(x_query: torch.Tensor,\n",
        "                             x_anchor: torch.Tensor,\n",
        "                             *,\n",
        "                             k: int | None = None,\n",
        "                             ridge_frac: float = PROXY_RIDGE_FRAC,\n",
        "                             ref_chunk: int = PROXY_REF_CHUNK,\n",
        "                             recompute: bool = False,\n",
        "                             gmm_batch: int = PROXY_GMM_BATCH,\n",
        "                             recompute_max_anchors: int = PROXY_RECOMPUTE_MAX_ANCHORS,\n",
        "                             return_params: bool = False\n",
        "                             ) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    assert x_query.dim() == 2 and x_anchor.dim() == 2, \"Inputs must be [N, D]\"\n",
        "    device = x_query.device\n",
        "    dtype  = x_query.dtype\n",
        "    Nq, D  = x_query.shape\n",
        "    Nr, Da = x_anchor.shape\n",
        "    assert Da == D, f\"Dim mismatch: x_anchor has D={Da}, x_query has D={D}\"\n",
        "\n",
        "    if k is None:\n",
        "        k = _adaptive_k_default(Nr, D)\n",
        "\n",
        "    # ---------- Fast path: standard local estimator ----------\n",
        "    if not recompute:\n",
        "        # kNN from query -> anchors\n",
        "        d2_knn, idx = _knn_topk_across_chunks(x_query, x_anchor, k=k, ref_chunk=ref_chunk)  # [Nq,k], [Nq,k]\n",
        "        # adaptive bandwidth from max d2 among the k-NN (per query)\n",
        "        tiny = torch.finfo(dtype).tiny\n",
        "        h2   = torch.maximum(d2_knn.max(dim=1).values, torch.tensor(tiny, device=device, dtype=dtype))  # [Nq]\n",
        "        w    = torch.exp(-d2_knn / (2.0 * h2[:, None]))                                                 # [Nq,k]\n",
        "        denom= w.sum(dim=1, keepdim=True).clamp_min(torch.finfo(dtype).eps)                             # [Nq,1]\n",
        "        X_nb = x_anchor[idx]                                                                            # [Nq,k,D]\n",
        "\n",
        "        mu   = (w.unsqueeze(-1) * X_nb).sum(dim=1) / denom                                              # [Nq,D]\n",
        "        diff = X_nb - mu.unsqueeze(1)                                                                    # [Nq,k,D]\n",
        "        var  = (w.unsqueeze(-1) * (diff * diff)).sum(dim=1) / denom                                      # [Nq,D]\n",
        "\n",
        "        # ridge proportional to mean variance per point\n",
        "        tau   = ridge_frac * var.mean(dim=1, keepdim=True)                                              # [Nq,1]\n",
        "        var_r = (var + tau).clamp_min(torch.finfo(dtype).eps)                                           # [Nq,D]\n",
        "\n",
        "        score = (mu - x_query) / var_r                                                                   # [Nq,D]\n",
        "        return (score, mu, var_r) if return_params else score\n",
        "\n",
        "    # ---------- Recompute path: build per-anchor components, then KDE/GMM score ----------\n",
        "    if return_params:\n",
        "        raise NotImplementedError(\"return_params=True is only supported for recompute=False (fast path).\")\n",
        "\n",
        "    # Guardrail for very large anchor pools\n",
        "    if Nr > recompute_max_anchors:\n",
        "        print(f\"[WARN] recompute=True with Nr={Nr} > recompute_max_anchors={recompute_max_anchors}. \"\n",
        "              f\"Falling back to fast local estimator.\")\n",
        "        return kernel_proxy_score_torch(\n",
        "            x_query, x_anchor, k=k, ridge_frac=ridge_frac, ref_chunk=ref_chunk,\n",
        "            recompute=False, gmm_batch=gmm_batch, recompute_max_anchors=recompute_max_anchors,\n",
        "            return_params=False\n",
        "        )\n",
        "\n",
        "    # Step 1: per-anchor local fits (exclude self)\n",
        "    k_self = min(k + 1, max(2, Nr))  # ensure at least 2 neighbors in the provisional set\n",
        "    d2_all, idx_all = _knn_topk_across_chunks(x_anchor, x_anchor, k=k_self, ref_chunk=ref_chunk)  # [Nr,k_self]\n",
        "\n",
        "    # Remove self from neighbor lists (if present), otherwise drop farthest\n",
        "    ar = torch.arange(Nr, device=device).view(-1, 1).expand(-1, k_self)  # [Nr,k_self]\n",
        "    is_self = (idx_all == ar)\n",
        "    has_self = is_self.any(dim=1)\n",
        "\n",
        "    keep_mask = torch.ones_like(idx_all, dtype=torch.bool)\n",
        "    if has_self.any():\n",
        "        self_pos = torch.argmax(is_self.int(), dim=1)  # position of 'self' per row\n",
        "        keep_mask[torch.arange(Nr, device=device), self_pos] = False\n",
        "\n",
        "    need_drop_farthest = ~has_self\n",
        "    if need_drop_farthest.any():\n",
        "        far_pos = torch.argmax(d2_all, dim=1)\n",
        "        keep_mask[torch.arange(Nr, device=device)[need_drop_farthest], far_pos[need_drop_farthest]] = False\n",
        "\n",
        "    idx_comp = idx_all[keep_mask].view(Nr, k_self - 1)   # [Nr, k]\n",
        "    d2_comp  = d2_all[keep_mask].view(Nr, k_self - 1)    # [Nr, k]\n",
        "\n",
        "    # Adaptive bandwidth per anchor\n",
        "    tiny = torch.finfo(dtype).tiny\n",
        "    h2_c = torch.maximum(d2_comp.max(dim=1).values, torch.tensor(tiny, device=device, dtype=dtype))  # [Nr]\n",
        "    w_c  = torch.exp(-d2_comp / (2.0 * h2_c[:, None]))                                              # [Nr,k]\n",
        "    denom_c = w_c.sum(dim=1, keepdim=True).clamp_min(torch.finfo(dtype).eps)                        # [Nr,1]\n",
        "    X_nb_c = x_anchor[idx_comp]                                                                     # [Nr,k,D]\n",
        "\n",
        "    # Component parameters (μ_j, diag Σ_j) with ridge\n",
        "    mu_c  = (w_c.unsqueeze(-1) * X_nb_c).sum(dim=1) / denom_c                                       # [Nr,D]\n",
        "    diffc = X_nb_c - mu_c.unsqueeze(1)                                                              # [Nr,k,D]\n",
        "    var_c = (w_c.unsqueeze(-1) * (diffc * diffc)).sum(dim=1) / denom_c                              # [Nr,D]\n",
        "    tau_c  = ridge_frac * var_c.mean(dim=1, keepdim=True)                                           # [Nr,1]\n",
        "    varr_c = (var_c + tau_c).clamp_min(torch.finfo(dtype).eps)                                      # [Nr,D]\n",
        "\n",
        "    # Step 2: evaluate mixture score at queries via two-pass log-sum-exp\n",
        "    two_pi = torch.tensor(2.0 * math.pi, device=device, dtype=dtype)\n",
        "    logdet_c = torch.sum(torch.log(two_pi * varr_c), dim=1)                                         # [Nr]\n",
        "\n",
        "    # Pass 1: get per-query max log weight\n",
        "    m = torch.full((Nq,), -float(\"inf\"), device=device, dtype=dtype)\n",
        "    for j0 in range(0, Nr, gmm_batch):\n",
        "        jb = slice(j0, min(j0 + gmm_batch, Nr))\n",
        "        mu_j  = mu_c[jb]          # [B,D]\n",
        "        var_j = varr_c[jb]        # [B,D]\n",
        "        logd  = logdet_c[jb]      # [B]\n",
        "        dX = x_query[:, None, :] - mu_j[None, :, :]                                               # [Nq,B,D]\n",
        "        quad = torch.sum(dX * dX / var_j[None, :, :], dim=2)                                      # [Nq,B]\n",
        "        logw = -0.5 * (quad + logd[None, :])                                                      # [Nq,B]\n",
        "        m = torch.maximum(m, torch.max(logw, dim=1).values)\n",
        "\n",
        "    # Pass 2: accumulate numerator and denominator in linear space\n",
        "    den = torch.zeros(Nq, device=device, dtype=dtype)\n",
        "    num = torch.zeros(Nq, D, device=device, dtype=dtype)\n",
        "    eps = torch.finfo(dtype).eps\n",
        "    for j0 in range(0, Nr, gmm_batch):\n",
        "        jb = slice(j0, min(j0 + gmm_batch, Nr))\n",
        "        mu_j  = mu_c[jb]          # [B,D]\n",
        "        var_j = varr_c[jb]        # [B,D]\n",
        "        logd  = logdet_c[jb]      # [B]\n",
        "        dX = x_query[:, None, :] - mu_j[None, :, :]                                               # [Nq,B,D]\n",
        "        quad = torch.sum(dX * dX / var_j[None, :, :], dim=2)                                      # [Nq,B]\n",
        "        logw = -0.5 * (quad + logd[None, :])                                                      # [Nq,B]\n",
        "        w_nb = torch.exp(logw - m[:, None])                                                       # [Nq,B]\n",
        "        comp_score = -dX / var_j[None, :, :]                                                      # [Nq,B,D]\n",
        "        num += torch.einsum('nb,nbd->nd', w_nb, comp_score)\n",
        "        den += torch.sum(w_nb, dim=1)\n",
        "\n",
        "    return num / (den[:, None] + eps)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def kernel_proxy_score_local_lrsvd(\n",
        "    x_query: torch.Tensor,\n",
        "    x_anchor: torch.Tensor,\n",
        "    *,\n",
        "    k: int | None = None,\n",
        "    rank: int | None = None,\n",
        "    ref_chunk: int = 2048,\n",
        "    # ridge / tail controls\n",
        "    ridge_mode: str = \"tail_mean\",\n",
        "    tail_trim_q: float = 0.1,\n",
        "    ridge_scale: float = 1.0,\n",
        "    ridge_floor: float = 1e-6,\n",
        "    lam_clip_mult: float = 1e3,\n",
        "    # k / bandwidth defaults\n",
        "    alpha: float = 0.4,\n",
        "    n0_for_k: int = 2000,\n",
        "    use_cpu_svd: bool = True,\n",
        "    enforce_dtype: torch.dtype | None = None,\n",
        "    # NEW:\n",
        "    diag_tail: bool = True,           # LR + diagonal tail (LR+D)\n",
        "    recompute: bool = False,          # enable k-mix recompute\n",
        "    k_mix: int | None = None,         # components per query in recompute (defaults to k)\n",
        "    comp_k: int | None = None,        # neighbors per anchor when building components (defaults to k)\n",
        "    ridge_frac: float = PROXY_RIDGE_FRAC\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Local LR-SVD proxy with diagonal tail option (diag_tail=True), and an optional\n",
        "    cheap k-mix recompute step (recompute=True) that mixes only over the query's\n",
        "    k_mix nearest anchors.  All heavy work is batched and stays on GPU.\n",
        "    \"\"\"\n",
        "    assert x_query.dim() == 2 and x_anchor.dim() == 2\n",
        "    if enforce_dtype is not None:\n",
        "        x_query  = x_query.to(enforce_dtype)\n",
        "        x_anchor = x_anchor.to(enforce_dtype)\n",
        "    device = x_query.device\n",
        "    dtype  = x_query.dtype\n",
        "    eps    = torch.finfo(dtype).eps\n",
        "    tiny   = torch.finfo(dtype).tiny\n",
        "\n",
        "    Nq, D = x_query.shape\n",
        "    Na, _ = x_anchor.shape\n",
        "    if rank is None:\n",
        "        rank = min(8, D)\n",
        "\n",
        "    # --- default k (as in your Script A) ---\n",
        "    if k is None:\n",
        "        k0 = max(4 * D, 48)\n",
        "        k_default = int(min(Na - 1, max(4 * D, round(k0 * ((Na / float(n0_for_k)) ** alpha)))))\n",
        "        k = max(1, k_default)\n",
        "    else:\n",
        "        k = int(max(1, min(k, Na - 1)))\n",
        "\n",
        "    if k_mix is None:\n",
        "        k_mix = k\n",
        "    if comp_k is None:\n",
        "        comp_k = k\n",
        "\n",
        "    # --- helper: KNN (in chunks) ---\n",
        "    def _knn(Q, R, k_val):\n",
        "        return _knn_topk_across_chunks(Q, R, k=k_val, ref_chunk=ref_chunk)\n",
        "\n",
        "    # ======================================================================\n",
        "    # (A) Fast path (no recompute): local LR+(diag tail) precision with Woodbury\n",
        "    # ======================================================================\n",
        "    if not recompute:\n",
        "        d2_knn, idx = _knn(x_query, x_anchor, k)                      # [Nq,k], [Nq,k]\n",
        "        h2  = torch.maximum(d2_knn.max(dim=1).values, torch.tensor(tiny, device=device, dtype=dtype))\n",
        "        w   = torch.exp(-d2_knn / (2.0 * h2[:, None]))                # [Nq,k]\n",
        "        denom = w.sum(dim=1, keepdim=True).clamp_min(eps)\n",
        "        X_nb  = x_anchor[idx]                                         # [Nq,k,D]\n",
        "        mu    = (w[..., None] * X_nb).sum(dim=1) / denom              # [Nq,D]\n",
        "        diff  = X_nb - mu[:, None, :]                                  # [Nq,k,D]\n",
        "\n",
        "        out = torch.empty_like(x_query)\n",
        "\n",
        "        # τ̂ from SVD tail; LR+D optional\n",
        "        def _tau_scalar_from_tail(S, r_eff):\n",
        "            m_total = S.shape[0]\n",
        "            if m_total <= r_eff: return float(ridge_floor)\n",
        "            lam_tail = S[r_eff:]**2\n",
        "            if ridge_mode == \"tail_trimmed\":\n",
        "                m = lam_tail.numel()\n",
        "                if m > 2:\n",
        "                    kdrop = int(max(1, min(m-1, round(tail_trim_q * m))))\n",
        "                    lam_tail = lam_tail.sort().values[:m-kdrop]\n",
        "            elif ridge_mode == \"tail_median\":\n",
        "                return max(ridge_floor, ridge_scale * float(lam_tail.median().item()))\n",
        "            return max(ridge_floor, ridge_scale * float(lam_tail.mean().item()))\n",
        "\n",
        "        for i in range(Nq):\n",
        "            wi = (w[i] / denom[i, 0]).to(dtype)                       # [k]\n",
        "            R  = (wi.sqrt().unsqueeze(1) * diff[i]).contiguous()      # [k,D]\n",
        "\n",
        "            R_svd = R.cpu() if use_cpu_svd and R.is_cuda else R\n",
        "            try:\n",
        "                U_, S, Vh = torch.linalg.svd(R_svd, full_matrices=False)\n",
        "                if use_cpu_svd and R.is_cuda:\n",
        "                    S  = S.to(device=device, dtype=dtype)\n",
        "                    Vh = Vh.to(device=device, dtype=dtype)\n",
        "                r_eff = int(min(rank, Vh.shape[0]))\n",
        "            except RuntimeError:\n",
        "                S, Vh, r_eff = None, None, 0\n",
        "\n",
        "            if (S is None) or (r_eff == 0):\n",
        "                # pure diagonal fallback\n",
        "                # diagonal variance estimate:\n",
        "                var = (wi[:, None] * (diff[i]**2)).sum(0)             # [D]\n",
        "                tau_s = ridge_frac * var.mean()\n",
        "                var_r = (var + tau_s).clamp_min(eps)\n",
        "                out[i] = (mu[i] - x_query[i]) / var_r\n",
        "                continue\n",
        "\n",
        "            V_r = Vh[:r_eff, :].T                                     # [D,r_eff]\n",
        "            lam = (S[:r_eff]**2)\n",
        "            # --- LR+D tail ---\n",
        "            if diag_tail:\n",
        "                # per-dim tail energy: τ_d ≈ sum_{j>r} λ_j (u_j ⊙ u_j)\n",
        "                if Vh.shape[0] > r_eff:\n",
        "                    V_tail = Vh[r_eff:, :].T                          # [D, D-r_eff]\n",
        "                    lam_tail = (S[r_eff:]**2)                         # [D-r_eff]\n",
        "                    if ridge_mode == \"tail_trimmed\":\n",
        "                        m = lam_tail.numel()\n",
        "                        if m > 2:\n",
        "                            kdrop = int(max(1, min(m-1, round(tail_trim_q * m))))\n",
        "                            lam_tail = lam_tail.sort().values[:m-kdrop]\n",
        "                            V_tail   = V_tail[:, :lam_tail.shape[0]]\n",
        "                    tau_vec = ridge_scale * torch.maximum(\n",
        "                        torch.full((D,), ridge_floor, device=device, dtype=dtype),\n",
        "                        (V_tail*V_tail @ lam_tail)\n",
        "                    )                                                  # [D]\n",
        "                else:\n",
        "                    tau_vec = torch.full((D,), ridge_floor, device=device, dtype=dtype)\n",
        "\n",
        "                lam = torch.minimum(lam, lam_clip_mult * tau_vec.max())\n",
        "                invD = 1.0 / (tau_vec + eps)                           # [D]\n",
        "                dX   = (x_query[i] - mu[i])                            # [D]\n",
        "                invDv = invD * dX\n",
        "                if r_eff > 0:\n",
        "                    A = torch.diag(1.0 / (lam + eps)) + V_r.T @ (invD[:, None] * V_r)\n",
        "                    z = torch.linalg.solve(A, V_r.T @ invDv)\n",
        "                    u = invDv - (invD[:, None] * V_r) @ z              # Σ^{-1}(x-μ)\n",
        "                else:\n",
        "                    u = invDv\n",
        "                out[i] = -u\n",
        "            else:\n",
        "                # scalar tau tail (old path)\n",
        "                tau_s = _tau_scalar_from_tail(S, r_eff)\n",
        "                lam   = torch.minimum(lam, lam_clip_mult * tau_s)\n",
        "                dX    = (x_query[i] - mu[i]).unsqueeze(0)              # [1,D]\n",
        "                T     = dX @ V_r                                       # [1,r]\n",
        "                scale = lam / (tau_s * (tau_s + lam) + eps)            # [r]\n",
        "                term  = (T * scale) @ V_r.T\n",
        "                u     = dX / (tau_s + eps) - term\n",
        "                out[i]= (-u).squeeze(0)\n",
        "\n",
        "        return out\n",
        "\n",
        "    # ======================================================================\n",
        "    # (B) k-mix recompute: mix only over each query's k_mix nearest anchors\n",
        "    #      (components are diagonal locals for cheapness)\n",
        "    # ======================================================================\n",
        "    # Step B.1: find per-query KNN anchors\n",
        "    d2_qk, idx_qk = _knn(x_query, x_anchor, k_mix)                     # [Nq,k_mix], [Nq,k_mix]\n",
        "    uniq_idx = torch.unique(idx_qk.reshape(-1))\n",
        "    Nu = uniq_idx.numel()\n",
        "\n",
        "    # Step B.2: build local diagonal components (μ_j, diag Σ_j) for the subset\n",
        "    #           using each anchor's own comp_k nearest neighbors (exclude self)\n",
        "    d2_all, idx_all = _knn(x_anchor[uniq_idx], x_anchor, comp_k + 1)   # [Nu,comp_k+1]\n",
        "    # mask out self and re-select top comp_k\n",
        "    self_mask = idx_all.eq(uniq_idx.view(-1,1))\n",
        "    d2_all = d2_all.masked_fill(self_mask, float('inf'))\n",
        "    d2_comp, ord2 = torch.topk(d2_all, k=comp_k, dim=1, largest=False)\n",
        "    idx_comp = torch.gather(idx_all, 1, ord2)                          # [Nu,comp_k]\n",
        "\n",
        "    tiny_t = torch.tensor(tiny, device=device, dtype=dtype)\n",
        "    h2_c   = torch.maximum(d2_comp.max(dim=1).values, tiny_t)          # [Nu]\n",
        "    w_c    = torch.exp(-d2_comp / (2.0 * h2_c[:, None]))               # [Nu,comp_k]\n",
        "    denom_c= w_c.sum(dim=1, keepdim=True).clamp_min(eps)               # [Nu,1]\n",
        "    X_nb_c = x_anchor[idx_comp]                                        # [Nu,comp_k,D]\n",
        "    mu_c   = (w_c[..., None] * X_nb_c).sum(1) / denom_c                # [Nu,D]\n",
        "    diff_c = X_nb_c - mu_c[:, None, :]                                  # [Nu,comp_k,D]\n",
        "    var_c  = (w_c[..., None] * (diff_c*diff_c)).sum(1) / denom_c       # [Nu,D]\n",
        "    tau_c  = ridge_frac * var_c.mean(dim=1, keepdim=True)              # [Nu,1]\n",
        "    varr_c = (var_c + tau_c).clamp_min(eps)                            # [Nu,D]\n",
        "    logdet_c = torch.sum(torch.log(2.0 * math.pi * varr_c), dim=1)     # [Nu]\n",
        "\n",
        "    # map global anchor id -> row in the subset arrays\n",
        "    pos_map = torch.full((Na,), -1, device=device, dtype=torch.long)\n",
        "    pos_map[uniq_idx] = torch.arange(Nu, device=device, dtype=torch.long)\n",
        "    pos = pos_map[idx_qk]                                              # [Nq,k_mix]\n",
        "\n",
        "    # gather per-query component params\n",
        "    mu_sel  = mu_c[pos]                                                # [Nq,k_mix,D]\n",
        "    var_sel = varr_c[pos]                                              # [Nq,k_mix,D]\n",
        "    logdsel = logdet_c[pos]                                            # [Nq,k_mix]\n",
        "\n",
        "    dX   = x_query[:, None, :] - mu_sel                                # [Nq,k_mix,D]\n",
        "    quad = torch.sum(dX * (dX / var_sel), dim=2)                       # [Nq,k_mix]\n",
        "    logw = -0.5 * (quad + logdsel)                                     # [Nq,k_mix]\n",
        "\n",
        "    m = torch.max(logw, dim=1, keepdim=True).values                    # [Nq,1]\n",
        "    W = torch.exp(logw - m)                                            # [Nq,k_mix]\n",
        "    comp_score = -(dX / var_sel)                                       # [Nq,k_mix,D]\n",
        "    num = torch.einsum('nk,nkd->nd', W, comp_score)                    # [Nq,D]\n",
        "    den = W.sum(dim=1).clamp_min(eps)                                  # [Nq]\n",
        "    return num / den[:, None]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def precompute_proxy_scores(dataset_cpu: torch.Tensor,\n",
        "                            anchor_pool: torch.Tensor,\n",
        "                            *,\n",
        "                            batch: int = PRECOMP_BATCH,\n",
        "                            k: int | None = None,\n",
        "                            ref_chunk: int = PROXY_REF_CHUNK,\n",
        "                            recompute: bool = PROXY_RECOMPUTE,\n",
        "                            gmm_batch: int = PROXY_GMM_BATCH,   # unused now (k-mix)\n",
        "                            recompute_max_anchors: int = PROXY_RECOMPUTE_MAX_ANCHORS\n",
        "                            ) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Precompute proxy scores for the entire dataset and return them on CPU.\n",
        "    If recompute=True, uses the k-mix path (mix over query's KNN anchors only).\n",
        "    \"\"\"\n",
        "    N, D = dataset_cpu.shape\n",
        "    out = torch.empty(N, D, dtype=torch.float32)\n",
        "\n",
        "    for start in range(0, N, batch):\n",
        "        end = min(start + batch, N)\n",
        "        q = dataset_cpu[start:end].to(DEVICE, non_blocking=True)\n",
        "\n",
        "        s = kernel_proxy_score_local_lrsvd(\n",
        "            q, anchor_pool,\n",
        "            k=k,\n",
        "            rank=min(PROXY_RANK, q.shape[1]),\n",
        "            ref_chunk=ref_chunk,\n",
        "            ridge_mode=PROXY_RIDGE_MODE,\n",
        "            tail_trim_q=PROXY_TAU_TRIM_Q,\n",
        "            ridge_scale=PROXY_TAU_SCALE,\n",
        "            ridge_floor=PROXY_TAU_FLOOR,\n",
        "            lam_clip_mult=PROXY_LAM_CLIP_MULT,\n",
        "            alpha=PROXY_ALPHA,\n",
        "            use_cpu_svd=True,\n",
        "            enforce_dtype=torch.float32,\n",
        "            diag_tail=PROXY_DIAG_TAIL,\n",
        "            recompute=recompute,\n",
        "            k_mix=PROXY_K_MIX,\n",
        "            comp_k=(PROXY_COMP_K if PROXY_COMP_K is not None else None),\n",
        "            ridge_frac=PROXY_RIDGE_FRAC\n",
        "        )\n",
        "        out[start:end] = s.detach().to('cpu', non_blocking=True)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------- Nets ----------------------\n",
        "class LambdaNet(nn.Module):\n",
        "    \"\"\"Small MLP → scalar λ∈(0,1).\"\"\"\n",
        "    def __init__(self, dim: int, hidden: int = 128):\n",
        "        super().__init__()\n",
        "        self.f = nn.Sequential(\n",
        "            nn.Linear(dim + 1, hidden), nn.SiLU(),\n",
        "            nn.Linear(hidden, hidden),  nn.SiLU(),\n",
        "            nn.Linear(hidden, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
        "        return self.f(torch.cat([x, torch.log(t)], dim=1)).squeeze(-1)\n",
        "\n",
        "class CriticNet(nn.Module):\n",
        "    \"\"\"Outputs σ_t-scaled score estimate (we unscale at eval).\"\"\"\n",
        "    def __init__(self, dim: int, hidden: int = 512):\n",
        "        super().__init__()\n",
        "        self.f = nn.Sequential(\n",
        "            nn.Linear(dim + 1, hidden), nn.SiLU(),\n",
        "            nn.Linear(hidden, hidden),  nn.SiLU(),\n",
        "            nn.Linear(hidden, dim),\n",
        "        )\n",
        "    def forward(self, y: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
        "        return self.f(torch.cat([y, torch.log(t)], dim=1))\n",
        "\n",
        "def update_ema(target: nn.Module, src: nn.Module, decay: float):\n",
        "    with torch.no_grad():\n",
        "        for p_t, p_s in zip(target.parameters(), src.parameters()):\n",
        "            p_t.data.mul_(decay).add_(p_s.data, alpha=(1.0 - decay))\n",
        "\n",
        "# ----------------- Build targets -----------------\n",
        "params, sampler_func, score_func = get_gmm_funcs(NUM_C, k_dim=K_DIM, variant=VARIANT, comp_std=STD,\n",
        "                                                 overall_scale=SCALE, seed=0, embedding_mode = EMBED_MODE)[:3]\n",
        "means0_cp, stds0_cp, w0_cp = map(cp.asarray, params)\n",
        "means0_np, stds0_np, w0_np = params\n",
        "means0 = torch.as_tensor(means0_np, device=DEVICE).float().contiguous()\n",
        "stds0  = torch.as_tensor(stds0_np,  device=DEVICE).float().contiguous()\n",
        "w0     = torch.as_tensor(w0_np,     device=DEVICE).float().contiguous(); w0 /= w0.sum()\n",
        "\n",
        "\n",
        "def true_score_func_cp(y_cp, t_scalar):\n",
        "    return calculate_true_score_at_t(\n",
        "        y_cp, t_scalar, means0_cp, stds0_cp, w0_cp, batch_size=4096\n",
        "    )\n",
        "\n",
        "\n",
        "# --- Build a large anchor pool (prior samples) for proxy s0 ---\n",
        "with torch.no_grad():\n",
        "    proxy_pool = torch_sample_gmm(PROXY_N, means0, stds0, w0)  # [PROXY_N, D]\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    x_ref = torch_sample_gmm(N_REF, means0, stds0, w0)\n",
        "    s0_ref = kernel_proxy_score_local_lrsvd(\n",
        "        x_ref, proxy_pool,\n",
        "        rank=min(PROXY_RANK, x_ref.shape[1]),\n",
        "        ridge_mode=PROXY_RIDGE_MODE,\n",
        "        tail_trim_q=PROXY_TAU_TRIM_Q,\n",
        "        ridge_scale=PROXY_TAU_SCALE,\n",
        "        ridge_floor=PROXY_TAU_FLOOR,\n",
        "        lam_clip_mult=PROXY_LAM_CLIP_MULT,\n",
        "        alpha=PROXY_ALPHA,\n",
        "        use_cpu_svd=True,\n",
        "        enforce_dtype=torch.float32,\n",
        "        diag_tail=PROXY_DIAG_TAIL,\n",
        "        recompute=PROXY_RECOMPUTE,           # <-- use k-mix recompute if enabled\n",
        "        k_mix=PROXY_K_MIX,\n",
        "        comp_k=(PROXY_COMP_K if PROXY_COMP_K is not None else None),\n",
        "        ridge_frac=PROXY_RIDGE_FRAC\n",
        "    )\n",
        "\n",
        "D = means0.shape[1]\n",
        "\n",
        "# ------- Gate (λ-net) -------\n",
        "lam_net     = LambdaNet(D, hidden=HIDDEM_LAM).to(DEVICE)\n",
        "lam_net_ema = LambdaNet(D, hidden=HIDDEM_LAM).to(DEVICE)\n",
        "lam_net_ema.load_state_dict(lam_net.state_dict())\n",
        "print(f\"[INFO] Gate init: random (D={D}).\")\n",
        "\n",
        "# ------- Critic -------\n",
        "critic      = CriticNet(D, hidden=HIDDEN).to(DEVICE)\n",
        "critic_ema  = CriticNet(D, hidden=HIDDEN).to(DEVICE)\n",
        "critic_ema.load_state_dict(critic.state_dict())\n",
        "\n",
        "# Unscaled EMA score from the critic.\n",
        "# During training the critic target was:\n",
        "#   q_scaled = sigma_t * ((1-λ) * a + λ * b)  ≈  sigma_t * s_theta(x_t, t)\n",
        "# so at inference we simply undo the scale:\n",
        "#   s_theta(x_t, t) ≈ q_scaled / sigma_t\n",
        "\n",
        "\n",
        "# optional compile\n",
        "try:\n",
        "    if DEVICE == \"cuda\":\n",
        "        lam_net     = torch.compile(lam_net)\n",
        "        critic      = torch.compile(critic)\n",
        "        lam_net_ema = torch.compile(lam_net_ema)\n",
        "        critic_ema  = torch.compile(critic_ema)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "opt_lam    = optim.Adam(lam_net.parameters(), lr=LR_INIT*LR_LAM_SCALE, weight_decay=WD_LAM)\n",
        "opt_crit   = optim.Adam(critic.parameters(),  lr=LR_INIT)\n",
        "sched_lam  = torch.optim.lr_scheduler.CosineAnnealingLR(opt_lam,  T_max=STEPS, eta_min=LR_MIN*LR_LAM_SCALE)\n",
        "sched_crit = torch.optim.lr_scheduler.CosineAnnealingLR(opt_crit, T_max=STEPS, eta_min=LR_MIN)\n",
        "scaler_crit = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n",
        "scaler_lam  = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n",
        "mse        = nn.MSELoss()\n",
        "\n",
        "def update_all_ema():\n",
        "    update_ema(lam_net_ema, lam_net, EMA_DECAY)\n",
        "    update_ema(critic_ema, critic, EMA_DECAY)\n",
        "\n",
        "# ----------------- Fixed training dataset (CPU pinned, deterministic) -----------------\n",
        "TRAIN_SEED  = int(globals().get(\"TRAIN_SEED\", globals().get(\"SEED\", 0)))\n",
        "_gen_cpu    = torch.Generator(device='cpu').manual_seed(TRAIN_SEED)\n",
        "\n",
        "def _sample_gmm_cpu_with_gen(n, means_cpu, stds_cpu, w_cpu, gen):\n",
        "    K, D = means_cpu.shape\n",
        "    idx   = torch.multinomial(w_cpu, num_samples=n, replacement=True, generator=gen)\n",
        "    noise = torch.randn(n, D, generator=gen, dtype=means_cpu.dtype)\n",
        "    return means_cpu[idx] + noise * stds_cpu[idx].unsqueeze(1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _means_cpu = means0.detach().cpu().float()\n",
        "    _stds_cpu  = stds0.detach().cpu().float()\n",
        "    _w_cpu     = (w0 / w0.sum()).detach().cpu().float()\n",
        "\n",
        "    x0_train_cpu  = _sample_gmm_cpu_with_gen(N_Train, _means_cpu, _stds_cpu, _w_cpu, _gen_cpu).pin_memory()\n",
        "    u_train_cpu   = torch.rand(N_Train, 1, generator=_gen_cpu, dtype=torch.float32).pin_memory()\n",
        "    eps_train_cpu = torch.randn(N_Train, _means_cpu.shape[1], generator=_gen_cpu, dtype=torch.float32).pin_memory()\n",
        "\n",
        "# --- NEW: Optional precompute s0 proxies for training pool (aligned with indices) ---\n",
        "S0_TRAIN_CPU = None\n",
        "if PRECOMPUTE_TRAIN_SCORES:\n",
        "    print(f\"[INFO] Precomputing learned s0 proxies for training pool of size {N_Train}...\"\n",
        "          + (\" (recompute/KDE mode)\" if PROXY_RECOMPUTE else \"\"))\n",
        "    S0_TRAIN_CPU = precompute_proxy_scores(x0_train_cpu, proxy_pool, batch=PRECOMP_BATCH).pin_memory()\n",
        "    print(\"[INFO] Done precomputing s0 proxies.\")\n",
        "\n",
        "_train_ptr  = 0\n",
        "_train_perm = torch.randperm(N_Train, generator=_gen_cpu)\n",
        "\n",
        "def _next_batch_indices(B):\n",
        "    global _train_ptr, _train_perm\n",
        "    if _train_ptr + B <= N_Train:\n",
        "        idx = _train_perm[_train_ptr:_train_ptr+B]\n",
        "        _train_ptr += B\n",
        "        return idx\n",
        "    tail = _train_perm[_train_ptr:]\n",
        "    _train_perm = torch.randperm(N_Train, generator=_gen_cpu)\n",
        "    _train_ptr  = 0\n",
        "    need = B - tail.numel()\n",
        "    head = _train_perm[_train_ptr:_train_ptr+need]\n",
        "    _train_ptr += need\n",
        "    return torch.cat([tail, head], dim=0)\n",
        "\n",
        "# ----------------- NP teachers using proxy s0_ref -----------------\n",
        "@torch.no_grad()\n",
        "def kss_score_torch(y: torch.Tensor, t: torch.Tensor, x_ref: torch.Tensor, s0_ref: torch.Tensor) -> torch.Tensor:\n",
        "    B, D = y.shape\n",
        "    N    = x_ref.shape[0]\n",
        "    et   = torch.exp(-t); et2 = torch.exp(-2.0 * t)\n",
        "    var  = (1.0 - et2).clamp_min(1e-12); invv = 1.0 / var\n",
        "    mu   = et.view(B,1,1) * x_ref.view(1,N,D)\n",
        "    diff = y.view(B,1,D) - mu\n",
        "    d2   = (diff**2).sum(-1)\n",
        "    logw = -0.5 * invv.view(B,1) * d2\n",
        "    w    = torch.softmax(logw, dim=1)\n",
        "    return torch.exp(t).view(B,1) * (w @ s0_ref)\n",
        "\n",
        "@torch.no_grad()\n",
        "def tweedie_score_torch(y: torch.Tensor, t: torch.Tensor, x_ref: torch.Tensor) -> torch.Tensor:\n",
        "    B, D = y.shape\n",
        "    N    = x_ref.shape[0]\n",
        "    et   = torch.exp(-t); et2 = torch.exp(-2.0 * t)\n",
        "    var  = (1.0 - et2).clamp_min(1e-12); invv = 1.0 / var\n",
        "    mu   = et.view(B,1,1) * x_ref.view(1,N,D)\n",
        "    diff = y.view(B,1,D) - mu\n",
        "    d2   = (diff**2).sum(-1)\n",
        "    logw = -0.5 * invv.view(B,1) * d2\n",
        "    w    = torch.softmax(logw, dim=1)\n",
        "    mu_bar = (w.unsqueeze(-1) * mu).sum(dim=1)\n",
        "    return -invv.view(B,1) * (y - mu_bar)\n",
        "\n",
        "@torch.no_grad()\n",
        "def snis_lambda_torch(\n",
        "    y, t, x_ref, s0_ref,\n",
        "    s_kss=None, s_twd=None, eps: float = 1e-12,\n",
        "    max_ref: int | None = 10_000, chunk: int = 4096\n",
        "):\n",
        "    \"\"\"Streaming SNIS λ*: (Var[â]-Cov[â,b̂])/(Var[â]+Var[b̂]-2Cov[â,b̂]).\"\"\"\n",
        "    B, D = y.shape\n",
        "    N = x_ref.shape[0]\n",
        "    if (max_ref is not None) and (N > max_ref):\n",
        "        idx = torch.randperm(N, device=x_ref.device)[:max_ref]\n",
        "        x_ref  = x_ref[idx]\n",
        "        s0_ref = s0_ref[idx]\n",
        "        N = x_ref.shape[0]\n",
        "\n",
        "    et   = torch.exp(-t)                     # [B,1]\n",
        "    et2  = torch.exp(-2.0 * t)\n",
        "    var  = (1.0 - et2).clamp_min(1e-12)      # [B,1]\n",
        "    invv = 1.0 / var\n",
        "\n",
        "    # Pass 1\n",
        "    m = torch.full((B,), -float(\"inf\"), device=y.device, dtype=y.dtype)\n",
        "    s = torch.zeros(B, device=y.device, dtype=y.dtype)\n",
        "    acc_a = torch.zeros(B, D, device=y.device, dtype=y.dtype)\n",
        "    acc_b = torch.zeros(B, D, device=y.device, dtype=y.dtype)\n",
        "\n",
        "    for start in range(0, N, chunk):\n",
        "        end = min(start + chunk, N)\n",
        "        xr = x_ref[start:end]\n",
        "        s0 = s0_ref[start:end]\n",
        "        mu   = et.view(B,1,1) * xr.view(1,-1,D)\n",
        "        diff = y.view(B,1,D) - mu\n",
        "        d2   = (diff**2).sum(-1)\n",
        "        L    = -0.5 * invv.view(B,1) * d2\n",
        "        m_new = torch.maximum(m, L.max(dim=1).values)\n",
        "        scale = torch.exp(m - m_new)\n",
        "        wtil  = torch.exp(L - m_new[:,None])\n",
        "\n",
        "        a_i   = torch.exp(t).view(B,1,1) * s0.view(1,-1,D)\n",
        "        b_i   = -(invv.view(B,1,1)) * (y.view(B,1,D) - mu)\n",
        "        s     = s * scale + wtil.sum(1)\n",
        "        acc_a = acc_a * scale[:,None] + torch.einsum('bn,bnd->bd', wtil, a_i)\n",
        "        acc_b = acc_b * scale[:,None] + torch.einsum('bn,bnd->bd', wtil, b_i)\n",
        "        m = m_new\n",
        "\n",
        "    mu_a = acc_a / s[:,None]\n",
        "    mu_b = acc_b / s[:,None]\n",
        "\n",
        "    # Pass 2\n",
        "    S0    = torch.zeros(B, device=y.device, dtype=y.dtype)\n",
        "    Vknum = torch.zeros(B, device=y.device, dtype=y.dtype)\n",
        "    Vtnum = torch.zeros(B, device=y.device, dtype=y.dtype)\n",
        "    Cnum  = torch.zeros(B, device=y.device, dtype=y.dtype)\n",
        "    logZ  = torch.log(s) + m\n",
        "\n",
        "    for start in range(0, N, chunk):\n",
        "        end = min(start + chunk, N)\n",
        "        xr = x_ref[start:end]\n",
        "        s0 = s0_ref[start:end]\n",
        "        mu   = et.view(B,1,1) * xr.view(1,-1,D)\n",
        "        diff = y.view(B,1,D) - mu\n",
        "        d2   = (diff**2).sum(-1)\n",
        "        L    = -0.5 * invv.view(B,1) * d2\n",
        "        w    = torch.exp(L - logZ[:,None])\n",
        "        w2   = w * w\n",
        "        S0  += w2.sum(1)\n",
        "\n",
        "        a_i  = torch.exp(t).view(B,1,1) * s0.view(1,-1,D)\n",
        "        b_i  = -(invv.view(B,1,1)) * (y.view(B,1,D) - mu)\n",
        "        ac   = a_i - mu_a[:,None,:]\n",
        "        bc   = b_i - mu_b[:,None,:]\n",
        "\n",
        "        Vknum += (w2[:,:,None] * (ac*ac)).sum((1,2))\n",
        "        Vtnum += (w2[:,:,None] * (bc*bc)).sum((1,2))\n",
        "        Cnum  += (w2[:,:,None] * (ac*bc)).sum((1,2))\n",
        "\n",
        "    den = (1.0 - S0).clamp_min(1e-10)\n",
        "    Vk  = Vknum / den\n",
        "    Vt  = Vtnum / den\n",
        "    C   =  Cnum / den\n",
        "    lam = ((Vk - C) / (Vk + Vt - 2.0*C + eps)).clamp(0.0, 1.0)\n",
        "    return lam\n",
        "\n",
        "# ----------------- Batch maker (log-uniform t) -----------------\n",
        "_train_indices_last = None  # for exposing the last indices to caller\n",
        "@torch.no_grad()\n",
        "def make_batch(B, *, ref_chunk=None, max_ref=None, t_mode='log', return_idx=True):\n",
        "    global _train_indices_last\n",
        "    if ref_chunk is None:\n",
        "        ref_chunk = min(4096, int(globals().get(\"N_REF\", 4096)))\n",
        "    if max_ref is None:\n",
        "        max_ref = int(globals().get(\"N_REF\", 10_000))\n",
        "\n",
        "    # fixed pool slicing\n",
        "    idx = _next_batch_indices(B)\n",
        "    _train_indices_last = idx  # store\n",
        "    x0  = x0_train_cpu[idx].to(DEVICE, non_blocking=True)\n",
        "    u   = u_train_cpu[idx].to(DEVICE, non_blocking=True)\n",
        "    eps = eps_train_cpu[idx].to(DEVICE, non_blocking=True)\n",
        "\n",
        "    if t_mode == 'log':\n",
        "        logTmin = math.log(T_MIN); logTmax = math.log(T_MAX)\n",
        "        t = torch.exp(torch.tensor(logTmin, device=DEVICE) + u * (logTmax - logTmin))\n",
        "    elif t_mode == 'uniform':\n",
        "        t = torch.tensor(T_MIN, device=DEVICE) + u * (T_MAX - T_MIN)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown t_mode: {t_mode}\")\n",
        "\n",
        "    et    = torch.exp(-t)\n",
        "    sigma = torch.sqrt((1.0 - torch.exp(-2.0 * t)).clamp_min(1e-12))\n",
        "    x_t   = et * x0 + sigma * eps\n",
        "\n",
        "    # streamed NP teachers (for metrics and SNIS λ)\n",
        "    x_ref_src, s0_ref_src = x_ref, s0_ref\n",
        "    N_total = x_ref_src.shape[0]\n",
        "    if (max_ref is not None) and (N_total > max_ref):\n",
        "        sel = torch.randperm(N_total, generator=_gen_cpu, device='cpu')[:max_ref]\n",
        "        x_ref_src  = x_ref_src[sel]\n",
        "        s0_ref_src = s0_ref_src[sel]\n",
        "        N_total    = max_ref\n",
        "\n",
        "    Bsz, D = x_t.shape\n",
        "    y      = x_t\n",
        "    invv   = 1.0 / (1.0 - torch.exp(-2.0 * t)).clamp_min(1e-12)\n",
        "    y2     = (y*y).sum(-1)\n",
        "\n",
        "    m      = torch.full((Bsz,), -float(\"inf\"), device=DEVICE, dtype=y.dtype)\n",
        "    s      = torch.zeros(Bsz, device=DEVICE, dtype=y.dtype)\n",
        "    num_x  = torch.zeros(Bsz, D, device=DEVICE, dtype=y.dtype)\n",
        "    num_s0 = torch.zeros(Bsz, D, device=DEVICE, dtype=y.dtype)\n",
        "\n",
        "    for start in range(0, N_total, ref_chunk):\n",
        "        end = min(start + ref_chunk, N_total)\n",
        "        xr  = x_ref_src[start:end].to(DEVICE, non_blocking=True).contiguous()\n",
        "        s0  = s0_ref_src[start:end].to(DEVICE, non_blocking=True).contiguous()\n",
        "\n",
        "        xr2   = (xr*xr).sum(-1)\n",
        "        cross = y @ xr.T\n",
        "        d2    = y2[:,None] + (et*et) * xr2[None,:] - 2.0 * et * cross\n",
        "        L     = -0.5 * invv * d2\n",
        "\n",
        "        m_new = torch.maximum(m, L.max(dim=1).values)\n",
        "        scale = torch.exp(m - m_new)\n",
        "        wtil  = torch.exp(L - m_new[:,None])\n",
        "\n",
        "        s      = s * scale + wtil.sum(1)\n",
        "        num_x  = num_x  * scale[:,None] + torch.einsum('bn,nd->bd', wtil, xr)\n",
        "        num_s0 = num_s0 * scale[:,None] + torch.einsum('bn,nd->bd', wtil, s0)\n",
        "        m      = m_new\n",
        "\n",
        "    x_bar  = num_x  / (s[:,None] + 1e-30)\n",
        "    s0_bar = num_s0 / (s[:,None] + 1e-30)\n",
        "\n",
        "    s_kss  = torch.exp(t) * s0_bar\n",
        "    s_twd  = -invv * (y - torch.exp(-t) * x_bar)\n",
        "    s_true = true_score_xt_torch(x_t, t, means0, stds0, w0)\n",
        "\n",
        "    if return_idx:\n",
        "        return x_t, t, sigma, s_kss, s_twd, s_true, x0, idx\n",
        "    else:\n",
        "        return x_t, t, sigma, s_kss, s_twd, s_true, x0\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_evolved_gmm(n: int, t_scalar: float,\n",
        "                       means0: torch.Tensor, stds0: torch.Tensor, w0: torch.Tensor) -> torch.Tensor:\n",
        "    # Draw x_t directly from the OU-evolved mixture (means_t, stds_t).\n",
        "    t  = torch.tensor(t_scalar, device=means0.device, dtype=means0.dtype)\n",
        "    et = torch.exp(-t); et2 = torch.exp(-2.0 * t)\n",
        "    means_t = means0 * et                               # [K,D]\n",
        "    var_t   = stds0**2 * et2 + (1.0 - et2)             # [K,D]\n",
        "    stds_t  = torch.sqrt(var_t.clamp_min(1e-12))       # [K,D]\n",
        "    return torch_sample_gmm(n, means_t, stds_t, w0)\n",
        "\n",
        "# ----------------- Eval (γ-grid) -----------------\n",
        "@torch.no_grad()\n",
        "def eval_gamma_grid_rmse(n_t: int = EVAL_N_T, per_t: int = EVAL_PER_T):\n",
        "    t_grid = make_gamma_times(n_t=n_t)\n",
        "    sums = {k: 0.0 for k in [\n",
        "        \"KSSNP_TRUE\",\"TWDNP_TRUE\",\"BLNDNP_TRUE_NP\",\"BLNDNP_TRUE_NN\",\n",
        "        \"LAM_ABS_DIFF\",\"LAM_PEARSON\",\"Critic_RMSE\"\n",
        "    ]}\n",
        "    cnt_total = 0\n",
        "    sum_np = 0.0; sum_nn = 0.0\n",
        "    sum_np2 = 0.0; sum_nn2 = 0.0; sum_prod = 0.0\n",
        "\n",
        "    for t in t_grid.tolist():\n",
        "        x_eval = sample_evolved_gmm(per_t, t, means0, stds0, w0)\n",
        "        tcol   = torch.full((per_t,1), t, device=DEVICE, dtype=x_eval.dtype)\n",
        "        sigma  = torch.sqrt((1.0 - torch.exp(-2.0*tcol)).clamp_min(1e-12))\n",
        "\n",
        "        s_true = true_score_xt_torch(x_eval, tcol, means0, stds0, w0)\n",
        "        s_kss  = kss_score_torch(x_eval, tcol, x_ref, s0_ref)       # uses proxy s0_ref\n",
        "        s_twd  = tweedie_score_torch(x_eval, tcol, x_ref)\n",
        "\n",
        "        pred_crit = critic_ema(x_eval, tcol) / sigma\n",
        "\n",
        "        lam_np = snis_lambda_torch(x_eval, tcol, x_ref, s0_ref, s_kss, s_twd).view(-1,1)\n",
        "        lam_nn = lam_net_ema(x_eval, tcol).view(-1,1)\n",
        "\n",
        "        s_bl_np_np = (1 - lam_np)*s_kss + lam_np*s_twd\n",
        "        s_bl_np_nn = (1 - lam_nn)*s_kss + lam_nn*s_twd\n",
        "\n",
        "        sums[\"KSSNP_TRUE\"]     += rmse_t(s_kss,   s_true).item()\n",
        "        sums[\"TWDNP_TRUE\"]     += rmse_t(s_twd,   s_true).item()\n",
        "        sums[\"BLNDNP_TRUE_NP\"] += rmse_t(s_bl_np_np, s_true).item()\n",
        "        sums[\"BLNDNP_TRUE_NN\"] += rmse_t(s_bl_np_nn, s_true).item()\n",
        "        sums[\"Critic_RMSE\"]    += rmse_t(pred_crit, s_true).item()\n",
        "\n",
        "        d = (lam_nn - lam_np).abs().mean().item()\n",
        "        sums[\"LAM_ABS_DIFF\"]   += d\n",
        "\n",
        "        ln = lam_nn.view(-1); lp = lam_np.view(-1)\n",
        "        cnt_total += ln.numel()\n",
        "        sum_nn  += ln.sum().item(); sum_np  += lp.sum().item()\n",
        "        sum_nn2 += (ln*ln).sum().item(); sum_np2 += (lp*lp).sum().item()\n",
        "        sum_prod+= (ln*lp).sum().item()\n",
        "\n",
        "    for k in sums:\n",
        "        if k != \"LAM_PEARSON\":\n",
        "            sums[k] /= len(t_grid)\n",
        "\n",
        "    mean_nn = sum_nn / cnt_total; mean_np = sum_np / cnt_total\n",
        "    var_nn  = max(0.0, sum_nn2 / cnt_total - mean_nn**2)\n",
        "    var_np  = max(0.0, sum_np2 / cnt_total - mean_np**2)\n",
        "    cov     = (sum_prod / cnt_total) - mean_nn*mean_np\n",
        "    den = max(1e-12, math.sqrt(max(var_nn, 0.0) * max(var_np, 0.0)))\n",
        "    sums[\"LAM_PEARSON\"] = (cov / den) if den > 0 else 0.0\n",
        "    return sums\n",
        "\n",
        "\n",
        "# ===== add once, above your training loop =====\n",
        "import numpy as np, cupy as cp, torch\n",
        "@torch.no_grad()\n",
        "def ema_score_xt_torch(x_t: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
        "    if t.dim() == 1: t = t[:, None]\n",
        "    sigma_t  = torch.sqrt((1.0 - torch.exp(-2.0 * t)).clamp_min(1e-12))\n",
        "    q_scaled = critic_ema(x_t, t)\n",
        "    s = q_scaled / sigma_t\n",
        "    return torch.nan_to_num(s, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "def make_custom_score_from_torch(score_xt_torch, device):\n",
        "    def score_cp(y_cp, t_scalar):\n",
        "        y_np = cp.asnumpy(y_cp)\n",
        "        x_t  = torch.as_tensor(y_np, device=device, dtype=torch.float32)\n",
        "        t    = torch.full((x_t.shape[0], 1), float(t_scalar), device=device, dtype=x_t.dtype)\n",
        "        s    = score_xt_torch(x_t, t).detach().cpu().numpy().astype(np.float64)\n",
        "        return cp.asarray(s)\n",
        "    return score_cp\n",
        "\n",
        "# --- your EXACT blackbox sampler ---\n",
        "def blackbox_sampler(\n",
        "    N_part, time_pts, custom_score, sampler_func, *,\n",
        "    mode=\"heun_hop\", h_coeff=0.5, true_score_func=None,\n",
        "    p_prune = 0, likelyhood_func = None, loglik_grad_fn = None):\n",
        "    xp  = cp\n",
        "    dim = int(sampler_func(5).shape[1])           # match model/data dim\n",
        "    y   = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "    rmse_list = []                                # collect RMSE vs oracle\n",
        "\n",
        "    for k in range(len(time_pts)-1):\n",
        "        t_cur, t_prev = time_pts[k], time_pts[k+1]\n",
        "        dt    = t_cur - t_prev\n",
        "        noise = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "        S_cur  = custom_score(y, t_cur)           # [N_part, dim]\n",
        "        y_hat  = y + dt*(y + 2*S_cur) + xp.sqrt(2.0*dt)*noise\n",
        "        S_prev = custom_score(y_hat, t_prev)\n",
        "\n",
        "        if mode == \"heun_hop\":\n",
        "            A = xp.exp(dt); B = xp.exp(2*dt) - 1.0; C = xp.sqrt(B); D = B / A\n",
        "            y = A*y + h_coeff*B*S_prev + (1-h_coeff)*D*S_cur + C*noise\n",
        "        elif mode == \"ou_sde\":\n",
        "            y = y_hat\n",
        "        elif mode == \"pf_ode\":\n",
        "            y = y + dt*(y + S_cur)\n",
        "        elif mode == \"heun_pc\":\n",
        "            drift_avg = 0.5*(y + 2*S_cur) + 0.5*(y_hat + 2*S_prev)\n",
        "            y = y + dt*drift_avg + xp.sqrt(2.0 * dt) * noise\n",
        "        else:\n",
        "            raise ValueError(mode)\n",
        "\n",
        "    if true_score_func is not None:\n",
        "        S_cur_true  = true_score_func(y, t_cur)\n",
        "        S_prev_true = true_score_func(y_hat, t_prev)\n",
        "\n",
        "        S_cur_safe,  cur_idx_safe  = prune_cp_arr(S_cur,  p_percent = p_prune)\n",
        "        S_prev_safe, prev_idx_safe = prune_cp_arr(S_prev, p_percent = p_prune)\n",
        "\n",
        "        RMSE_cur  = float(xp_rmse(S_cur_safe,  S_cur_true[cur_idx_safe]))\n",
        "        RMSE_prev = float(xp_rmse(S_prev_safe, S_prev_true[prev_idx_safe]))\n",
        "\n",
        "        rmse_list.append(RMSE_cur)\n",
        "        rmse_list.append(RMSE_prev)\n",
        "        return y, rmse_list\n",
        "    else:\n",
        "        return y\n",
        "\n",
        "# build once and reuse\n",
        "_CUSTOM_SCORE = make_custom_score_from_torch(ema_score_xt_torch, DEVICE)\n",
        "\n",
        "def probe_with_run_comparison(epoch_or_step: int, N_ref = 2000, N_part = 2000,\n",
        "                              nsteps: int = 8 , trials = 3, p_prune = 50):\n",
        "    # force run_comparison to use THIS blackbox sampler\n",
        "    run_comparison.__globals__['blackbox_sampler'] = blackbox_sampler\n",
        "\n",
        "    samplers = [\n",
        "        ('heun_pc', 'custom', 'NN-Critic-EMA', _CUSTOM_SCORE),\n",
        "    ]\n",
        "    _ = run_comparison(\n",
        "        samplers           = samplers,\n",
        "        steps_list         = [16],\n",
        "        true_sampler_func  = sampler_func,\n",
        "        prior_sampler_func = sampler_func,\n",
        "        prior_score_func   = score_func,\n",
        "        true_score_func    = true_score_func_cp,\n",
        "        true_init_score    = score_func,\n",
        "        track_score_rmse   = True,\n",
        "        N_ref              = N_ref,\n",
        "        N_part             = N_part,\n",
        "        trials             = trials,\n",
        "        nrows              = 3,\n",
        "        mean_bins          = 80,\n",
        "        time_split         = 'power',\n",
        "        T_end              = T_end,\n",
        "        T_target           = T_target,\n",
        "        div                = 'M_KSD',\n",
        "        plot_hists         = True,\n",
        "        hist_mode          = 'pca',\n",
        "        display_mode       = 'min_label',\n",
        "        trial_name         = f'NN_hist_compare_e{epoch_or_step}',\n",
        "        p_prune            = p_prune,\n",
        "        ref_seed           = 1,\n",
        "        plot_res           = False,\n",
        "        hist_norm          = 2.0,\n",
        "        plot_prior         = True,\n",
        "        save_tag           = f\"run_data/SVDNN_epoch_{epoch_or_step}\",\n",
        "    )\n",
        "\n",
        "\n",
        "# ----------------- Training -------------------------\n",
        "SUBSTEPS = int(globals().get(\"SUBSTEPS\", 1))\n",
        "REF_CHUNK_TRAIN = min(4096, int(globals().get(\"N_REF\", 4096)))\n",
        "MAX_REF_TRAIN   = int(globals().get(\"N_REF\", 10_000))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _s0_local_runtime(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute the low-rank local proxy score that matches Script A (recompute=False):\n",
        "      s(x) = Σ^{-1}(μ - x) with Σ = τ I + V diag(λ) V^T,\n",
        "    using your anchors in `proxy_pool`.\n",
        "    \"\"\"\n",
        "    return kernel_proxy_score_local_lrsvd(\n",
        "        x, proxy_pool,\n",
        "        k=None,                                 # uses Script-A-matched adaptive-k inside\n",
        "        rank=PROXY_RANK,\n",
        "        ref_chunk=PROXY_REF_CHUNK,\n",
        "        ridge_mode=PROXY_RIDGE_MODE,\n",
        "        tail_trim_q=0.1,\n",
        "        ridge_scale=PROXY_TAU_SCALE,\n",
        "        ridge_floor=PROXY_TAU_FLOOR,\n",
        "        lam_clip_mult=PROXY_LAM_CLIP_MULT,\n",
        "        alpha=PROXY_ALPHA,\n",
        "        use_cpu_svd=True,\n",
        "        enforce_dtype=x.dtype\n",
        "    )\n",
        "\n",
        "\n",
        "def _s0_proxy_for_batch(x0_batch, idx_in_train):\n",
        "    if (S0_TRAIN_CPU is not None) and (idx_in_train is not None):\n",
        "        return S0_TRAIN_CPU[idx_in_train].to(DEVICE, non_blocking=True)\n",
        "\n",
        "    if USE_LOCAL_LRSVD_FOR_S0_BATCH:  # new flag\n",
        "        return kernel_proxy_score_local_lrsvd(x0_batch, proxy_pool, rank=min(PROXY_RANK, x0_batch.shape[1]), ridge_mode=\"tail_mean\",\n",
        "                                                 tail_trim_q=0.1,ridge_scale=1.0,ridge_floor=1e-6,lam_clip_mult=1e3,\n",
        "                                                alpha=0.4,use_cpu_svd=True,enforce_dtype=torch.float32)\n",
        "    return kernel_proxy_score_torch(x0_batch, proxy_pool, recompute=False,\n",
        "                                    ref_chunk=PROXY_REF_CHUNK, ridge_frac=PROXY_RIDGE_FRAC)\n",
        "\n",
        "\n",
        "for step in range(1, STEPS+1):\n",
        "    for _ in range(SUBSTEPS):\n",
        "        for _ in range(CRIT_STEPS_PER_LAM):\n",
        "              x_t, t, sigma_t, s_kss_np, s_twd_np, s_true, x0, idx_train = make_batch(\n",
        "                  BATCH, ref_chunk=REF_CHUNK_TRAIN, max_ref=MAX_REF_TRAIN, t_mode='log', return_idx=True\n",
        "              )\n",
        "              #######\n",
        "              with torch.no_grad():\n",
        "                  et   = torch.exp(-t)\n",
        "                  invv = 1.0 / (1.0 - torch.exp(-2.0 * t)).clamp_min(1e-12)\n",
        "\n",
        "                  # No resample: USE PRECOMPUTED if available (S0_TRAIN_CPU), else fallback\n",
        "                  s0_base    = _s0_proxy_for_batch(x0, idx_train)\n",
        "                  z_base_twd = x0\n",
        "\n",
        "                  a = torch.exp(t) * s0_base\n",
        "                  b = -invv * (x_t - et * z_base_twd)\n",
        "\n",
        "                  lam_frozen = lam_net_ema(x_t, t).unsqueeze(-1).clamp(LAM_EPS, 1.0 - LAM_EPS)\n",
        "                  z_scaled   = sigma_t * ((1 - lam_frozen) * a + lam_frozen * b)\n",
        "\n",
        "              opt_crit.zero_grad(set_to_none=True)\n",
        "              with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
        "                  q_scaled = critic(x_t, t)\n",
        "                  loss_crit = mse(q_scaled, z_scaled)\n",
        "\n",
        "              scaler_crit.scale(loss_crit).backward()\n",
        "              scaler_crit.unscale_(opt_crit)\n",
        "              torch.nn.utils.clip_grad_norm_(critic.parameters(), 1.0)\n",
        "              scaler_crit.step(opt_crit)\n",
        "              scaler_crit.update()\n",
        "              sched_crit.step()\n",
        "              update_ema(critic_ema, critic, EMA_DECAY)\n",
        "\n",
        "    # ---------------- Gate: 1 step against frozen critic from EMA ---------------\n",
        "    x_t, t, sigma_t, s_kss_np, s_twd_np, s_true, x0, idx_train = make_batch(\n",
        "        BATCH, ref_chunk=REF_CHUNK_TRAIN, max_ref=MAX_REF_TRAIN, t_mode='log', return_idx=True\n",
        "    )\n",
        "    #####\n",
        "    with torch.no_grad():\n",
        "        et   = torch.exp(-t)\n",
        "        invv = 1.0 / (1.0 - torch.exp(-2.0 * t)).clamp_min(1e-12)\n",
        "\n",
        "        s0_base  = _s0_proxy_for_batch(x0, idx_train)\n",
        "        z_base_twd = x0\n",
        "\n",
        "        a = torch.exp(t) * s0_base\n",
        "        b = -invv * (x_t - et * z_base_twd)\n",
        "        q_scaled_frozen = critic_ema(x_t, t)\n",
        "\n",
        "\n",
        "    opt_lam.zero_grad(set_to_none=True)\n",
        "    with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
        "        lam_pred = lam_net(x_t, t).unsqueeze(-1).clamp(LAM_EPS, 1.0 - LAM_EPS)\n",
        "        z_scaled = sigma_t * ((1 - lam_pred) * a + lam_pred * b)\n",
        "        loss_lam = mse(z_scaled, q_scaled_frozen)\n",
        "\n",
        "    if step > WARMUP_LAM_STEPS:\n",
        "        scaler_lam.scale(loss_lam).backward()\n",
        "        scaler_lam.unscale_(opt_lam)\n",
        "        torch.nn.utils.clip_grad_norm_(lam_net.parameters(), 1.0)\n",
        "        scaler_lam.step(opt_lam)\n",
        "        scaler_lam.update()\n",
        "        sched_lam.step()\n",
        "        update_ema(lam_net_ema, lam_net, EMA_DECAY)\n",
        "    else:\n",
        "        sched_lam.step()\n",
        "\n",
        "# ---------------- Logging every PRINT_EVERY steps ----------------\n",
        "    if step % PRINT_EVERY == 0:\n",
        "        metrics = eval_gamma_grid_rmse(n_t=EVAL_N_T, per_t=EVAL_PER_T)\n",
        "        lr_c = sched_crit.get_last_lr()[0]; lr_l = sched_lam.get_last_lr()[0]\n",
        "        lam_mean = lam_net_ema(x_t, t).mean().item()\n",
        "        lam_std  = lam_net_ema(x_t, t).std(unbiased=False).item()\n",
        "        print(f\"[{step:5d}/{STEPS}] lr(crit):{lr_c:.2e} lr(λ):{lr_l:.2e} | \"\n",
        "              f\"loss(crit):{loss_crit.item():.3e}\"\n",
        "              + (f\" loss(λ):{loss_lam.item():.3e}\" if step>WARMUP_LAM_STEPS else \" (λ warmup)\"))\n",
        "        print(f\"  λ(EMA) mean={lam_mean:.3f} std={lam_std:.3f}\")\n",
        "        print(\"  TRUE(γ): \"\n",
        "              f\"Critic RMSE:{metrics['Critic_RMSE']:.3e}  \"\n",
        "              f\"KNP:{metrics['KSSNP_TRUE']:.3e}  \"\n",
        "              f\"TNP:{metrics['TWDNP_TRUE']:.3e}  \"\n",
        "              f\"BNP[SNIS-λ]:{metrics['BLNDNP_TRUE_NP']:.3e}  \"\n",
        "              f\"BNP[NN-λ]: {metrics['BLNDNP_TRUE_NN']:.3e}  |  \")\n",
        "        if SHOW_LAMBDA_STATS:\n",
        "            print(f\"  λ agreement (net vs SNIS): mean|Δ|={metrics['LAM_ABS_DIFF']:.3e}  \"\n",
        "                  f\"pearson={metrics['LAM_PEARSON']:.3f}\\n\")\n",
        "\n",
        "        # --- end-to-end probe through EXACT blackbox path (Heun-PC, 10 steps) ---\n",
        "        try:\n",
        "            probe_with_run_comparison(step, nsteps=8, trials = 1)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] run_comparison probe failed: {e}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# === Save EMA versions ===\n",
        "def unwrap_module(m):\n",
        "    if hasattr(m, \"module\"):   m = m.module\n",
        "    if hasattr(m, \"_orig_mod\"): m = m._orig_mod\n",
        "    return m\n",
        "\n",
        "\n",
        "torch.save(\n",
        "    {\n",
        "        \"lam_net\": unwrap_module(lam_net_ema).state_dict(),\n",
        "        \"critic\":  unwrap_module(critic_ema).state_dict(),\n",
        "        \"dim\":     D,\n",
        "        \"hidden\":  HIDDEN,\n",
        "        \"proxy_meta\": {\n",
        "                      \"PROXY_N\": PROXY_N,\n",
        "                      \"PROXY_RIDGE_FRAC\": PROXY_RIDGE_FRAC,\n",
        "                      \"PROXY_ALPHA\": PROXY_ALPHA,\n",
        "                      \"PROXY_REF_CHUNK\": PROXY_REF_CHUNK,\n",
        "                      \"PRECOMPUTE_TRAIN_SCORES\": PRECOMPUTE_TRAIN_SCORES,\n",
        "                      \"PROXY_RECOMPUTE\": PROXY_RECOMPUTE,\n",
        "                      \"PROXY_GMM_BATCH\": PROXY_GMM_BATCH,\n",
        "                      \"PROXY_RECOMPUTE_MAX_ANCHORS\": PROXY_RECOMPUTE_MAX_ANCHORS,\n",
        "                      \"PROXY_RANK\": PROXY_RANK,\n",
        "                      \"PROXY_V_DTYPE\": str(PROXY_V_DTYPE),\n",
        "                      \"PROXY_TAU_MODE\": PROXY_TAU_MODE,\n",
        "                      \"PROXY_TAU_TRIM_Q\": PROXY_TAU_TRIM_Q,\n",
        "                      \"PROXY_TAU_SCALE\": PROXY_TAU_SCALE,\n",
        "                      \"PROXY_TAU_FLOOR\": PROXY_TAU_FLOOR,\n",
        "                      \"PROXY_LAM_CLIP_MULT\": PROXY_LAM_CLIP_MULT,\n",
        "}\n",
        "    },\n",
        "    SAVE_PATH,\n",
        ")\n",
        "print(f\"[INFO] EMA networks written to {SAVE_PATH}\")"
      ],
      "metadata": {
        "id": "UgjM6bo6e0LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Amortized vs Stochastic (Unamortized) vs Oracle-s0 Critic-Gate vs NN-DSM\n",
        "# Full comparison script\n",
        "# - Loads:\n",
        "#     * kss/twd/λ bundle:              \"kss_twd_lambda_ema.pt\"\n",
        "#     * critic (proxy s0):             \"learned_score_critic_gate_ema.pt\"\n",
        "#     * critic (amortized s0):         \"amort_learned_score_critic_gate_ema.pt\"\n",
        "#     * ORIGINAL critic-gate (oracle): \"critic_gate_ema.pt\"\n",
        "# - Assumes the following utilities are importable:\n",
        "#     get_gmm_funcs, OU_evolve_samples, calculate_true_score_at_t,\n",
        "#     tweedie_score_t, kss_score_t, snis_blend, snis_blend_oracle, run_comparison\n",
        "# ================================================================\n",
        "\n",
        "import os\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True,max_split_size_mb:256\")\n",
        "\n",
        "import torch, cupy as cp, math, gc\n",
        "from torch import nn\n",
        "\n",
        "DEVICE  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DTORCH  = torch.float32\n",
        "\n",
        "# -------------------- VRAM hygiene --------------------\n",
        "def clear_gpu_mem():\n",
        "    try:\n",
        "        import cupy as cp\n",
        "        cp.get_default_memory_pool().free_all_blocks()\n",
        "        cp.get_default_pinned_memory_pool().free_all_blocks()\n",
        "    except Exception:\n",
        "        pass\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        for dev in range(torch.cuda.device_count()):\n",
        "            torch.cuda.set_device(dev)\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.ipc_collect()\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "clear_gpu_mem()\n",
        "\n",
        "\n",
        "# -------------------- Model defs --------------------\n",
        "class ScoreNet(nn.Module):\n",
        "    def __init__(self, dim, hidden=512):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim+1, hidden), nn.SiLU(),\n",
        "            nn.Linear(hidden, hidden), nn.SiLU(),\n",
        "            nn.Linear(hidden, dim),\n",
        "        )\n",
        "    def forward(self, x, t):\n",
        "        return self.net(torch.cat([x, torch.log(t)], dim=1))\n",
        "\n",
        "class LambdaNet(nn.Module):\n",
        "    def __init__(self, dim, hidden=128):\n",
        "        super().__init__()\n",
        "        self.f = nn.Sequential(\n",
        "            nn.Linear(dim+1, hidden), nn.SiLU(),\n",
        "            nn.Linear(hidden, hidden), nn.SiLU(),\n",
        "            nn.Linear(hidden, 1), nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x, t):\n",
        "        return self.f(torch.cat([x, torch.log(t)], dim=1)).squeeze(-1)\n",
        "\n",
        "class CriticNet(nn.Module):\n",
        "    def __init__(self, dim, hidden=512):\n",
        "        super().__init__()\n",
        "        self.f = nn.Sequential(\n",
        "            nn.Linear(dim+1, hidden), nn.SiLU(),\n",
        "            nn.Linear(hidden, hidden), nn.SiLU(),\n",
        "            nn.Linear(hidden, dim),\n",
        "        )\n",
        "    def forward(self, y, t):\n",
        "        # predicts σ_t * s*(y,t)\n",
        "        return self.f(torch.cat([y, torch.log(t)], dim=1))\n",
        "\n",
        "# -------------------- Infer dims/hidden from state dict --------------------\n",
        "def _infer_hidden_from_state_dict(sd: dict, first_layer_prefix_options=(\"net.0.weight\", \"f.0.weight\")):\n",
        "    key = None\n",
        "    for k in first_layer_prefix_options:\n",
        "        if k in sd:\n",
        "            key = k\n",
        "            break\n",
        "    if key is None:\n",
        "        for k in sd.keys():\n",
        "            if k.endswith(\"0.weight\"):\n",
        "                key = k\n",
        "                break\n",
        "    if key is None:\n",
        "        raise KeyError(\"Could not infer first layer weight key ('.0.weight') in state dict.\")\n",
        "    w = sd[key]   # [hidden, dim_in]\n",
        "    hidden = w.shape[0]\n",
        "    dim_plus_one = w.shape[1]\n",
        "    dim = dim_plus_one - 1  # we append log(t)\n",
        "    return hidden, dim\n",
        "\n",
        "\n",
        "\n",
        "# -------------------- Load critics --------------------\n",
        "\n",
        "\n",
        "# 0) Scalar Critic Gate trained with exact s0\n",
        "#critic_ckpt = torch.load(\"critic_gate_ema.pt\", map_location=DEVICE)\n",
        "#crit_sd = critic_ckpt.get(\"critic_net\") or critic_ckpt.get(\"critic\")\n",
        "#if crit_sd is None:\n",
        "#    raise KeyError(\"Critic weights not found in learned_score_critic_gate_ema.pt.\")\n",
        "#H_crit, D_crit = _infer_hidden_from_state_dict(crit_sd)\n",
        "#critic_net = CriticNet(D_crit, hidden=H_crit).to(DEVICE).eval()\n",
        "#critic_net.load_state_dict(crit_sd)\n",
        "\n",
        "\n",
        "# 1) Scalar Critic Gate trained with learned proxy s0\n",
        "#critic_proxy_ckpt = torch.load(\"learned_score_critic_gate_ema.pt\", map_location=DEVICE)\n",
        "#crit_proxy_sd = critic_proxy_ckpt.get(\"critic_net\") or critic_proxy_ckpt.get(\"critic\")\n",
        "#if crit_proxy_sd is None:\n",
        "#    raise KeyError(\"Critic weights not found in learned_score_critic_gate_ema.pt.\")\n",
        "#H_crit_proxy, D_crit_proxy = _infer_hidden_from_state_dict(crit_proxy_sd)\n",
        "#critic_proxy_net = CriticNet(D_crit_proxy, hidden=H_crit_proxy).to(DEVICE).eval()\n",
        "#critic_proxy_net.load_state_dict(crit_proxy_sd)\n",
        "\n",
        "\n",
        "# 2) Tweedie-only DSM (NN-DSM)\n",
        "twd_ckpt = torch.load(\"tweedie_distill_ema.pt\", map_location=DEVICE)\n",
        "twd_sd   = twd_ckpt.get(\"net_twd\") or twd_ckpt.get(\"net\") or twd_ckpt\n",
        "H_twd, D_twd = _infer_hidden_from_state_dict(\n",
        "    twd_sd, first_layer_prefix_options=(\"net.0.weight\",)\n",
        ")\n",
        "net_twd = ScoreNet(D_twd, hidden=H_twd).to(DEVICE).eval()\n",
        "net_twd.load_state_dict(twd_sd)\n",
        "\n",
        "\n",
        "# 3) Scalar Critic Gate trained with low rank learned proxy s0\n",
        "#lr_critic_proxy_ckpt = torch.load(\"learned_lr_score_critic_gate_ema2.pt\", map_location=DEVICE)\n",
        "#lr_crit_proxy_sd = lr_critic_proxy_ckpt.get(\"critic_net\") or lr_critic_proxy_ckpt.get(\"critic\")\n",
        "#if lr_crit_proxy_sd is None:\n",
        "#    raise KeyError(\"Critic weights not found in learned_lr_score_critic_gate_ema.pt.\")\n",
        "#H_lr_crit_proxy, D_lr_crit_proxy = _infer_hidden_from_state_dict(lr_crit_proxy_sd)\n",
        "#lr_critic_proxy_net = CriticNet(D_lr_crit_proxy, hidden=H_lr_crit_proxy).to(DEVICE).eval()\n",
        "#lr_critic_proxy_net.load_state_dict(lr_crit_proxy_sd)\n",
        "\n",
        "# sanity: match dims with critic you already loaded\n",
        "#if \"D_crit_proxy\" in globals():\n",
        "#    assert D_twd == D_crit_proxy, f\"Dim mismatch: DSM D={D_twd}, critic D={D_crit_proxy}\"\n",
        "#    assert D_lr_crit_proxy == D_crit_proxy, f\"Dim mismatch: lr-critic D={D_lr_crit_proxy}, critic D={D_crit_proxy}\"\n",
        "# Use a single expected data dimension inferred from loaded nets\n",
        "#D_expected = D_twd  # (same as D_crit_proxy; asserted above)\n",
        "\n",
        "\n",
        "# -------------------- Load critics --------------------\n",
        "# 4) Scalar Critic Gate trained with recomputed learned proxy s0\n",
        "\n",
        "recomp_critic_proxy_ckpt = torch.load(\"learned_score_critic_gate_recomp_ema.pt\", map_location=DEVICE)\n",
        "recomp_crit_proxy_sd = recomp_critic_proxy_ckpt.get(\"critic_net\") or recomp_critic_proxy_ckpt.get(\"critic\")\n",
        "if recomp_crit_proxy_sd is None:\n",
        "    raise KeyError(\"Critic weights not found in learned_score_critic_gate_recomp_ema.pt.\")\n",
        "H_recomp_crit_proxy, D_recomp_crit_proxy = _infer_hidden_from_state_dict(recomp_crit_proxy_sd)\n",
        "recomp_critic_proxy_net = CriticNet(D_recomp_crit_proxy, hidden=H_recomp_crit_proxy).to(DEVICE).eval()\n",
        "recomp_critic_proxy_net.load_state_dict(recomp_crit_proxy_sd)\n",
        "\n",
        "\n",
        "# -------------------- Distribution (match training) --------------------\n",
        "NUM_C   = globals().get(\"NUM_C\",   2000)\n",
        "K_DIM   = globals().get(\"K_DIM\",   12)\n",
        "VARIANT = globals().get(\"VARIANT\", \"helix\")\n",
        "STD     = globals().get(\"STD\",     0.12)\n",
        "SCALE   = globals().get(\"SCALE\",   3.0)\n",
        "EMBED_MODE = globals().get(\"EMBED_MODE\", \"sine_wiggle\")\n",
        "\n",
        "T_target = globals().get(\"T_target\", 1e-3)\n",
        "T_end    = globals().get(\"T_end\",    1.5)\n",
        "\n",
        "\n",
        "SEED  = globals().get(\"SEED\", 0)\n",
        "N_ref_sampling = globals().get(\"N_ref_sampling\", 5000)\n",
        "ref_seed = SEED\n",
        "\n",
        "params, sampler_func, _ = get_gmm_funcs(\n",
        "    num_c=NUM_C, k_dim=K_DIM, variant=VARIANT,\n",
        "    comp_std=STD, overall_scale=SCALE, seed=SEED, embedding_mode = EMBED_MODE)[:3]\n",
        "means0_cp, stds0_cp, w0_cp = map(cp.asarray, params)\n",
        "\n",
        "# -------------------- CuPy/Torch bridges --------------------\n",
        "from torch.utils.dlpack import from_dlpack as torch_from_dlpack\n",
        "from torch.utils.dlpack import to_dlpack as torch_to_dlpack\n",
        "\n",
        "def cp_to_torch_float32(x_cp):\n",
        "    x32 = cp.ascontiguousarray(x_cp.astype(cp.float32))\n",
        "    return torch_from_dlpack(x32.toDlpack()).to(DEVICE)\n",
        "\n",
        "def torch_to_cp_float64(x_t):\n",
        "    x_t = x_t.detach().contiguous().to('cpu' if DEVICE=='cpu' else DEVICE)\n",
        "    x_cp = cp.fromDlpack(torch_to_dlpack(x_t))\n",
        "    return x_cp.astype(cp.float64)\n",
        "\n",
        "# -------------------- True score helper (CuPy) --------------------\n",
        "def true_score_func_cp(y_cp, t_scalar):\n",
        "    return calculate_true_score_at_t(\n",
        "        y_cp, t_scalar, means0_cp, stds0_cp, w0_cp, batch_size=4096\n",
        "    )\n",
        "\n",
        "# For final-time metric:\n",
        "score_func = lambda y_cp: true_score_func_cp(y_cp, 1e-6)\n",
        "evol_sampler_func = lambda n: sampler_func(n)\n",
        "\n",
        "# -------------------- Dim check --------------------\n",
        "D_expected = D\n",
        "def _ensure_2d_y(y_t: torch.Tensor, D_expected: int) -> torch.Tensor:\n",
        "    if y_t.ndim == 1:\n",
        "        raise ValueError(f\"Got 1D input of shape {tuple(y_t.shape)}; expected (B,{D_expected}).\")\n",
        "    if y_t.shape[1] != D_expected:\n",
        "        raise ValueError(f\"Dim mismatch: got (B,{y_t.shape[1]}), expected (B,{D_expected}).\")\n",
        "    return y_t\n",
        "\n",
        "# -------------------- NN score callables → CuPy (float64) --------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def nn_twd_score_cp(y_cp, t_scalar):\n",
        "    y_t = cp_to_torch_float32(y_cp); y_t = _ensure_2d_y(y_t, D_expected)\n",
        "    t_t = torch.full((y_t.shape[0], 1), float(t_scalar), device=DEVICE, dtype=DTORCH)\n",
        "    sigma = torch.sqrt((1.0 - torch.exp(-2.0*t_t)).clamp_min(1e-12))\n",
        "    s = net_twd(y_t, t_t) / sigma\n",
        "    return torch_to_cp_float64(s)\n",
        "\n",
        "@torch.no_grad()\n",
        "def lam_net_cp(y_cp, t_scalar):\n",
        "    y_t = cp_to_torch_float32(y_cp); y_t = _ensure_2d_y(y_t, D_expected)\n",
        "    t_t = torch.full((y_t.shape[0], 1), float(t_scalar), device=DEVICE, dtype=DTORCH)\n",
        "    lam = lam_net(y_t, t_t)\n",
        "    return torch_to_cp_float64(lam)\n",
        "\n",
        "def make_nn_blend_score(net_kss, net_twd, lam_net_ema=None, D_expected=None, device=\"cuda\"):\n",
        "    def score_nn(y_cp, t_scalar):\n",
        "        y_t = torch.as_tensor(cp.asnumpy(y_cp), device=device, dtype=torch.float32)\n",
        "        B, Ddim = y_t.shape\n",
        "        if D_expected is not None:\n",
        "            assert Ddim == D_expected, f\"dim mismatch: x has {Ddim}, model expects {D_expected}\"\n",
        "        t = torch.full((B,1), float(t_scalar), device=device, dtype=y_t.dtype)\n",
        "        sigma = torch.sqrt((1.0 - torch.exp(-2.0*t)).clamp_min(1e-12))\n",
        "        s_kss = net_kss(y_t, t) / sigma\n",
        "        s_twd = net_twd(y_t, t) / sigma\n",
        "        lam = (0.5 * torch.ones((B,1), device=device, dtype=y_t.dtype)\n",
        "               if lam_net_ema is None else lam_net_ema(y_t, t).unsqueeze(-1))\n",
        "        s = (1 - lam) * s_kss + lam * s_twd\n",
        "        return cp.asarray(s.detach().cpu().numpy(), dtype=cp.float64)\n",
        "    return score_nn\n",
        "\n",
        "def make_nn_critic_score_cp(critic_module: nn.Module, D_expected: int):\n",
        "    @torch.no_grad()\n",
        "    def f(y_cp, t_scalar):\n",
        "        y_t = cp_to_torch_float32(y_cp); y_t = _ensure_2d_y(y_t, D_expected)\n",
        "        t_t = torch.full((y_t.shape[0], 1), float(t_scalar), device=DEVICE, dtype=DTORCH)\n",
        "        sigma = torch.sqrt((1.0 - torch.exp(-2.0*t_t)).clamp_min(1e-12))\n",
        "        s = critic_module(y_t, t_t) / sigma\n",
        "        return torch_to_cp_float64(s)\n",
        "    return f\n",
        "\n",
        "@torch.no_grad()\n",
        "def nn_twd_score_cp(y_cp, t_scalar):\n",
        "    y_t = cp_to_torch_float32(y_cp); _ensure_2d_y(y_t, D_expected)\n",
        "    t_t = torch.full((y_t.shape[0], 1), float(t_scalar), device=DEVICE, dtype=DTORCH)\n",
        "    sigma = torch.sqrt((1.0 - torch.exp(-2.0 * t_t)).clamp_min(1e-12))\n",
        "    s = net_twd(y_t, t_t) / sigma   # DSM predicts sigma * s\n",
        "    return torch_to_cp_float64(s)\n",
        "\n",
        "# ---- critic callables ----\n",
        "#nn_critic_score_cp = make_nn_blend_score(net_kss=critic_net, net_twd=net_twd)\n",
        "#nn_critic_proxy_score_cp = make_nn_critic_score_cp(critic_proxy_net, D_crit_proxy)\n",
        "#nn_critic_lr_proxy_score_cp = make_nn_critic_score_cp(lr_critic_proxy_net, D_lr_crit_proxy)\n",
        "nn_critic_recomp_score_cp = make_nn_critic_score_cp(recomp_critic_proxy_net, D_recomp_crit_proxy)\n",
        "\n",
        "\n",
        "# === NP reference at final time ===\n",
        "x0_ref = sampler_func(N_ref_sampling, seed=(ref_seed if ref_seed!='rand' else None))\n",
        "s0_ref = score_func(x0_ref)\n",
        "\n",
        "# -------------------- Build estimator dictionary --------------------\n",
        "def build_estimators(x0_ref, s0_ref, true_score_func_cp):\n",
        "    def np_twd(y_cp, t): return tweedie_score_t(y_cp, t, x0_ref)\n",
        "    def np_kss(y_cp, t): return kss_score_t(y_cp, t, x0_ref, s0_ref)\n",
        "    def np_blend(y_cp, t):\n",
        "        lam = snis_blend(y_cp, x0_ref, s0_ref, t)\n",
        "        sk, st = np_kss(y_cp, t), np_twd(y_cp, t)\n",
        "        return (1.0 - lam)[:,None]*sk + lam[:,None]*st\n",
        "    return {\n",
        "      \"NP-Tweedie\"               : np_twd,\n",
        "      \"NP-KSS\"                   : np_kss,\n",
        "      \"NP-Blend(SNIS)\"           : np_blend,\n",
        "      \"True\"                     : true_score_func_cp,\n",
        "      #\"NN-Critic\"                 : nn_critic_score_cp,\n",
        "      #\"NN-Critic-proxy-score\"    : nn_critic_proxy_score_cp,\n",
        "      #\"NN-Critic-lr-proxy-score\" : nn_critic_lr_proxy_score_cp,\n",
        "      \"NN-Critic-recomp-score\"   : nn_critic_recomp_score_cp,\n",
        "      \"NN-Tweedie\"               : nn_twd_score_cp,   # <-- add this\n",
        "}\n",
        "estimators = build_estimators(x0_ref, s0_ref, true_score_func_cp)\n",
        "\n",
        "# -------------------- Main experiment --------------------\n",
        "def main():\n",
        "    sampler = 'heun_pc'\n",
        "    samplers = [\n",
        "        #(sampler, 'custom', 'NN-Critic-proxy-score',    estimators['NN-Critic-proxy-score']),\n",
        "        #(sampler, 'custom', 'NN-Critic',         estimators['NN-Critic']),\n",
        "        (sampler, 'custom', \"Critic Gate\", estimators[\"NN-Critic-recomp-score\"]),\n",
        "        (sampler, 'custom', 'DSM',               estimators['NN-Tweedie']),\n",
        "        #(sampler, 'custom', 'NN-Critic-lr-proxy-score', estimators['NN-Critic-lr-proxy-score'])\n",
        "        ]\n",
        "\n",
        "    results = run_comparison(\n",
        "        samplers        = samplers,\n",
        "        steps_list      = [10],\n",
        "        true_sampler_func = sampler_func,\n",
        "        prior_sampler_func    = sampler_func,\n",
        "        prior_score_func      = score_func,\n",
        "        true_score_func = true_score_func_cp,\n",
        "        true_init_score = score_func,\n",
        "        track_score_rmse= True,\n",
        "        N_ref           = 5000,\n",
        "        N_part          = 5000,\n",
        "        trials          = 20,\n",
        "        nrows           = 4,\n",
        "        mean_bins       = 120,\n",
        "        time_split      = 'power',\n",
        "        T_end           = T_end,\n",
        "        T_target        = T_target,\n",
        "        div             = 'OT_MMD',\n",
        "        plot_hists      = True,\n",
        "        hist_mode       = 'rand',\n",
        "        display_mode    = 'min_label',\n",
        "        trial_name      = 'NN_hist_compare',\n",
        "        p_prune         = 1,\n",
        "        ref_seed        = 1,\n",
        "        plot_res = False,\n",
        "        hist_norm= 3.45,\n",
        "        plot_prior = True,\n",
        "        rand_seed = False,\n",
        "        self_normalize_hists = False,\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIPqxl9lITL8",
        "outputId": "efefc286-03a1-4357-a9af-62d268eadd3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2644331221.py:192: DeprecationWarning: This function is deprecated in favor of cupy.from_dlpack\n",
            "  x_cp = cp.fromDlpack(torch_to_dlpack(x_t))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 steps, Critic Gate: OT_MMD =3.400e-02, OT_MMD_floor = 1.900e-02, score_RMSE=1.216e+32, time=74.7s\n",
            " \n",
            "10 steps, DSM: OT_MMD =3.864e-02, OT_MMD_floor = 1.936e-02, score_RMSE=3.262e+01, time=74.6s\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm run_data/*.png"
      ],
      "metadata": {
        "id": "0CN8cxRLIYEH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}