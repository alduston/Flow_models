{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "en1yR6yBRium"
      },
      "outputs": [],
      "source": [
        "# =================== 2D AVRC training + sampling (+ standard Rectified Flow) ===================\n",
        "# Includes:\n",
        "#   • PyTorch-native OT-Flow-style 2D samplers\n",
        "#   • AVRC2D (x1-only) trainer in R^2\n",
        "#   • Standard Rectified Flow (RF) baseline trainer in R^2\n",
        "#   • 2D metrics (MMD, sliced-W2), utilities, and samplers\n",
        "#\n",
        "# Changelog (diagnostics):\n",
        "#   • sliced_w2 now supports max_n subsampling for faster diagnostics\n",
        "#   • (unchanged otherwise; logging/timing added in the viz/main cell)\n",
        "\n",
        "import math, time, os\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -------------------------------- device & dtype --------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "TDTYPE = torch.float32\n",
        "\n",
        "# --------------------------------- random utils ---------------------------------------\n",
        "def seed_everything(seed: int | None):\n",
        "    if seed is None: return\n",
        "    torch.manual_seed(seed); np.random.seed(seed)\n",
        "\n",
        "# -------------------------------- time embedding --------------------------------------\n",
        "def t_embed(t: torch.Tensor):\n",
        "    \"\"\"Simple 4-dim time features; t in [0,1], returns (B,4).\"\"\"\n",
        "    return torch.cat([t, torch.sin(2*math.pi*t), torch.cos(2*math.pi*t), t*t], dim=1)\n",
        "\n",
        "# ---------------------------------- MLP blocks ----------------------------------------\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, din, dout, hidden=128, depth=4, act=nn.SiLU):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        d = din\n",
        "        for _ in range(depth-1):\n",
        "            layers += [nn.Linear(d, hidden), act()]\n",
        "            d = hidden\n",
        "        layers += [nn.Linear(d, dout)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "# ------------------------------ Encoders / heads (D-dim) ------------------------------\n",
        "class GaussianHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Outputs [mu (D), logvar (D)] with last layer zeroed so initial mu=0, logvar=0.\n",
        "    \"\"\"\n",
        "    def __init__(self, din: int, D: int, hidden=128, depth=4):\n",
        "        super().__init__()\n",
        "        self.D = D\n",
        "        self.mlp = MLP(din, 2*D, hidden=hidden, depth=depth)\n",
        "        # force constant zero init\n",
        "        for m in reversed(list(self.mlp.modules())):\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.zeros_(m.weight); nn.init.zeros_(m.bias); break\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.mlp(x)\n",
        "        mu, logvar = h[:, :self.D], h[:, self.D:]\n",
        "        return mu, logvar\n",
        "\n",
        "def reparam(mu, logvar):\n",
        "    std = torch.exp(0.5*logvar)\n",
        "    eps = torch.randn_like(std)\n",
        "    return mu + std*eps\n",
        "\n",
        "def kl_normal_diag(mu, logvar):\n",
        "    \"\"\"\n",
        "    E_x [ KL( N(mu, diag(exp(logvar))) || N(0,I) ) ] averaged over batch.\n",
        "    \"\"\"\n",
        "    elem = mu.pow(2) + logvar.exp() - logvar - 1.0\n",
        "    return 0.5 * torch.mean(elem.sum(dim=1))\n",
        "\n",
        "class EncoderX1Only(nn.Module):\n",
        "    \"\"\"z0 = E(x1) (destination-only encoder) in D dims.\"\"\"\n",
        "    def __init__(self, D=2, hidden=128, depth=4):\n",
        "        super().__init__()\n",
        "        self.D = D\n",
        "        self.head = GaussianHead(D, D, hidden=hidden, depth=depth)\n",
        "    def forward(self, x1): return self.head(x1)\n",
        "\n",
        "class Velocity(nn.Module):\n",
        "    \"\"\"v(x,t) -> R^D (mean-field velocity in data space).\"\"\"\n",
        "    def __init__(self, D=2, hidden=128, depth=4):\n",
        "        super().__init__()\n",
        "        self.D = D\n",
        "        self.mlp = MLP(D+4, D, hidden=hidden, depth=depth)\n",
        "    def forward(self, x, t): return self.mlp(torch.cat([x, t_embed(t)], dim=1))\n",
        "\n",
        "# -------------------------------- Schedules / frames ----------------------------------\n",
        "def lin_sched(a, b, step, total):\n",
        "    s = min(max(step / max(total, 1), 0.0), 1.0)\n",
        "    return a + (b - a) * s\n",
        "\n",
        "def _make_log_stride_rounds(total_rounds: int,\n",
        "                            first_dense: int = 20,\n",
        "                            growth: float = 2.0,\n",
        "                            stride0: int = 1,\n",
        "                            max_stride: int | None = None) -> list[int]:\n",
        "    frames: list[int] = []\n",
        "    start = 1\n",
        "    L = max(1, int(first_dense))\n",
        "    stride = max(1, int(stride0))\n",
        "    while start <= total_rounds:\n",
        "        end = min(total_rounds, start + L - 1)\n",
        "        use_stride = min(stride, max_stride) if (max_stride is not None) else stride\n",
        "        frames.extend(range(start, end + 1, use_stride))\n",
        "        start = end + 1\n",
        "        L = max(1, int(round(L * growth)))\n",
        "        stride = max(1, int(round(stride * growth)))\n",
        "    return sorted(set(frames))\n",
        "\n",
        "# --------------------------------- 2D OT samplers -------------------------------------\n",
        "# All return torch.Tensor [n,2] on current device/dtype.\n",
        "@torch.no_grad()\n",
        "def sample_two_moons(n, gap=0.5, rad=1.0, noise=0.08):\n",
        "    n1 = n//2; n2 = n - n1\n",
        "    u1 = torch.rand(n1, device=device, dtype=TDTYPE) * math.pi\n",
        "    u2 = torch.rand(n2, device=device, dtype=TDTYPE) * math.pi\n",
        "    x1 = torch.stack([rad*torch.cos(u1), rad*torch.sin(u1)], dim=1)\n",
        "    x2 = torch.stack([rad*(1.0-torch.cos(u2)), -rad*torch.sin(u2)-gap], dim=1)\n",
        "    X  = torch.cat([x1, x2], dim=0)\n",
        "    if noise>0: X = X + noise*torch.randn_like(X)\n",
        "    return X\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_spiral(n, a=0.5, b=0.25, tmin=0.0, tmax=4.0*math.pi, noise=0.08):\n",
        "    t = tmin + (tmax - tmin)*torch.rand(n, device=device, dtype=TDTYPE)\n",
        "    r = a + b*t\n",
        "    X = torch.stack([r*torch.cos(t), r*torch.sin(t)], dim=1)\n",
        "    if noise>0: X = X + noise*torch.randn_like(X)\n",
        "    return X\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_rings(n, radii=(1.0, 2.0, 3.0), sigma_r=0.08):\n",
        "    radii = torch.tensor(radii, device=device, dtype=TDTYPE)\n",
        "    idx = torch.randint(0, radii.numel(), (n,), device=device)\n",
        "    R = radii[idx] + sigma_r*torch.randn(n, device=device, dtype=TDTYPE)\n",
        "    th = 2*math.pi*torch.rand(n, device=device, dtype=TDTYPE)\n",
        "    X  = torch.stack([R*torch.cos(th), R*torch.sin(th)], dim=1)\n",
        "    return X\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_checker_grid(n, cells=4, fill_frac=0.98, jitter=0.01, span=4.0):\n",
        "    i = torch.randint(0, cells, (n,), device=device)\n",
        "    j = torch.randint(0, cells, (n,), device=device)\n",
        "    parity = (i + j) % 2\n",
        "    j = (j + parity) % cells  # shift into even parity\n",
        "    cell = span / float(cells)\n",
        "    centers = torch.stack([i, j], dim=1).to(TDTYPE) + 0.5\n",
        "    U = (torch.rand(n, 2, device=device, dtype=TDTYPE) - 0.5) * (fill_frac * cell)\n",
        "    X = (-span/2.0) + centers * cell + U\n",
        "    if jitter>0: X = X + jitter*torch.randn_like(X)\n",
        "    return X\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_checker_stripes(n, span=4.0, noise=0.08):\n",
        "    x1 = (torch.rand(n, device=device, dtype=TDTYPE) - 0.5) * span\n",
        "    x2 = (torch.rand(n, device=device, dtype=TDTYPE) - 0.5) * span\n",
        "    x2 = x2 + ((torch.floor(x1) % 2) * (span/4.0))\n",
        "    X  = torch.stack([x1, x2], dim=1)\n",
        "    if noise>0: X = X + noise*torch.randn_like(X)\n",
        "    return X\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_pinwheel(n, radial_std=0.25, tangential_std=0.05, n_arms=5, rate=0.25):\n",
        "    k = torch.randint(0, n_arms, (n,), device=device)\n",
        "    r = radial_std*torch.randn(n, device=device, dtype=TDTYPE) + 1.0\n",
        "    base = k.to(TDTYPE) * (2.0*math.pi/n_arms) + rate*torch.randn(n, device=device, dtype=TDTYPE)\n",
        "    X = torch.stack([r*torch.cos(base), r*torch.sin(base)], dim=1)\n",
        "    noise = torch.randn(n, 2, device=device, dtype=TDTYPE)\n",
        "    c, s = torch.cos(base), torch.sin(base)\n",
        "    R = torch.stack([torch.stack([c, -s], dim=1),\n",
        "                     torch.stack([s,  c], dim=1)], dim=1)  # (n,2,2)\n",
        "    tang = torch.einsum('nij,nj->ni', R, noise) * tangential_std\n",
        "    return X + tang\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_scurve(n, tmin=-math.pi, tmax=math.pi, noise=0.08):\n",
        "    t = tmin + (tmax - tmin)*torch.rand(n, device=device, dtype=TDTYPE)\n",
        "    x = t\n",
        "    y = torch.sin(t) + 0.25*torch.sin(3.0*t)\n",
        "    X = torch.stack([x, y], dim=1)\n",
        "    if noise>0: X = X + noise*torch.randn_like(X)\n",
        "    return X\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_eight_gaussians(n, radius=4.0, std=0.10, weights=None):\n",
        "    ang = torch.linspace(0.0, 2.0*math.pi, 9, device=device, dtype=TDTYPE)[:-1]\n",
        "    means = torch.stack([radius*torch.cos(ang), radius*torch.sin(ang)], dim=1)  # (8,2)\n",
        "    if weights is None:\n",
        "        w = torch.full((8,), 1/8, device=device, dtype=TDTYPE)\n",
        "    else:\n",
        "        w = torch.tensor(weights, device=device, dtype=TDTYPE)\n",
        "        w = w / (w.sum() + 1e-12)\n",
        "    comp = torch.multinomial(w, num_samples=n, replacement=True)  # (n,)\n",
        "    mu = means[comp]                                             # (n,2)\n",
        "    return mu + std*torch.randn(n, 2, device=device, dtype=TDTYPE)\n",
        "\n",
        "def sample_rose_knot(\n",
        "    n,\n",
        "    k: int = 9,                 # number of petals (odd gives k petals; even gives 2k)\n",
        "    R: float = 1.25,            # base radius (≈ Gaussian scale)\n",
        "    alpha: float = 0.6,         # petal amplitude (0<alpha<1)\n",
        "    turns: float = 2.0,         # how many full wraps around the origin\n",
        "    noise: float = 0.06,        # overall noise magnitude\n",
        "    aniso: float = 2.0          # tangential vs radial noise ratio (>1 => thinner petals)\n",
        "):\n",
        "    \"\"\"\n",
        "    Adversarial 'rose-knot' distribution to maximize independent-coupling crossings.\n",
        "    Polar param: r(θ) = R * (1 + alpha * cos(k θ)).\n",
        "    θ ~ Uniform[0, 2π * turns], then add anisotropic (tangent-heavy) noise.\n",
        "\n",
        "    Typical radius range: R*(1-alpha) .. R*(1+alpha).\n",
        "    Defaults give ~0.5 .. ~2.0, i.e., near standard Gaussian scale.\n",
        "    \"\"\"\n",
        "    # angles across multiple wraps\n",
        "    theta = (2.0 * math.pi * turns) * torch.rand(n, device=device, dtype=TDTYPE)\n",
        "\n",
        "    # rose radius\n",
        "    r = R * (1.0 + alpha * torch.cos(k * theta))\n",
        "\n",
        "    # base points on the curve\n",
        "    x = r * torch.cos(theta)\n",
        "    y = r * torch.sin(theta)\n",
        "    base = torch.stack([x, y], dim=1)\n",
        "\n",
        "    # unit radial & tangential directions\n",
        "    ur = torch.stack([torch.cos(theta), torch.sin(theta)], dim=1)            # (n,2)\n",
        "    ut = torch.stack([-torch.sin(theta), torch.cos(theta)], dim=1)           # (n,2)\n",
        "\n",
        "    # anisotropic noise: thin in radial, fatter along tangent\n",
        "    radial_std = noise\n",
        "    tang_std   = noise * aniso\n",
        "    eps_r = radial_std * torch.randn(n, 1, device=device, dtype=TDTYPE)\n",
        "    eps_t = tang_std   * torch.randn(n, 1, device=device, dtype=TDTYPE)\n",
        "\n",
        "    X = base + eps_r * ur + eps_t * ut\n",
        "    return X\n",
        "\n",
        "\n",
        "# --------------------------- NEW: Sierpiński triangle target ---------------------------\n",
        "@torch.no_grad()\n",
        "def sample_sierpinski(\n",
        "    n: int,\n",
        "    *,\n",
        "    burn_in: int = 20,      # mixing steps (contractive, so this is plenty)\n",
        "    iters: int = 20,        # additional iterations; final state is sampled\n",
        "    scale: float = 2.8,     # centers to origin and scales to ~Gaussian radius\n",
        "    noise: float = 0.03,    # small jitter to thicken filaments\n",
        "):\n",
        "    \"\"\"\n",
        "    Sierpiński triangle via a 3-map IFS:\n",
        "      f_i(x) = 0.5*(x + v_i), i in {1,2,3}, with equilateral-triangle vertices.\n",
        "    We take 'burn_in + iters' contractive steps in batch, then center & scale.\n",
        "\n",
        "    The default (scale≈2.8) puts the outer radius ~1.6, i.e., close to N(0,I) scale.\n",
        "    \"\"\"\n",
        "    # Equilateral triangle vertices (unit side)\n",
        "    v = torch.tensor([\n",
        "        [0.0, 0.0],\n",
        "        [1.0, 0.0],\n",
        "        [0.5, math.sqrt(3.0)/2.0],\n",
        "    ], device=device, dtype=TDTYPE)\n",
        "\n",
        "    # Start anywhere (zeros is fine; contraction kills init quickly)\n",
        "    x = torch.zeros(n, 2, device=device, dtype=TDTYPE)\n",
        "\n",
        "    steps = burn_in + iters\n",
        "    for _ in range(steps):\n",
        "        idx = torch.randint(0, 3, (n,), device=device)\n",
        "        x = 0.5 * (x + v[idx])\n",
        "\n",
        "    # Center at triangle centroid and scale to ~Gaussian-ish spread\n",
        "    centroid = torch.tensor([0.5, math.sqrt(3.0)/6.0], device=device, dtype=TDTYPE)\n",
        "    x = (x - centroid) * scale\n",
        "\n",
        "    # Thin isotropic noise to avoid degenerate filaments\n",
        "    if noise > 0:\n",
        "        x = x + noise * torch.randn_like(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------- TARGET toggle & source ----------------------------\n",
        "TARGET = \"moons\"  # add: {\"rose_knot\",\"rose\",\"flower\"} as new options too\n",
        "\n",
        "def set_target(name: str):\n",
        "    global TARGET\n",
        "    TARGET = str(name).lower().strip()\n",
        "\n",
        "def sample_source_torch(n, D=2):\n",
        "    return torch.randn(n, D, device=device, dtype=TDTYPE)\n",
        "\n",
        "def sample_target_torch(n):\n",
        "    key = TARGET\n",
        "    if key in (\"moons\", \"two_moons\", \"two-moons\"):      return sample_two_moons(n)\n",
        "    if key == \"spiral\":                                 return sample_spiral(n)\n",
        "    if key in (\"rings\",\"concentric\"):                   return sample_rings(n)\n",
        "    if key in (\"checker\",\"checkerboard\",\"checker_grid\"):return sample_checker_grid(n)\n",
        "    if key in (\"checker_stripes\",\"checker-legacy\"):     return sample_checker_stripes(n)\n",
        "    if key == \"pinwheel\":                               return sample_pinwheel(n)\n",
        "    if key in (\"scurve\",\"s-curve\",\"s_curve\"):           return sample_scurve(n)\n",
        "    if key in (\"8g\",\"8gaussians\",\"eight_gaussians\"):    return sample_eight_gaussians(n)\n",
        "    if key in (\"rose_knot\",\"rose\",\"flower\"):            return sample_rose_knot(n)   # from earlier\n",
        "    if key in (\"sierpinski\",\"gasket\",\"tri_gasket\"):     return sample_sierpinski(n)  # <-- NEW\n",
        "    return sample_two_moons(n)\n",
        "\n",
        "def make_pairs_random(n):\n",
        "    x0 = sample_source_torch(n, D=2)\n",
        "    x1 = sample_target_torch(n)\n",
        "    return x0, x1, None\n",
        "\n",
        "# --------------------------------- Metrics (2D) ---------------------------------------\n",
        "@torch.no_grad()\n",
        "def mmd_rbf_nd(x: torch.Tensor, y: torch.Tensor, sigma=None, max_n:int = 8192):\n",
        "    \"\"\"\n",
        "    Unbiased MMD with Gaussian kernel in R^d. Subsamples to avoid OOM.\n",
        "    Returns scalar float.\n",
        "    \"\"\"\n",
        "    x = x.reshape(x.size(0), -1); y = y.reshape(y.size(0), -1)\n",
        "    if x.size(0) > max_n: x = x[torch.randint(0, x.size(0), (max_n,), device=x.device)]\n",
        "    if y.size(0) > max_n: y = y[torch.randint(0, y.size(0), (max_n,), device=y.device)]\n",
        "    n, m = x.size(0), y.size(0)\n",
        "\n",
        "    if sigma is None:\n",
        "        take = min(3000, n + m)\n",
        "        xy = torch.cat([x, y], dim=0)\n",
        "        sel = torch.randint(0, xy.size(0), (take,), device=xy.device)\n",
        "        pd = torch.cdist(xy[sel], xy[sel], p=2)\n",
        "        sigma = torch.median(pd[pd>0]).clamp(min=1e-4)\n",
        "    gamma = 1.0 / (2.0 * sigma**2)\n",
        "\n",
        "    Kxx = torch.exp(-gamma * torch.cdist(x, x, p=2).pow(2))\n",
        "    Kyy = torch.exp(-gamma * torch.cdist(y, y, p=2).pow(2))\n",
        "    Kxy = torch.exp(-gamma * torch.cdist(x, y, p=2).pow(2))\n",
        "    mmd2 = (Kxx.sum() - torch.diagonal(Kxx).sum())/(n*(n-1) + 1e-12) \\\n",
        "         + (Kyy.sum() - torch.diagonal(Kyy).sum())/(m*(m-1) + 1e-12) \\\n",
        "         - 2.0 * Kxy.mean()\n",
        "    return float(mmd2.clamp(min=0).sqrt().detach().cpu())\n",
        "\n",
        "@torch.no_grad()\n",
        "def sliced_w2(x: torch.Tensor, y: torch.Tensor, L: int = 128, max_n: int | None = None):\n",
        "    \"\"\"\n",
        "    Sliced Wasserstein-2: average 1D W2 over random projections u ~ Unif(S^{d-1}).\n",
        "    If max_n is provided, subsample both x and y to at most max_n points.\n",
        "    \"\"\"\n",
        "    x = x.reshape(x.size(0), -1); y = y.reshape(y.size(0), -1)\n",
        "    n = min(x.size(0), y.size(0))\n",
        "    if (max_n is not None) and (n > max_n):\n",
        "        idx = torch.randperm(n, device=x.device)[:max_n]\n",
        "        x = x[idx]; y = y[idx]\n",
        "        n = max_n\n",
        "    else:\n",
        "        x = x[:n]; y = y[:n]\n",
        "    d = x.size(1)\n",
        "    u = torch.randn(L, d, device=x.device, dtype=x.dtype)\n",
        "    u = u / (u.norm(dim=1, keepdim=True) + 1e-12)\n",
        "    xs = (x @ u.T).sort(dim=0).values     # (n,L)\n",
        "    ys = (y @ u.T).sort(dim=0).values\n",
        "    w2_per = torch.mean((xs - ys).pow(2), dim=0).sqrt()  # (L,)\n",
        "    return float(w2_per.mean().detach().cpu())\n",
        "\n",
        "\n",
        "# ---------------------------- 3D crossings (t on X-axis) ----------------------------\n",
        "from matplotlib import cm\n",
        "from matplotlib.colors import Normalize\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "\n",
        "\n",
        "def _set_pane_color(ax, axis: str, rgba):\n",
        "    try:\n",
        "        getattr(ax, f\"{axis}axis\").pane.set_facecolor(rgba)  # mpl ≥ 3.7\n",
        "    except Exception:\n",
        "        getattr(ax, f\"w_{axis}axis\").set_pane_color(rgba)    # back-compat\n",
        "        getattr(ax, f\"w_{axis}axis\").set_pane_color(rgba)\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def plot_crossings_hist_and_chords_2d(\n",
        "    model_or_sampler=None, *,\n",
        "    pairs_mode=\"encoder\",\n",
        "    n_pairs=60_000,\n",
        "    subset_lines=160,\n",
        "    subset_strategy=\"random\",\n",
        "    line_indices=None,                    # <— NEW: fixed row indices for chords\n",
        "    plane_mode=\"scatter\",\n",
        "    bins=220,\n",
        "    midplane=False, mid_t=0.5,\n",
        "    mid_bins=180,\n",
        "    density_gamma=0.6,\n",
        "    cmap_ref=\"Blues\", cmap_tgt=\"Oranges\", cmap_mid=\"Purples\",\n",
        "    line_color_mode=\"target_angle_turbo\",\n",
        "    solid_line_color=\"#7CFC00\",\n",
        "    line_alpha=0.5, line_width=.5, line_glow=False,\n",
        "    hsv_sat=0.95, hsv_val=0.95,\n",
        "    bg=\"white\",\n",
        "    view_elev=12, view_azim=185,\n",
        "    seed=None,\n",
        "    pairs=None,\n",
        "    title=None,\n",
        "    save_path=None, show=True,\n",
        "    name_for_console=\"crossings\",\n",
        "    t_inset_ref: float = 0.0,\n",
        "    t_inset_tgt: float = -0.02,\n",
        "    mark_hits: bool = True,\n",
        "    hit_ms: float = 1.0, hit_alpha: float = 0.8, hit_mew: float = 1.0, hit_color: str = \"k\",\n",
        "    iso_extent_std: float = 3.5,\n",
        "    iso_ring_levels: tuple = (1.0, 2.0, 3.0),\n",
        "):\n",
        "    import os\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.cm as cm\n",
        "    import matplotlib.patheffects as pe\n",
        "    from matplotlib.colors import PowerNorm\n",
        "    from matplotlib import colors as mcolors\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed); torch.manual_seed(seed)\n",
        "\n",
        "    # --------- assemble pairs ----------\n",
        "    if pairs is not None:\n",
        "        x_ref, x_tgt = pairs\n",
        "    else:\n",
        "        if pairs_mode == \"encoder\":\n",
        "            assert hasattr(model_or_sampler, \"Enc\"), \"encoder pairs require AVRC2D model\"\n",
        "            x1  = sample_target_torch(n_pairs)\n",
        "            mu, logv = model_or_sampler.Enc(x1)\n",
        "            eps = torch.randn_like(mu)\n",
        "            x_ref, x_tgt = (mu + torch.exp(0.5*logv)*eps), x1\n",
        "        elif pairs_mode == \"gauss\":\n",
        "            x_ref = torch.randn(n_pairs, 2, device=device, dtype=TDTYPE)\n",
        "            x_tgt = sample_target_torch(n_pairs)\n",
        "        else:\n",
        "            raise ValueError(\"pairs_mode must be 'encoder' or 'gauss' or pass pairs=(x_ref,x_tgt)\")\n",
        "\n",
        "    # ---- harden shapes to Nx2 before any [:, 0] indexing ----\n",
        "    def _to_np_2d(a):\n",
        "        A = a.detach().cpu().numpy() if torch.is_tensor(a) else np.asarray(a)\n",
        "        A = np.asarray(A)\n",
        "        if A.ndim == 1:\n",
        "            if A.size % 2 != 0:\n",
        "                raise ValueError(f\"Expected even length to reshape to (*,2); got {A.size}\")\n",
        "            A = A.reshape(-1, 2)\n",
        "        return A\n",
        "\n",
        "    Xr = _to_np_2d(x_ref)\n",
        "    Xt = _to_np_2d(x_tgt)\n",
        "\n",
        "    # bounds (fixed): combine target extent with isotropic Gaussian extent\n",
        "    Q = 0.997\n",
        "    x1t_min, x1t_max = np.quantile(Xt[:,0], 1-Q), np.quantile(Xt[:,0], Q)\n",
        "    x2t_min, x2t_max = np.quantile(Xt[:,1], 1-Q), np.quantile(Xt[:,1], Q)\n",
        "    R = float(iso_extent_std)\n",
        "    x1min, x1max = min(-R, x1t_min), max(R, x1t_max)\n",
        "    x2min, x2max = min(-R, x2t_min), max(R, x2t_max)\n",
        "\n",
        "    # --------- figure setup ----------\n",
        "    plt.style.use(\"default\")\n",
        "    fig = plt.figure(figsize=(11.5, 6.8))\n",
        "    ax  = fig.add_subplot(111, projection=\"3d\")\n",
        "    ax.set_facecolor(bg)\n",
        "    tickc = \"white\" if bg == \"black\" else \"black\"\n",
        "    if bg == \"black\":\n",
        "        fig.patch.set_facecolor(\"black\")\n",
        "        _set_pane_color(ax, 'x', (0, 0, 0, 0))\n",
        "        _set_pane_color(ax, 'y', (0, 0, 0, 0))\n",
        "        _set_pane_color(ax, 'z', (0, 0, 0, 0))\n",
        "        for spine in ax.spines.values(): spine.set_color(\"white\")\n",
        "\n",
        "    ax.view_init(elev=view_elev, azim=view_azim)\n",
        "    t_lo = min(0.0, 0.0 + t_inset_ref)\n",
        "    t_hi = max(1.0, 1.0 + t_inset_tgt)\n",
        "    ax.set_xlim(t_lo, t_hi)\n",
        "    ax.set_ylim(x1min, x1max); ax.set_zlim(x2min, x2max)\n",
        "    ax.set_xlabel(\"t\", color=tickc); ax.set_ylabel(\"$x_1$\", color=tickc); ax.set_zlabel(\"$x_2$\", color=tickc)\n",
        "    ax.tick_params(colors=tickc)\n",
        "\n",
        "    # --------- plane helpers ----------\n",
        "    def _plane_heat(ax, t_const, X, cmap, bins, alpha=0.97):\n",
        "        H, xedges, yedges = np.histogram2d(X[:,0], X[:,1], bins=bins,\n",
        "                                           range=[[x1min, x1max],[x2min, x2max]], density=True)\n",
        "        norm = PowerNorm(gamma=max(1e-3, density_gamma))\n",
        "        C = cm.get_cmap(cmap)(norm(H).T)\n",
        "        yy, zz = np.meshgrid(xedges[:-1], yedges[:-1], indexing=\"ij\")\n",
        "        tt = np.full_like(yy, float(t_const))\n",
        "        ax.plot_surface(tt, yy, zz, rstride=1, cstride=1,\n",
        "                        facecolors=C, shade=False, antialiased=False, linewidth=0, alpha=alpha)\n",
        "        return norm\n",
        "\n",
        "    def _density_lookup(X, bins):\n",
        "        H, xe, ye = np.histogram2d(\n",
        "            X[:,0], X[:,1], bins=bins,\n",
        "            range=[[x1min, x1max],[x2min, x2max]], density=True\n",
        "        )\n",
        "        i1 = np.clip(np.searchsorted(xe, X[:,0]) - 1, 0, H.shape[0]-1)\n",
        "        i2 = np.clip(np.searchsorted(ye, X[:,1]) - 1, 0, H.shape[1]-1)\n",
        "        d = H[i1, i2]\n",
        "        return H, d\n",
        "\n",
        "    if plane_mode == \"heatmap\":\n",
        "        cnr = _plane_heat(ax, 0.0, Xr, cmap_ref, bins=bins, alpha=0.96)\n",
        "        cnt = _plane_heat(ax, 1.0, Xt, cmap_tgt, bins=bins, alpha=0.96)\n",
        "        cbr = fig.colorbar(cm.ScalarMappable(norm=cnr, cmap=cmap_ref), ax=ax, fraction=0.026, pad=0.04)\n",
        "        cbt = fig.colorbar(cm.ScalarMappable(norm=cnt, cmap=cmap_tgt), ax=ax, fraction=0.026, pad=0.01)\n",
        "        cbr.set_label(\"ref density @ t=0\"); cbt.set_label(\"target density @ t=1\")\n",
        "    else:\n",
        "        Href, d_ref = _density_lookup(Xr, bins)\n",
        "        Htgt, d_tgt = _density_lookup(Xt, bins)\n",
        "        cnr = PowerNorm(gamma=max(1e-3, density_gamma))\n",
        "        cnt = PowerNorm(gamma=max(1e-3, density_gamma))\n",
        "        ax.scatter(np.full(Xr.shape[0], 0.0), Xr[:,0], Xr[:,1],\n",
        "                   c=d_ref, cmap=cmap_ref, norm=cnr, s=1.2, alpha=0.65,\n",
        "                   depthshade=False, zorder=1, rasterized=True)\n",
        "        cbr = fig.colorbar(cm.ScalarMappable(norm=cnr, cmap=cmap_ref), ax=ax, fraction=0.026, pad=0.04)\n",
        "        cbt = fig.colorbar(cm.ScalarMappable(norm=cnt, cmap=cmap_tgt), ax=ax, fraction=0.026, pad=0.01)\n",
        "        cbr.set_label(\"ref density @ t=0\"); cbt.set_label(\"target density @ t=1\")\n",
        "\n",
        "    # --------- optional mid-plane ----------\n",
        "    if midplane:\n",
        "        M = 0.5*(Xr + Xt)\n",
        "        Hm, xe, ye = np.histogram2d(M[:,0], M[:,1], bins=mid_bins,\n",
        "                                    range=[[x1min, x1max],[x2min, x2max]], density=True)\n",
        "        norm_m = PowerNorm(gamma=0.7 if density_gamma is None else density_gamma)\n",
        "        Cm = cm.get_cmap(cmap_mid)(norm_m(Hm).T)\n",
        "        yy, zz = np.meshgrid(xe[:-1], ye[:-1], indexing=\"ij\")\n",
        "        tt = np.full_like(yy, float(mid_t))\n",
        "        ax.plot_surface(tt, yy, zz, rstride=1, cstride=1,\n",
        "                        facecolors=Cm, shade=False, antialiased=False, linewidth=0, alpha=0.60, zorder=2)\n",
        "\n",
        "    # --------- choose chords (fixed if line_indices provided) ----------\n",
        "    if line_indices is not None:\n",
        "        keep_idx = np.asarray(line_indices, dtype=int)\n",
        "        keep_idx = keep_idx[(keep_idx >= 0) & (keep_idx < Xr.shape[0])]\n",
        "        if keep_idx.size == 0:\n",
        "            keep_idx = np.arange(min(subset_lines, Xr.shape[0]))\n",
        "    else:\n",
        "        disp  = Xt - Xr\n",
        "        theta = np.arctan2(disp[:,1], disp[:,0]) % (2*np.pi)\n",
        "        length= np.linalg.norm(disp, axis=1)\n",
        "        if subset_lines >= Xr.shape[0]:\n",
        "            keep_idx = np.arange(Xr.shape[0])\n",
        "        elif subset_strategy == \"angle_stratified\":\n",
        "            nb = max(8, int(np.sqrt(subset_lines)))\n",
        "            bins_theta = np.linspace(0, 2*np.pi, nb+1)\n",
        "            keep = []\n",
        "            for b in range(nb):\n",
        "                mask = (theta >= bins_theta[b]) & (theta < bins_theta[b+1])\n",
        "                cand = np.where(mask)[0]\n",
        "                if cand.size == 0: continue\n",
        "                k = max(1, int(np.ceil(subset_lines/nb)))\n",
        "                sel = cand[np.argsort(length[cand])[-k:]] if cand.size > k else cand\n",
        "                keep.append(sel)\n",
        "            keep_idx = np.unique(np.concatenate(keep))[:subset_lines]\n",
        "        elif subset_strategy == \"longest\":\n",
        "            keep_idx = np.argsort(length)[-subset_lines:]\n",
        "        else:\n",
        "            keep_idx = np.random.choice(Xr.shape[0], size=subset_lines, replace=False)\n",
        "\n",
        "    # --------- line color util ----------\n",
        "    turbo = cm.get_cmap(\"turbo\")\n",
        "    def _color_from_target(x_t):\n",
        "        if line_color_mode == \"solid\":\n",
        "            return solid_line_color\n",
        "        elif line_color_mode == \"target_angle_turbo\":\n",
        "            ang = (np.arctan2(x_t[1], x_t[0]) % (2*np.pi)) / (2*np.pi)\n",
        "            return turbo(ang)\n",
        "        elif line_color_mode == \"target_angle_hsv\":\n",
        "            ang = (np.arctan2(x_t[1], x_t[0]) % (2*np.pi)) / (2*np.pi)\n",
        "            return mcolors.hsv_to_rgb([ang, hsv_sat, hsv_val])\n",
        "        else:\n",
        "            u = (x_t[0] - x1min) / (x1max - x1min + 1e-9)\n",
        "            v = (x_t[1] - x2min) / (x2max - x2min + 1e-9)\n",
        "            c1 = cm.get_cmap(\"plasma\")(u); c2 = cm.get_cmap(\"viridis\")(v)\n",
        "            rgb = 0.60*np.array(c1[:3]) + 0.40*np.array(c2[:3])\n",
        "            return np.clip(rgb, 0, 1)\n",
        "\n",
        "    # --------- draw the lines ----------\n",
        "    pefx = [pe.Stroke(linewidth=line_width+1.2, foreground=\"k\", alpha=0.85), pe.Normal()] if line_glow else None\n",
        "    t0_draw = 0.0 + t_inset_ref\n",
        "    t1_draw = 1.0 + t_inset_tgt\n",
        "\n",
        "    target_hits_y, target_hits_z = [], []\n",
        "    for i in keep_idx.tolist():\n",
        "        x0, x1v = Xr[i], Xt[i]\n",
        "        line_col = _color_from_target(x1v)\n",
        "        ax.plot([t0_draw, t1_draw], [x0[0], x1v[0]], [x0[1], x1v[1]],\n",
        "                color=line_col, alpha=line_alpha, lw=line_width,\n",
        "                linestyle=\"--\", dash_capstyle=\"round\", path_effects=pefx, zorder=5, rasterized=True)\n",
        "        if mark_hits:\n",
        "            ax.plot([0.0], [x0[0]], [x0[1]],\n",
        "                    marker='x', markersize=hit_ms, markeredgewidth=hit_mew,\n",
        "                    color=line_col, alpha=hit_alpha, zorder=8)\n",
        "            target_hits_y.append(x1v[0]); target_hits_z.append(x1v[1])\n",
        "\n",
        "    # --------- TARGET scatter drawn last (scatter mode) ----------\n",
        "    if plane_mode == \"scatter\":\n",
        "        ax.scatter(np.full(Xt.shape[0], 1.0), Xt[:,0], Xt[:,1],\n",
        "                   c=d_tgt, cmap=cmap_tgt, norm=cnt, s=1.2, alpha=0.85,\n",
        "                   depthshade=False, zorder=20, rasterized=True)\n",
        "        try: ax.collections[-1].set_zsort('min')\n",
        "        except Exception: pass\n",
        "\n",
        "    if mark_hits and len(target_hits_y) > 0:\n",
        "        ax.plot([1.0]*len(target_hits_y), target_hits_y, target_hits_z,\n",
        "                linestyle='None', marker='x', markersize=hit_ms, markeredgewidth=hit_mew,\n",
        "                color=hit_color, alpha=hit_alpha, zorder=25)\n",
        "\n",
        "    # means + rings + save\n",
        "    mr = Xr.mean(axis=0); mt = Xt.mean(axis=0)\n",
        "    ax.scatter([0.0],[mr[0]],[mr[1]], s=70, c=\"#00e5ff\", edgecolors=\"k\",\n",
        "               depthshade=False, label=\"ref mean\", zorder=30)\n",
        "    ax.scatter([1.0],[mt[0]],[mt[1]], s=70, c=\"#ffd400\", edgecolors=\"k\",\n",
        "               depthshade=False, label=\"target mean\", zorder=30)\n",
        "    ax.legend(loc=\"upper left\", facecolor=\"none\", edgecolor=(\"white\" if bg==\"black\" else \"black\"))\n",
        "    ax.set_title(title or f\"Crossings ({pairs_mode} pairs) — 3D\", color=tickc)\n",
        "\n",
        "    if iso_ring_levels and len(iso_ring_levels) > 0:\n",
        "        tt = np.linspace(0, 2*np.pi, 600)\n",
        "        for r in iso_ring_levels:\n",
        "            yy = r * np.cos(tt); zz = r * np.sin(tt)\n",
        "            ax.plot(np.zeros_like(tt), yy, zz, color=(1,1,1,0.95), lw=0.9, ls=\"--\", zorder=3)\n",
        "\n",
        "    Xproj = np.vstack([Xr, Xt]) - np.mean(np.vstack([Xr, Xt]), axis=0, keepdims=True)\n",
        "    _, _, vh = np.linalg.svd(Xproj, full_matrices=False)\n",
        "    v = vh[0]\n",
        "    p0, p1 = Xr @ v, Xt @ v\n",
        "    m = min(4000, p0.size); I = np.random.choice(p0.size, size=m, replace=False)\n",
        "    r0 = np.argsort(np.argsort(p0[I])); r1 = np.argsort(np.argsort(p1[I]))\n",
        "    invs = np.sum(np.sign(r0[:,None]-r0[None,:]) != np.sign(r1[:,None]-r1[None,:]))\n",
        "    print(f\"[{name_for_console}] approx crossing ratio: {invs/(m*(m-1)):.4f} (m={m})\")\n",
        "\n",
        "    if save_path is not None:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        fig.savefig(save_path, dpi=160, bbox_inches=\"tight\", facecolor=fig.get_facecolor())\n",
        "    if show: plt.show()\n",
        "    else:    plt.close(fig)\n",
        "\n",
        "\n",
        "# ========================= NEW: RF-on-current-coupling probe ==========================\n",
        "# Put this in the same cell as AVRC2D / utilities (after your imports).\n",
        "\n",
        "# --- config additions ----------------------------------------------------------------\n",
        "@dataclass\n",
        "class AVRCConfig2D:\n",
        "    D: int = 2\n",
        "    rounds: int = 1000\n",
        "    batch: int = 4096\n",
        "    log_every: int = 10\n",
        "\n",
        "    init_default: str = \"gauss\"          # {\"gauss\",\"encoder\",\"identity_fuzz\"}\n",
        "    init_fuzz_eps: float = 1e-2          # ε for identity+fuzz (variance, not std)\n",
        "\n",
        "    # nets\n",
        "    enc_hidden: int = 128\n",
        "    enc_depth:  int = 4\n",
        "    vel_hidden: int = 128\n",
        "    vel_depth:  int = 4\n",
        "\n",
        "    # critic (velocity head)\n",
        "    pretrain_steps: int = 10000\n",
        "    critic_lr: float = 1e-3\n",
        "    critic_weight_decay: float = 1e-5\n",
        "    critic_clip: float = 1.0\n",
        "    critic_huber_delta: float = 1e9\n",
        "\n",
        "    # time emphasis\n",
        "    critic_t_alpha: float = 1.0 #0.5\n",
        "    critic_t_beta:  float = 1.0 #4.0\n",
        "    critic_t_gamma: float = 0.0 #4.0\n",
        "    # --- in your config (defaults shown) ---\n",
        "    critic_match_rf_midpoints: bool = True\n",
        "    critic_match_rf_K: int | None = None   # None -> reuse recon_k\n",
        "\n",
        "    # organizer\n",
        "    enc_lr: float = 1e-3\n",
        "    ed_clip: float = 1.0\n",
        "    lam_disp_start: float = 0.0\n",
        "    lam_disp_end:   float = 1.0\n",
        "    lam_kl_start:   float = 1.0\n",
        "    lam_kl_end:     float = 1.0\n",
        "    unbiased_dispersion: bool = True\n",
        "    # NEW: separate anneal for the alignment piece (None => copy lam_disp schedule)\n",
        "    lam_align_start: float | None = None\n",
        "    lam_align_end:   float | None = None\n",
        "    post_anneal_rounds: int = 0\n",
        "\n",
        "    # teacher policy\n",
        "    teacher_mode: str = \"intra\"\n",
        "    intra_ema_decay: float = 0.95\n",
        "    ema_decay: float = 0.98\n",
        "    critic_adapt_min: int = 20\n",
        "    critic_adapt_max: int = 1000\n",
        "    critic_tol: float = 0.97\n",
        "\n",
        "    # eval\n",
        "    log_k: int = 8\n",
        "    log_trials: int = 50\n",
        "    log_n: int = 8192\n",
        "    log_mmd_max_n: int = 8192\n",
        "    agg_kl_batch: int = 65536\n",
        "    recon_k: int = 8\n",
        "    recon_n: int = 8192\n",
        "\n",
        "    # ---- crossings-3D viz (already added earlier) ----\n",
        "    k_plot: int = 10\n",
        "    viz_cross_dir: str = \"viz_crossings3d\"\n",
        "    viz_subset_strategy: str = \"random\"   # {\"angle_stratified\",\"longest\",\"random\"}\n",
        "    viz_seed: int = 123\n",
        "    viz_camera: tuple = (20, -30)\n",
        "    viz_pairs: int = 100000\n",
        "    viz_subset_lines: int = 160\n",
        "    viz_bins: int = 160\n",
        "    viz_plane_mode: str = \"scatter\"\n",
        "    viz_density_gamma: float = 1.2\n",
        "    viz_cmap_ref: str = \"Blues\"\n",
        "    viz_cmap_tgt: str = \"Oranges\"\n",
        "    viz_seed: int = 123\n",
        "    viz_camera: tuple = (20, -30)\n",
        "\n",
        "    # === NEW: periodic “fresh RF on current coupling” probe ====================\n",
        "    test_rf_every: int = 10              # run probe every this many rounds\n",
        "    test_rf_steps: int = 10_000          # RF training iters\n",
        "    test_rf_batch: int = 2048\n",
        "    test_rf_lr: float = 1e-3\n",
        "    test_rf_clip: float = 1.0\n",
        "    test_rf_hidden: int = 128\n",
        "    test_rf_depth: int  = 4\n",
        "    test_rf_log_every: int = 10000\n",
        "    test_rf_Ks: tuple = (2,4,8,16, 32)\n",
        "    test_rf_n: int = 80_000              # samples used for metrics/plots\n",
        "    test_rf_mmd_max_n: int = 8192\n",
        "    test_rf_bins: int = 200\n",
        "    test_rf_outdir: str = \"rf_snapshots\" # where the grid image + txt logs go\n",
        "    test_rf_seed: int | None = 777\n",
        "\n",
        "\n",
        "    # ---- add to AVRCConfig2D ----------------------------------------------------\n",
        "    # Latent scatter snapshots (same cadence/dir as crossings frames)\n",
        "    viz_latent: bool = True\n",
        "    viz_latent_color_mode: str = \"hsv2d\"  # {\"hsv2d\",\"target_angle_turbo\"}\n",
        "    viz_latent_bg: str = \"black\"\n",
        "    viz_latent_s: float = 50.0\n",
        "    viz_latent_alpha: float = 0.75\n",
        "    viz_latent_rings: bool = True\n",
        "    viz_latent_ring_levels: tuple = (1.0, 2.0, 3.0)\n",
        "    viz_latent_points: int = 1000   # set as you like (<= viz_pairs)\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_init_points(n: int,\n",
        "                       mode: str = \"gauss\",\n",
        "                       *,\n",
        "                       enc: nn.Module | None = None,\n",
        "                       eps: float = 1e-2):\n",
        "    \"\"\"\n",
        "    Returns z0 ~ q0(. | x1) depending on `mode`.\n",
        "      - \"gauss\":            N(0, I)         (ignores x1)\n",
        "      - \"encoder\":          reparam(Enc(x1))  (needs enc)\n",
        "      - \"identity_fuzz\":    N(x1, eps I)\n",
        "    \"\"\"\n",
        "    if mode == \"gauss\":\n",
        "        return torch.randn(n, 2, device=device, dtype=TDTYPE)\n",
        "\n",
        "    elif mode == \"encoder\":\n",
        "        assert enc is not None, \"encoder init requires enc\"\n",
        "        x1 = sample_target_torch(n)\n",
        "        mu, logv = enc(x1)\n",
        "        return reparam(mu, logv)\n",
        "\n",
        "    elif mode in (\"identity_fuzz\", \"x_fuzz\", \"id_fuzz\"):\n",
        "        x1 = sample_target_torch(n)\n",
        "        return x1 + math.sqrt(max(eps, 1e-12)) * torch.randn_like(x1)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"unknown init mode: {mode}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- RF pieces (shared) ---------------------------------------------------------------\n",
        "class VelocityX2D(nn.Module):\n",
        "    \"\"\"Rectified-flow velocity v_x(x,t): R^2 × [0,1] → R^2.\"\"\"\n",
        "    def __init__(self, hidden=128, depth=4):\n",
        "        super().__init__()\n",
        "        self.net = MLP(2+4, 2, hidden=hidden, depth=depth)\n",
        "    def forward(self, x, t): return self.net(torch.cat([x, t_embed(t)], dim=1))\n",
        "\n",
        "@torch.no_grad()\n",
        "def _disp_metrics_vec(pred: torch.Tensor, ell: torch.Tensor):\n",
        "    eps = 1e-8\n",
        "    resid2 = (pred-ell).pow(2).sum(dim=1)\n",
        "    ell2   = ell.pow(2).sum(dim=1)\n",
        "    mse  = float(resid2.mean().detach().cpu())\n",
        "    nmse = float((resid2/(ell2+eps)).mean().detach().cpu())\n",
        "    return {\"mse\": mse, \"nmse\": nmse}\n",
        "\n",
        "def train_rectified_flow_on_pairs_2d(make_pairs_fn,\n",
        "                                     steps=10_000, batch=2048, lr=1e-3, clip=1.0,\n",
        "                                     hidden=128, depth=4, log_every=200, seed=None):\n",
        "    \"\"\"\n",
        "    Generic RF trainer on arbitrary pair generator:\n",
        "      make_pairs_fn(B) -> (x0, x1), both (B,2) tensors on device.\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed); np.random.seed(seed)\n",
        "    Vx = VelocityX2D(hidden=hidden, depth=depth).to(device)\n",
        "    opt = torch.optim.Adam(Vx.parameters(), lr=lr, betas=(0.9,0.99))\n",
        "    t0 = time.time()\n",
        "    for it in range(1, steps+1):\n",
        "        x0, x1 = make_pairs_fn(batch)                 # endpoints\n",
        "        t  = torch.rand(batch,1, device=device, dtype=TDTYPE)\n",
        "        xt = (1.0 - t)*x0 + t*x1\n",
        "        ell= (x1 - x0).detach()\n",
        "        pred = Vx(xt, t)\n",
        "        loss = F.mse_loss(pred, ell)\n",
        "        opt.zero_grad(set_to_none=True); loss.backward()\n",
        "        nn.utils.clip_grad_norm_(Vx.parameters(), clip); opt.step()\n",
        "        if (it % log_every) == 0:\n",
        "            dm = _disp_metrics_vec(pred, ell)\n",
        "            print(f\"[fresh-RF] step {it}/{steps}  loss={float(loss):.4f}  NMSE={dm['nmse']:.4f}  (+{time.time()-t0:.1f}s)\")\n",
        "            t0 = time.time()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sampler(n: int, nfe: int, init=\"gauss\", enc: nn.Module | None = None, fuzz_eps: float = 1e-2):\n",
        "        x = sample_init_points(n, init, enc=enc, eps=fuzz_eps)\n",
        "        dt = 1.0/float(max(nfe,1))\n",
        "        for i in range(nfe):\n",
        "            t = torch.full((n,1), (i+0.5)*dt, device=device, dtype=TDTYPE)\n",
        "            x = x + dt * Vx(x, t)\n",
        "        return x\n",
        "    return Vx, sampler\n",
        "\n",
        "\n",
        "\n",
        "# --- plotting for the probe -----------------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def _plot_rf_probe_grid_2d(\n",
        "    sampler, enc, Ks, n, bins=180, out_path=None,\n",
        "    title=\"fresh RF probe\",\n",
        "    cmap=\"magma\",\n",
        "    gamma=0.42,             # lower -> brighter (0.35–0.55 good)\n",
        "    vmax_percentile=99.7,   # clip global vmax to boost brightness\n",
        "    # --- unused now: kept to avoid changing the signature ---\n",
        "    tgt_outer_sigma=2.6,\n",
        "    tgt_outer_frac=0.10,\n",
        "    tgt_linewidth=1.6\n",
        "):\n",
        "    \"\"\"\n",
        "    Grid with rows=K and cols=3:\n",
        "      [Target histogram | init=encoder | init=gauss]\n",
        "    Uses the SAME x_tgt draw for all panels.\n",
        "    \"\"\"\n",
        "    import os, gc, numpy as np, matplotlib.pyplot as plt, matplotlib as mpl\n",
        "    from matplotlib.colors import PowerNorm\n",
        "\n",
        "    # -------------------- NUCLEAR RESET --------------------\n",
        "    try: plt.close('all')\n",
        "    except Exception: pass\n",
        "    mpl.rcdefaults()\n",
        "\n",
        "    with plt.rc_context({\n",
        "        \"figure.facecolor\": \"black\",\n",
        "        \"axes.facecolor\":   \"black\",\n",
        "        \"savefig.facecolor\":\"black\",\n",
        "        \"axes.edgecolor\":   \"white\",\n",
        "        \"axes.labelcolor\":  \"white\",\n",
        "        \"xtick.color\":      \"white\",\n",
        "        \"ytick.color\":      \"white\",\n",
        "    }):\n",
        "        if out_path is not None:\n",
        "            os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "\n",
        "        # Fixed target\n",
        "        x_tgt = sample_target_torch(n).detach().cpu().numpy()\n",
        "\n",
        "        # Stable bounds from target\n",
        "        q = 0.997\n",
        "        xlim = (np.quantile(x_tgt[:,0], 1-q), np.quantile(x_tgt[:,0], q))\n",
        "        xlim = [val* 1.2 for val in xlim]\n",
        "        ylim = (np.quantile(x_tgt[:,1], 1-q), np.quantile(x_tgt[:,1], q))\n",
        "        ylim = [val* 1.2 for val in ylim]\n",
        "\n",
        "        # Histograms\n",
        "        y_edges = np.linspace(xlim[0], xlim[1], bins+1)\n",
        "        z_edges = np.linspace(ylim[0], ylim[1], bins+1)\n",
        "\n",
        "        # Target density (shown directly in leftmost column)\n",
        "        Ht, *_ = np.histogram2d(x_tgt[:,0], x_tgt[:,1], bins=[y_edges, z_edges], density=True)\n",
        "\n",
        "        # Collect model histograms for global norm\n",
        "        panels = []\n",
        "        for K in Ks:\n",
        "            xe = sampler(n, K, init=\"encoder\", enc=enc).detach().cpu().numpy()\n",
        "            He, *_ = np.histogram2d(xe[:,0], xe[:,1], bins=[y_edges, z_edges], density=True)\n",
        "            panels.append((K, \"init: encoder\", He))\n",
        "\n",
        "            xg = sampler(n, K, init=\"gauss\", enc=enc).detach().cpu().numpy()\n",
        "            Hg, *_ = np.histogram2d(xg[:,0], xg[:,1], bins=[y_edges, z_edges], density=True)\n",
        "            panels.append((K, \"init: gauss\", Hg))\n",
        "\n",
        "        # Global normalization (include target histogram)\n",
        "        all_vals = np.concatenate([Ht.ravel()] + [H.ravel() for (_, _, H) in panels])\n",
        "        vmax = np.percentile(all_vals, vmax_percentile)\n",
        "        norm = PowerNorm(gamma=max(1e-3, float(gamma)), vmin=0.0, vmax=max(1e-9, vmax))\n",
        "\n",
        "        # Now 3 columns: Target | Encoder | Gauss\n",
        "        fig, axs = plt.subplots(len(Ks), 3, figsize=(12.0, 3.0*len(Ks)), sharex=True, sharey=True)\n",
        "        if len(Ks) == 1:\n",
        "            axs = np.array([axs])\n",
        "\n",
        "        for r, K in enumerate(Ks):\n",
        "            # Leftmost: target histogram\n",
        "            ax_t = axs[r, 0]\n",
        "            ax_t.set_facecolor(\"black\")\n",
        "            ax_t.imshow(\n",
        "                Ht.T, origin=\"lower\",\n",
        "                extent=[xlim[0], xlim[1], ylim[0], ylim[1]],\n",
        "                cmap=cmap, norm=norm, interpolation=\"bilinear\", alpha=1.0, aspect=\"equal\"\n",
        "            )\n",
        "            ax_t.set_title(\"target\", color='w', pad=3)\n",
        "            if r == len(Ks)-1: ax_t.set_xlabel(\"x\", color='w')\n",
        "            ax_t.set_ylabel(\"y\", color='w')\n",
        "            ax_t.tick_params(color='w', labelcolor='w')\n",
        "\n",
        "            # Model panels (no contours)\n",
        "            for c, init_label in enumerate((\"init: encoder\", \"init: gauss\"), start=1):\n",
        "                H = panels[2*r + (c-1)][2]\n",
        "                ax = axs[r, c]\n",
        "                ax.set_facecolor(\"black\")\n",
        "                ax.imshow(\n",
        "                    H.T, origin=\"lower\",\n",
        "                    extent=[xlim[0], xlim[1], ylim[0], ylim[1]],\n",
        "                    cmap=cmap, norm=norm, interpolation=\"bilinear\", alpha=1.0, aspect=\"equal\"\n",
        "                )\n",
        "                ax.set_title(f\"K={K}  ({init_label})\", color='w', pad=3)\n",
        "                if r == len(Ks)-1: ax.set_xlabel(\"x\", color='w')\n",
        "                if c == 1: ax.set_ylabel(\"y\", color='w')\n",
        "                ax.tick_params(color='w', labelcolor='w')\n",
        "\n",
        "        fig.suptitle(title, y=0.995, color='w')\n",
        "        fig.tight_layout()\n",
        "        if out_path is not None:\n",
        "            fig.savefig(out_path, dpi=190, bbox_inches=\"tight\", facecolor='black')\n",
        "        plt.close(fig)\n",
        "\n",
        "    gc.collect()\n",
        "    return out_path\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def plot_encoder_means_colored_by_x(\n",
        "    Enc, x1, eps, *,\n",
        "    out_path=None,\n",
        "    color_mode=\"hsv2d\",\n",
        "    bg=\"black\",\n",
        "    s=.5, alpha=0.85, edge_lw=0.0,\n",
        "    add_gaussian_rings=True,\n",
        "    ring_levels=(1.0, 2.0, 3.0),\n",
        "    title=None,\n",
        "    iso_extent_std: float = 3.2,\n",
        "    # NEW: density background options\n",
        "    add_color_density: bool = False,\n",
        "    density_res: int = 60,           # grid resolution (pixels per side)\n",
        "    density_alpha: float = 0.65,      # overlay transparency\n",
        "    density_chunk: int = 256,         # batch size for summation over means\n",
        "    density_clip_q: float = 99.5,     # robust clip for intensity normalization\n",
        "):\n",
        "    \"\"\"\n",
        "    If add_color_density is True, shades the background by sum_i N_i(g)*color_i,\n",
        "    where N_i(g) is the unnormalized Gaussian density contribution at gridpoint g\n",
        "    from the encoder's covariance for sample i, and color_i is that sample's color.\n",
        "    Hue comes from the weighted contributors; brightness tracks total mass.\n",
        "    \"\"\"\n",
        "    import numpy as np, os\n",
        "    import matplotlib.pyplot as plt\n",
        "    from matplotlib import colors as mcolors\n",
        "    import matplotlib.cm as cm\n",
        "\n",
        "    # Expect Enc(x1) -> (mu, logv or cov). We accept:\n",
        "    #  - logv shape (N,2): diagonal log-variance\n",
        "    #  - cov  shape (N,2,2): full covariance\n",
        "    mu, second = Enc(x1)\n",
        "    M  = mu.detach().cpu().numpy()          # (N, 2)\n",
        "    X  = x1.detach().cpu().numpy()          # (N, 2)\n",
        "    if M.ndim == 1: M = M.reshape(-1, 2)\n",
        "    if X.ndim == 1: X = X.reshape(-1, 2)\n",
        "\n",
        "    # Color per x\n",
        "    q = 0.997\n",
        "    if color_mode == \"target_angle_turbo\":\n",
        "        ang = (np.arctan2(X[:,1], X[:,0]) % (2*np.pi)) / (2*np.pi)\n",
        "        colors = cm.get_cmap(\"turbo\")(ang)[:, :3]   # drop alpha\n",
        "    else:\n",
        "        x1min, x1max = np.quantile(X[:,0], 1-q), np.quantile(X[:,0], q)\n",
        "        x2min, x2max = np.quantile(X[:,1], 1-q), np.quantile(X[:,1], q)\n",
        "        u = np.clip((X[:,0] - x1min) / (x1max - x1min + 1e-9), 0, 1)\n",
        "        v = np.clip((X[:,1] - x2min) / (x2max - x2min + 1e-9), 0, 1)\n",
        "        hsv = np.stack([u, 0.35 + 0.65*v, np.full_like(u, 0.95)], axis=1)\n",
        "        colors = mcolors.hsv_to_rgb(hsv)            # (N, 3)\n",
        "\n",
        "    R = float(iso_extent_std)\n",
        "    xlim = (-R, R); ylim = (-R, R)\n",
        "\n",
        "    plt.style.use(\"default\")\n",
        "    fig, ax = plt.subplots(figsize=(7.4, 8.2))\n",
        "    fig.patch.set_facecolor(bg); ax.set_facecolor(bg)\n",
        "    tickc = \"white\" if bg == \"black\" else \"black\"\n",
        "    for spine in ax.spines.values(): spine.set_color(tickc)\n",
        "\n",
        "    # ---------- NEW: density-tinted background ----------\n",
        "    if add_color_density:\n",
        "        N = M.shape[0]\n",
        "        # Parse covariance description\n",
        "        cov_mode = \"iso1\"\n",
        "        a = b = c = None   # entries of inv(cov): [[a, b], [b, c]]\n",
        "        vx = vy = None\n",
        "\n",
        "        if second is not None:\n",
        "            S = second.detach().cpu().numpy()\n",
        "            if S.ndim == 2 and S.shape[1] == 2:\n",
        "                # diagonal log-variance\n",
        "                vx = np.exp(S[:, 0])\n",
        "                vy = np.exp(S[:, 1])\n",
        "                cov_mode = \"diag\"\n",
        "            elif S.ndim == 3 and S.shape[1:] == (2, 2):\n",
        "                # full covariance; build inverse once\n",
        "                det = S[:, 0, 0]*S[:, 1, 1] - S[:, 0, 1]*S[:, 1, 0]\n",
        "                # Handle near-singular just in case\n",
        "                det = np.where(det == 0, 1e-12, det)\n",
        "                a =  S[:, 1, 1] / det\n",
        "                b = -S[:, 0, 1] / det\n",
        "                c =  S[:, 0, 0] / det\n",
        "                cov_mode = \"full\"\n",
        "            else:\n",
        "                cov_mode = \"iso1\"  # fallback\n",
        "        else:\n",
        "            cov_mode = \"iso1\"\n",
        "\n",
        "        # Build grid\n",
        "        xs = np.linspace(xlim[0], xlim[1], density_res)\n",
        "        ys = np.linspace(ylim[0], ylim[1], density_res)\n",
        "        GX, GY = np.meshgrid(xs, ys, indexing=\"xy\")  # (H, W)\n",
        "\n",
        "        accum_rgb   = np.zeros((density_res, density_res, 3), dtype=np.float64)\n",
        "        density_sum = np.zeros((density_res, density_res), dtype=np.float64)\n",
        "\n",
        "        # Sum contributions in chunks to keep memory sane\n",
        "        for start in range(0, N, density_chunk):\n",
        "            end = min(start + density_chunk, N)\n",
        "            bx = M[start:end, 0][:, None, None]  # (B,1,1)\n",
        "            by = M[start:end, 1][:, None, None]\n",
        "            dx = GX[None, :, :] - bx             # (B,H,W)\n",
        "            dy = GY[None, :, :] - by\n",
        "\n",
        "            if cov_mode == \"diag\":\n",
        "                vx_b = vx[start:end][:, None, None]\n",
        "                vy_b = vy[start:end][:, None, None]\n",
        "                qf = (dx*dx)/np.maximum(vx_b, 1e-12) + (dy*dy)/np.maximum(vy_b, 1e-12)\n",
        "            elif cov_mode == \"full\":\n",
        "                a_b = a[start:end][:, None, None]\n",
        "                b_b = b[start:end][:, None, None]\n",
        "                c_b = c[start:end][:, None, None]\n",
        "                qf = a_b*dx*dx + 2.0*b_b*dx*dy + c_b*dy*dy\n",
        "            else:\n",
        "                # isotropic variance = 1 (if nothing provided)\n",
        "                qf = dx*dx + dy*dy\n",
        "\n",
        "            dens = np.exp(-0.5 * qf)             # (B,H,W), unnormalized per-sample kernel\n",
        "            density_sum += dens.sum(axis=0)       # (H,W)\n",
        "\n",
        "            # Weighted color sum: sum_i N_i(g)*color_i\n",
        "            col_b = colors[start:end, :]          # (B,3)\n",
        "            # dens[...,None] * col_b[:,None,None,:] -> (B,H,W,3); sum over B\n",
        "            accum_rgb += (dens[..., None] * col_b[:, None, None, :]).sum(axis=0)\n",
        "\n",
        "        # Convert to displayable RGB in [0,1]:\n",
        "        # (sum_i dens_i * color_i) is scaled by a global constant so that max intensity ~= 1.\n",
        "        # We keep hue from contributors by dividing by density_sum to get avg color,\n",
        "        # then multiply by normalized intensity for brightness: result is proportional\n",
        "        # to the unnormalized color sum up to a single global factor.\n",
        "        max_d = np.percentile(density_sum, density_clip_q)\n",
        "        if not np.isfinite(max_d) or max_d <= 0:\n",
        "            max_d = density_sum.max() + 1e-12\n",
        "        intensity = np.clip(density_sum / max_d, 0.0, 1.0)        # (H,W)\n",
        "        avg_color = accum_rgb / (density_sum[..., None] + 1e-12)  # (H,W,3)\n",
        "        img = np.clip(avg_color * intensity[..., None], 0.0, 1.0)\n",
        "\n",
        "        ax.imshow(\n",
        "            img, extent=(xlim[0], xlim[1], ylim[0], ylim[1]),\n",
        "            origin=\"lower\", interpolation=\"bilinear\",\n",
        "            alpha=density_alpha, zorder=0\n",
        "        )\n",
        "    # ---------- END density-tinted background ----------\n",
        "\n",
        "    # Scatter the means on top\n",
        "    ax.scatter(M[:,0], M[:,1], s=s, c=colors, alpha=alpha, linewidths=edge_lw, zorder=2)\n",
        "\n",
        "    if add_gaussian_rings:\n",
        "        t = np.linspace(0, 2*np.pi, 600)\n",
        "        for r in ring_levels:\n",
        "            yy = r * np.cos(t); zz = r * np.sin(t)\n",
        "            ax.plot(yy, zz, color=(1,1,1,0.9), lw=0.9, ls=\"--\", zorder=3)\n",
        "\n",
        "    ax.set_xlim(*xlim); ax.set_ylim(*ylim)\n",
        "    ax.set_aspect('equal', adjustable='box')\n",
        "    ax.set_xlabel(\"$\\\\mu_1$\", color=tickc); ax.set_ylabel(\"$\\\\mu_2$\", color=tickc)\n",
        "    ax.tick_params(colors=tickc)\n",
        "    ax.set_title(title or \"Encoder means μ(x) (colored by x)\", color=tickc, pad=8)\n",
        "\n",
        "    if out_path:\n",
        "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "        fig.savefig(out_path, dpi=200, bbox_inches=\"tight\", facecolor=fig.get_facecolor())\n",
        "    plt.close(fig)\n",
        "    return out_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def plot_encoder_latent_colored_by_x(\n",
        "    Enc, x1, eps, *,\n",
        "    out_path=None,\n",
        "    color_mode=\"hsv2d\",\n",
        "    bg=\"black\",\n",
        "    s=.5, alpha=0.75, edge_lw=0.0,\n",
        "    add_gaussian_rings=True,\n",
        "    ring_levels=(1.0, 2.0, 3.0),\n",
        "    title=None,\n",
        "    iso_extent_std: float = 3.5,\n",
        "):\n",
        "    import numpy as np, os\n",
        "    import matplotlib.pyplot as plt\n",
        "    from matplotlib import colors as mcolors\n",
        "    import matplotlib.cm as cm\n",
        "\n",
        "    mu, logv = Enc(x1)\n",
        "    z  = mu + torch.exp(0.5 * logv) * eps\n",
        "    Z  = z.detach().cpu().numpy()\n",
        "    X  = x1.detach().cpu().numpy()\n",
        "    if Z.ndim == 1: Z = Z.reshape(-1, 2)\n",
        "    if X.ndim == 1: X = X.reshape(-1, 2)\n",
        "\n",
        "    q = 0.997\n",
        "    R = float(iso_extent_std)\n",
        "    xlim = (-R, R)\n",
        "    ylim = (-R, R)\n",
        "\n",
        "    if color_mode == \"target_angle_turbo\":\n",
        "        ang = (np.arctan2(X[:,1], X[:,0]) % (2*np.pi)) / (2*np.pi)\n",
        "        colors = cm.get_cmap(\"turbo\")(ang)\n",
        "    else:\n",
        "        x1min, x1max = np.quantile(X[:,0], 1-q), np.quantile(X[:,0], q)\n",
        "        x2min, x2max = np.quantile(X[:,1], 1-q), np.quantile(X[:,1], q)\n",
        "        u = np.clip((X[:,0] - x1min) / (x1max - x1min + 1e-9), 0, 1)\n",
        "        v = np.clip((X[:,1] - x2min) / (x2max - x2min + 1e-9), 0, 1)\n",
        "        hsv = np.stack([u, 0.35 + 0.65*v, np.full_like(u, 0.95)], axis=1)\n",
        "        colors = mcolors.hsv_to_rgb(hsv)\n",
        "\n",
        "    plt.style.use(\"default\")\n",
        "    fig, ax = plt.subplots(figsize=(7.4, 8.2))\n",
        "    fig.patch.set_facecolor(bg); ax.set_facecolor(bg)\n",
        "    tickc = \"white\" if bg == \"black\" else \"black\"\n",
        "    for spine in ax.spines.values(): spine.set_color(tickc)\n",
        "\n",
        "    ax.scatter(Z[:,0], Z[:,1], s=s, c=colors, alpha=alpha, linewidths=edge_lw)\n",
        "\n",
        "    if add_gaussian_rings:\n",
        "        t = np.linspace(0, 2*np.pi, 600)\n",
        "        for r in ring_levels:\n",
        "            yy = r * np.cos(t); zz = r * np.sin(t)\n",
        "            ax.plot(yy, zz, color=(1,1,1,0.9), lw=0.9, ls=\"--\")\n",
        "\n",
        "    ax.set_xlim(*xlim); ax.set_ylim(*ylim)\n",
        "    ax.set_aspect('equal', adjustable='box')\n",
        "    ax.tick_params(colors=tickc)\n",
        "    ax.set_title(title or \"Encoder latent (colored by x)\", color=tickc, pad=8)\n",
        "\n",
        "    if out_path:\n",
        "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "        fig.savefig(out_path, dpi=200, bbox_inches=\"tight\", facecolor=fig.get_facecolor())\n",
        "    plt.close(fig)\n",
        "    return out_path\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def make_current_V_sampler(V: nn.Module, *, fuzz_eps: float = 1e-2):\n",
        "    def _sampler(n: int, nfe: int, init: str = \"gauss\", enc: nn.Module | None = None):\n",
        "        x = sample_init_points(n, init, enc=enc, eps=fuzz_eps)\n",
        "        if nfe <= 0: return x\n",
        "        dt = 1.0/float(max(nfe,1))\n",
        "        for i in range(nfe):\n",
        "            t = torch.full((n,1), (i+0.5)*dt, device=device, dtype=TDTYPE)\n",
        "            x = x + dt * V(x, t)\n",
        "        return x\n",
        "    return _sampler\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _eval_probe_models_2x2(\n",
        "    samplers: dict,             # {\"RF\": rf_sampler, \"V\": v_sampler}\n",
        "    enc, Ks, n=80_000, mmd_max_n=8192,\n",
        "    out_img=None, tag=\"RF vs V\", bins=200,\n",
        "    # viz knobs (kept for API compatibility; unused now)\n",
        "    cmap=\"magma\", gamma=0.42, vmax_percentile=99.7,\n",
        "    tgt_outer_sigma=2.6, tgt_outer_frac=0.10, tgt_linewidth=1.6\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate & plot five columns per K:\n",
        "      [Target, RF (gauss), RF (encoder), V (gauss), V (encoder)]\n",
        "    All panels share the SAME x_tgt draw per call.\n",
        "    \"\"\"\n",
        "    import os, gc, numpy as np, matplotlib.pyplot as plt, matplotlib as mpl\n",
        "    from matplotlib.colors import PowerNorm\n",
        "\n",
        "    # ---- shared target draw for fairness ----\n",
        "    x_tgt = sample_target_torch(n).to(device)\n",
        "\n",
        "    # ---- metrics table ----\n",
        "    tbl = {m: {\"gauss\": {}, \"encoder\": {}} for m in samplers.keys()}\n",
        "\n",
        "    def _run_one(model_key: str, init: str, K: int):\n",
        "        x = samplers[model_key](n, K, init=init, enc=enc)  # (n,2)\n",
        "        w2 = sliced_w2(x, x_tgt, L=128, max_n=20_000)\n",
        "        mmd= mmd_rbf_nd(x, x_tgt, max_n=mmd_max_n)\n",
        "        return float(w2), float(mmd), x.detach().cpu().numpy()\n",
        "\n",
        "    # Collect histograms for all panels (for a global brightness norm per K)\n",
        "    perK_panels = {}   # K -> [(title, H)]  (first entry will be (\"target\", Ht))\n",
        "    perK_arrays = {}   # K -> concatenated raveled H including target\n",
        "\n",
        "    # First pass: compute metrics & histograms\n",
        "    yx = x_tgt.detach().cpu().numpy()\n",
        "    q = 0.997\n",
        "\n",
        "    xlim = (np.quantile(yx[:,0], 1-q), np.quantile(yx[:,0], q))\n",
        "    xlim = [val * 1.2 for val in xlim]\n",
        "\n",
        "    ylim = (np.quantile(yx[:,1], 1-q), np.quantile(yx[:,1], q))\n",
        "    ylim = [val * 1.2 for val in ylim]\n",
        "\n",
        "    y_edges = np.linspace(xlim[0], xlim[1], bins+1)\n",
        "    z_edges = np.linspace(ylim[0], ylim[1], bins+1)\n",
        "\n",
        "    # Target histogram (shown directly in leftmost column)\n",
        "    Ht, *_ = np.histogram2d(yx[:,0], yx[:,1], bins=[y_edges, z_edges], density=True)\n",
        "\n",
        "    # Evaluate for each K and build panels\n",
        "    order = [(\"RF\",\"gauss\"), (\"RF\",\"encoder\"), (\"V\",\"gauss\"), (\"V\",\"encoder\")]\n",
        "    for K in Ks:\n",
        "        panels = [(\"target\", Ht)]     # leftmost column for every row\n",
        "        flat_vals = [Ht.ravel()]      # include target in normalization\n",
        "        for (model_key, init) in order:\n",
        "            sw2, mmd, x_np = _run_one(model_key, init, K)\n",
        "            tbl[model_key][init][K] = (sw2, mmd)\n",
        "            H, *_ = np.histogram2d(x_np[:,0], x_np[:,1], bins=[y_edges, z_edges], density=True)\n",
        "            title = f\"{model_key} ({init})\"\n",
        "            panels.append((title, H))\n",
        "            flat_vals.append(H.ravel())\n",
        "        perK_panels[K] = panels\n",
        "        perK_arrays[K] = np.concatenate(flat_vals)\n",
        "\n",
        "    # ---- print compact tables (unchanged) ----\n",
        "    hdr = \"model/init\".ljust(14) + \" | \" + \"  \".join([f\"K={K:^3d}  SW2   MMD\" for K in Ks])\n",
        "    print(\"\\n=== probe:\", tag, \"===\\n\" + hdr + \"\\n\" + \"-\"*len(hdr))\n",
        "    for (model_key, init) in order:\n",
        "        s = f\"{model_key}/{init}\".ljust(14) + \" | \"\n",
        "        for K in Ks:\n",
        "            sw2, mmd = tbl[model_key][init][K]\n",
        "            s += f\" {sw2:5.3f} {mmd:5.3f} \"\n",
        "        print(s)\n",
        "    print()\n",
        "\n",
        "    # -------------------- plotting (now 5 columns) --------------------\n",
        "    try: plt.close('all')\n",
        "    except Exception: pass\n",
        "    mpl.rcdefaults()\n",
        "\n",
        "    with plt.rc_context({\n",
        "        \"figure.facecolor\": \"black\",\n",
        "        \"axes.facecolor\":   \"black\",\n",
        "        \"savefig.facecolor\":\"black\",\n",
        "        \"axes.edgecolor\":   \"white\",\n",
        "        \"axes.labelcolor\":  \"white\",\n",
        "        \"xtick.color\":      \"white\",\n",
        "        \"ytick.color\":      \"white\",\n",
        "    }):\n",
        "        if out_img is not None:\n",
        "            os.makedirs(os.path.dirname(out_img), exist_ok=True)\n",
        "\n",
        "        rows = len(Ks); cols = 5\n",
        "        fig, axs = plt.subplots(rows, cols, figsize=(14.0, 2.9*rows), sharex=True, sharey=True)\n",
        "        if rows == 1: axs = np.array([axs])\n",
        "\n",
        "        for r, K in enumerate(Ks):\n",
        "            vmax = np.percentile(perK_arrays[K], vmax_percentile)\n",
        "            norm = PowerNorm(gamma=max(1e-3, float(gamma)), vmin=0.0, vmax=max(1e-9, vmax))\n",
        "            for c, (title, H) in enumerate(perK_panels[K]):\n",
        "                ax = axs[r, c]\n",
        "                ax.imshow(\n",
        "                    H.T, origin='lower',\n",
        "                    extent=[xlim[0], xlim[1], ylim[0], ylim[1]],\n",
        "                    cmap=cmap, norm=norm, interpolation=\"bilinear\", alpha=1.0, aspect='equal'\n",
        "                )\n",
        "                # (Contours removed)\n",
        "\n",
        "                # Titles/labels\n",
        "                if c == 0:\n",
        "                    ax.set_title(\"target\", color='w', pad=3)\n",
        "                else:\n",
        "                    ax.set_title(f\"K={K} — {title}\", color='w', pad=3)\n",
        "                if r == rows-1: ax.set_xlabel(\"x\", color='w')\n",
        "                if c == 0:      ax.set_ylabel(\"y\", color='w')\n",
        "                ax.tick_params(color='w', labelcolor='w')\n",
        "\n",
        "        fig.suptitle(tag + \" — histograms\", y=0.995, color='w')\n",
        "        fig.tight_layout()\n",
        "        if out_img is not None:\n",
        "            fig.savefig(out_img, dpi=190, bbox_inches=\"tight\", facecolor='black')\n",
        "        plt.close(fig)\n",
        "\n",
        "    gc.collect()\n",
        "    return tbl\n",
        "\n",
        "\n",
        "# --- AVRC2D: hook the probe into the training loop -----------------------------------\n",
        "class AVRC2D:\n",
        "    def __init__(self, cfg=AVRCConfig2D()):\n",
        "        self.cfg = cfg\n",
        "        D = cfg.D\n",
        "        self.Enc = EncoderX1Only(D=D, hidden=cfg.enc_hidden, depth=cfg.enc_depth).to(device)\n",
        "        self.V   = Velocity(D=D,    hidden=cfg.vel_hidden,  depth=cfg.vel_depth).to(device)\n",
        "        self.V_teacher = Velocity(D=D, hidden=cfg.vel_hidden, depth=cfg.vel_depth).to(device)\n",
        "        self.V_teacher.load_state_dict(self.V.state_dict())\n",
        "        for p in self.V_teacher.parameters(): p.requires_grad_(False)\n",
        "\n",
        "        self.opt_v   = torch.optim.Adam(self.V.parameters(),   lr=cfg.critic_lr, betas=(0.9,0.99),\n",
        "                                        weight_decay=self.cfg.critic_weight_decay)\n",
        "        self.opt_enc = torch.optim.Adam(self.Enc.parameters(), lr=cfg.enc_lr,    betas=(0.9,0.99))\n",
        "\n",
        "        self._round_ema_shadow = None\n",
        "        self.history = {'critic': [], 'organizer': [], 'sw2_k': [], 'mmd_k': [],\n",
        "                        'agg_kl': [], 'recon_mse': []}\n",
        "        self._lam_disp = cfg.lam_disp_start\n",
        "        self._lam_kl   = cfg.lam_kl_start\n",
        "        self._lam_align = (cfg.lam_align_start\n",
        "                           if cfg.lam_align_start is not None else cfg.lam_disp_start)\n",
        "\n",
        "        # persistent subset for crossings movie (already in your version)\n",
        "        self._viz_ready = False\n",
        "        self._viz_x1 = None\n",
        "        self._viz_eps = None\n",
        "                # NEW: fixed indices for lines and for latent mini-scatter\n",
        "        self._viz_keep_idx = None        # indices for chords/lines\n",
        "        self._viz_small_idx = None       # indices for latent scatter subset\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _init_crossings_subset(self):\n",
        "        if self._viz_ready: return\n",
        "        os.makedirs(self.cfg.viz_cross_dir, exist_ok=True)\n",
        "\n",
        "        # lock RNGs and sample once\n",
        "        torch.manual_seed(self.cfg.viz_seed); np.random.seed(self.cfg.viz_seed)\n",
        "        self._viz_x1  = sample_target_torch(self.cfg.viz_pairs).detach()\n",
        "        self._viz_eps = torch.randn(self.cfg.viz_pairs, self.cfg.D, device=device, dtype=TDTYPE).detach()\n",
        "\n",
        "        # --- precompute a *fixed* set of line indices (based on round-0 geometry) ---\n",
        "        # compute z0 using the cached eps (not fresh noise)\n",
        "        mu0, logv0 = self.Enc(self._viz_x1)\n",
        "        z0 = mu0 + torch.exp(0.5 * logv0) * self._viz_eps   # (N,2)\n",
        "        Xr0 = z0.detach().cpu().numpy()\n",
        "        Xt0 = self._viz_x1.detach().cpu().numpy()\n",
        "\n",
        "        N = Xr0.shape[0]\n",
        "        L = min(self.cfg.viz_subset_lines, N)\n",
        "\n",
        "        disp  = Xt0 - Xr0\n",
        "        theta = np.arctan2(disp[:,1], disp[:,0]) % (2*np.pi)\n",
        "        length= np.linalg.norm(disp, axis=1)\n",
        "\n",
        "        if L >= N:\n",
        "            keep_idx = np.arange(N)\n",
        "        elif self.cfg.viz_subset_strategy == \"angle_stratified\":\n",
        "            nb = max(8, int(np.sqrt(L)))\n",
        "            bins_theta = np.linspace(0, 2*np.pi, nb+1)\n",
        "            picks = []\n",
        "            for b in range(nb):\n",
        "                mask = (theta >= bins_theta[b]) & (theta < bins_theta[b+1])\n",
        "                cand = np.where(mask)[0]\n",
        "                if cand.size == 0: continue\n",
        "                k = max(1, int(np.ceil(L/nb)))\n",
        "                sel = cand[np.argsort(length[cand])[-k:]] if cand.size > k else cand\n",
        "                picks.append(sel)\n",
        "            keep_idx = np.unique(np.concatenate(picks))[:L]\n",
        "        elif self.cfg.viz_subset_strategy == \"longest\":\n",
        "            keep_idx = np.argsort(length)[-L:]\n",
        "        else:  # \"random\"\n",
        "            rng = np.random.default_rng(self.cfg.viz_seed ^ 0xA5A5A5)  # deterministic but separate\n",
        "            keep_idx = rng.choice(N, size=L, replace=False)\n",
        "\n",
        "        self._viz_keep_idx = np.asarray(keep_idx, dtype=int)\n",
        "\n",
        "        # --- fixed subset for the 2D latent scatter movie ---\n",
        "        M  = min(self.cfg.viz_latent_points, N)\n",
        "        rng2 = np.random.default_rng(self.cfg.viz_seed ^ 0x5A5A5A)\n",
        "        self._viz_small_idx = np.asarray(rng2.choice(N, size=M, replace=False), dtype=int)\n",
        "\n",
        "        self._viz_ready = True\n",
        "\n",
        "    # ... (all your existing methods stay the same) ...\n",
        "\n",
        "    # === NEW: helper to build pair generator for “current coupling” ===========\n",
        "    def _make_pairs_encoder_current(self):\n",
        "        @torch.no_grad()\n",
        "        def gen(n):\n",
        "            x1 = sample_target_torch(int(n))\n",
        "            mu, logv = self.Enc(x1)\n",
        "            x0 = reparam(mu, logv)\n",
        "            return x0, x1\n",
        "        return gen\n",
        "\n",
        "    # === NEW: run the RF probe (RF vs current V) ==================================\n",
        "    def _run_rf_probe(self, round_idx: int):\n",
        "        os.makedirs(self.cfg.test_rf_outdir, exist_ok=True)\n",
        "        tag = f\"RF-on-coupling@r{round_idx:05d}\"\n",
        "        print(f\"\\n[probe] Training fresh RF on current coupling … ({tag})\")\n",
        "\n",
        "        # 1) Train a fresh RF on the CURRENT coupling pairs\n",
        "        Vx, rf_sampler = train_rectified_flow_on_pairs_2d(\n",
        "            make_pairs_fn=self._make_pairs_encoder_current(),\n",
        "            steps=self.cfg.test_rf_steps,\n",
        "            batch=self.cfg.test_rf_batch,\n",
        "            lr=self.cfg.test_rf_lr,\n",
        "            clip=self.cfg.test_rf_clip,\n",
        "            hidden=self.cfg.test_rf_hidden,\n",
        "            depth=self.cfg.test_rf_depth,\n",
        "            log_every=self.cfg.test_rf_log_every,\n",
        "            seed=self.cfg.test_rf_seed,\n",
        "        )\n",
        "\n",
        "        # 2) Build a sampler for the CURRENT velocity field V with the SAME interface\n",
        "        v_sampler = make_current_V_sampler(self.V)\n",
        "\n",
        "        # 3) Evaluate & visualize both samplers, for both inits\n",
        "        img = os.path.join(self.cfg.test_rf_outdir, f\"rf_probe_{round_idx:05d}.png\")\n",
        "        tbl = _eval_probe_models_2x2(\n",
        "            samplers={\"RF\": rf_sampler, \"V\": v_sampler},\n",
        "            enc=self.Enc,\n",
        "            Ks=self.cfg.test_rf_Ks,\n",
        "            n=self.cfg.test_rf_n,\n",
        "            mmd_max_n=self.cfg.test_rf_mmd_max_n,\n",
        "            out_img=img,\n",
        "            tag=tag,\n",
        "            bins=self.cfg.test_rf_bins\n",
        "        )\n",
        "\n",
        "        # 4) Save numbers\n",
        "        txt = os.path.join(self.cfg.test_rf_outdir, f\"rf_probe_{round_idx:05d}.txt\")\n",
        "        with open(txt, \"w\") as f:\n",
        "            f.write(f\"{tag}\\n\")\n",
        "            for model in (\"RF\",\"V\"):\n",
        "                for init in (\"gauss\",\"encoder\"):\n",
        "                    for K in self.cfg.test_rf_Ks:\n",
        "                        sw2, mmd = tbl[model][init][K]\n",
        "                        f.write(f\"{model},{init},K={K},SW2={sw2:.6f},MMD={mmd:.6f}\\n\")\n",
        "        print(f\"[probe] saved: {img}  and {txt}\")\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _save_crossings_frame(self, round_idx: int):\n",
        "        \"\"\"Render & save a 3D crossings frame using fixed x1, ε, and fixed line indices.\"\"\"\n",
        "        if not self._viz_ready:\n",
        "            self._init_crossings_subset()\n",
        "\n",
        "        # current encoder stats on the fixed x1/eps\n",
        "        mu, logv = self.Enc(self._viz_x1)\n",
        "        z0 = mu + torch.exp(0.5 * logv) * self._viz_eps\n",
        "\n",
        "        elev, azim = self.cfg.viz_camera\n",
        "        out = os.path.join(self.cfg.viz_cross_dir, f\"cross_{round_idx:05d}.png\")\n",
        "        plot_crossings_hist_and_chords_2d(\n",
        "            pairs=(z0, self._viz_x1),\n",
        "            plane_mode=self.cfg.viz_plane_mode,\n",
        "            bins=self.cfg.viz_bins,\n",
        "            subset_lines=len(self._viz_keep_idx),\n",
        "            subset_strategy=self.cfg.viz_subset_strategy,  # ignored because we pass line_indices\n",
        "            line_indices=self._viz_keep_idx,               # <-- fixed lines every round\n",
        "            density_gamma=self.cfg.viz_density_gamma,\n",
        "            cmap_ref=self.cfg.viz_cmap_ref,\n",
        "            cmap_tgt=self.cfg.viz_cmap_tgt,\n",
        "            view_elev=elev, view_azim=azim,\n",
        "            title=f\"AVRC coupling — round {round_idx}\",\n",
        "            save_path=out,\n",
        "            show=False\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _save_latent_scatter_frame(self, round_idx: int):\n",
        "        if not self._viz_ready:\n",
        "            self._init_crossings_subset()\n",
        "        # use the cached fixed subset indices\n",
        "        idx = torch.as_tensor(self._viz_small_idx, device=device, dtype=torch.long)  # <— ensure long\n",
        "        x1_small  = self._viz_x1[idx]\n",
        "        eps_small = self._viz_eps[idx]\n",
        "\n",
        "        out = os.path.join(self.cfg.viz_cross_dir, f\"latent_{round_idx:05d}.png\")\n",
        "        plot_encoder_latent_colored_by_x(\n",
        "            self.Enc, x1_small, eps_small,\n",
        "            out_path=out,\n",
        "            color_mode=self.cfg.viz_latent_color_mode,\n",
        "            bg=self.cfg.viz_latent_bg,\n",
        "            s=self.cfg.viz_latent_s,\n",
        "            alpha=self.cfg.viz_latent_alpha,\n",
        "            add_gaussian_rings=self.cfg.viz_latent_rings,\n",
        "            ring_levels=self.cfg.viz_latent_ring_levels,\n",
        "            title=f\"Encoder latent (colored by x) — round {round_idx}\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _save_latent_mu_frame(self, *, round_idx: int):\n",
        "        if not self._viz_ready:\n",
        "            self._init_crossings_subset()\n",
        "        out_dir = self.cfg.viz_cross_dir\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        out_path = os.path.join(out_dir, f\"latent_mu_{round_idx:05d}.png\")\n",
        "        return plot_encoder_means_colored_by_x(\n",
        "            Enc=self.Enc,\n",
        "            x1=self._viz_x1,         # fixed\n",
        "            eps=self._viz_eps,       # unused; kept for API parity\n",
        "            out_path=out_path,\n",
        "            color_mode=self.cfg.viz_latent_color_mode,\n",
        "            bg=self.cfg.viz_latent_bg,\n",
        "            s=self.cfg.viz_latent_s,\n",
        "            alpha=self.cfg.viz_latent_alpha,\n",
        "            add_gaussian_rings=self.cfg.viz_latent_rings,\n",
        "            ring_levels=self.cfg.viz_latent_ring_levels,\n",
        "            iso_extent_std=3.2,\n",
        "            title=f\"Encoder means μ(x) — r={round_idx:05d}\",\n",
        "            add_color_density = False\n",
        "        )\n",
        "\n",
        "\n",
        "    # ------------------------------ helpers -----------------------------------\n",
        "    def _reset_critic_optimizer(self):\n",
        "        self.opt_v = torch.optim.Adam(self.V.parameters(), lr=self.cfg.critic_lr, betas=(0.9,0.99),\n",
        "                                      weight_decay=self.cfg.critic_weight_decay)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _start_intra_ema(self):\n",
        "        if self.cfg.teacher_mode == \"intra\":\n",
        "            self._round_ema_shadow = [p.detach().clone() for p in self.V.parameters()]\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _accumulate_intra_ema(self):\n",
        "        if self.cfg.teacher_mode != \"intra\" or self._round_ema_shadow is None: return\n",
        "        d = self.cfg.intra_ema_decay\n",
        "        for s, p in zip(self._round_ema_shadow, self.V.parameters()):\n",
        "            s.mul_(d).add_(p.detach(), alpha=(1.0 - d))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _commit_intra_ema_to_teacher(self):\n",
        "        if self.cfg.teacher_mode == \"intra\":\n",
        "            if self._round_ema_shadow is None:\n",
        "                self.V_teacher.load_state_dict(self.V.state_dict())\n",
        "            else:\n",
        "                for pt, s in zip(self.V_teacher.parameters(), self._round_ema_shadow):\n",
        "                    pt.copy_(s)\n",
        "            self._round_ema_shadow = None\n",
        "        elif self.cfg.teacher_mode == \"hard\":\n",
        "            self.V_teacher.load_state_dict(self.V.state_dict())\n",
        "        elif self.cfg.teacher_mode == \"ema\":\n",
        "            for pt, ps in zip(self.V_teacher.parameters(), self.V.parameters()):\n",
        "                pt.data.mul_(self.cfg.ema_decay).add_(ps.data, alpha=(1.0 - self.cfg.ema_decay))\n",
        "\n",
        "    # ------------------------------ losses ------------------------------------\n",
        "    def _critic_loss(self, z0, z1, t, w):\n",
        "        zt  = (1.0 - t)*z0 + t*z1               # (B,D)\n",
        "        ell = (z1 - z0).detach()                # (B,D)\n",
        "        pred= self.V(zt, t)                     # (B,D)\n",
        "        r   = pred - ell\n",
        "        # Huber on vector residual (use L2 inside / L1 outside, per-sample)\n",
        "        r2  = (r**2).sum(dim=1, keepdim=True)\n",
        "        a   = r.abs().sum(dim=1, keepdim=True)\n",
        "        quad= 0.5*r2\n",
        "        lin = self.cfg.critic_huber_delta*(a - 0.5*self.cfg.critic_huber_delta)\n",
        "        hub = torch.where(a <= self.cfg.critic_huber_delta, quad, lin)\n",
        "        return (w * hub).mean()\n",
        "\n",
        "    def critic_step(self, B=None):\n",
        "        B = B or self.cfg.batch\n",
        "        with torch.no_grad():\n",
        "            xt1 = sample_target_torch(B)\n",
        "            mu0, logv0 = self.Enc(xt1)\n",
        "            z0 = reparam(mu0, logv0)   # start at encoder agg\n",
        "            z1 = xt1                   # end is data\n",
        "        #t = torch.distributions.Beta(self.cfg.critic_t_alpha, self.cfg.critic_t_beta).sample((B,1)).to(device=device, dtype=TDTYPE)\n",
        "        #w = ((1.0 - t) ** self.cfg.critic_t_gamma).detach(); w = w / (w.mean() + 1e-8)\n",
        "        if getattr(self.cfg, \"critic_match_rf_midpoints\", False):\n",
        "            K = self.cfg.critic_match_rf_K or self.cfg.recon_k\n",
        "            idx = torch.randint(0, K, (B,), device=device)\n",
        "            t = ((idx + 0.5) / float(K)).view(B,1).type_as(z0)  # exactly RF midpoints\n",
        "            w = torch.ones_like(t)                               # no extra weights\n",
        "        else:\n",
        "            t = torch.distributions.Beta(self.cfg.critic_t_alpha, self.cfg.critic_t_beta)\\\n",
        "                .sample((B,1)).to(device=device, dtype=TDTYPE)\n",
        "            w = ((1.0 - t) ** self.cfg.critic_t_gamma); w = w / (w.mean() + 1e-8)\n",
        "        loss = self._critic_loss(z0, z1, t, w)\n",
        "        self.opt_v.zero_grad(set_to_none=True); loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.V.parameters(), self.cfg.critic_clip)\n",
        "        self.opt_v.step()\n",
        "        self._accumulate_intra_ema()\n",
        "        return float(loss.item())\n",
        "\n",
        "    def organizer_step(self, B=None):\n",
        "        B = B or self.cfg.batch\n",
        "        xt1 = sample_target_torch(B)\n",
        "        mu0_enc, logv0 = self.Enc(xt1)     # (B,D)\n",
        "        z0 = reparam(mu0_enc, logv0)       # start\n",
        "        z1 = xt1                            # end\n",
        "\n",
        "        # K midpoints\n",
        "        K = self.cfg.recon_k\n",
        "        reps = (B + K - 1)//K\n",
        "        t_mid = (torch.arange(K, device=device, dtype=TDTYPE) + 0.5)/float(K)\n",
        "        t = t_mid.repeat_interleave(reps)[:B].view(B,1)\n",
        "        w = torch.full_like(t, 1.0/(K*K))\n",
        "\n",
        "        zt = (1.0 - t)*z0 + t*z1\n",
        "        ell= (z1 - z0)\n",
        "\n",
        "        if self.cfg.unbiased_dispersion:\n",
        "            mu_t = self.V_teacher(zt, t).detach()\n",
        "            t0   = torch.zeros_like(t)\n",
        "            mu_0 = self.V_teacher(z0, t0).detach()  # <-- detach mu_0\n",
        "\n",
        "            a = ell - mu_t                  # (v_t - ℓ) with a sign flip\n",
        "            b = mu_t - mu_0                 # (v_t - v_0), fully detached\n",
        "\n",
        "            a2     = (a * a).sum(dim=1, keepdim=True)                 # ||v_t - ℓ||^2  (dispersion)\n",
        "            b2_log = ((b * b).sum(dim=1, keepdim=True)).detach()      # for logging only (no grads)\n",
        "\n",
        "            # unbiased proxy for ⟨b, v_t - ℓ⟩; grads flow only via ell (i.e., z0)\n",
        "            cross  = (b * (ell - mu_0)).sum(dim=1, keepdim=True)\n",
        "\n",
        "            denom       = (w.mean() + 1e-12)\n",
        "            disp_piece  = (w * a2).mean() / denom\n",
        "            align_piece = (w * (2.0 * cross - b2_log)).mean() / denom\n",
        "        else:\n",
        "            # biased variant has only the dispersion piece available\n",
        "            with torch.no_grad():\n",
        "                v_targ = self.V_teacher(zt, t)\n",
        "            diff2 = (v_targ - ell).pow(2).sum(dim=1, keepdim=True)\n",
        "            disp_piece  = (w * diff2).mean() / (w.mean() + 1e-12)\n",
        "            align_piece = torch.tensor(0.0, device=zt.device, dtype=zt.dtype)\n",
        "\n",
        "        loss_kl = kl_normal_diag(mu0_enc, logv0)\n",
        "\n",
        "        # NEW: separate weights\n",
        "        loss = (self._lam_disp  * disp_piece\n",
        "              + self._lam_align * align_piece\n",
        "              + self._lam_kl    * loss_kl)\n",
        "\n",
        "        self.opt_enc.zero_grad(set_to_none=True); loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.Enc.parameters(), self.cfg.ed_clip)\n",
        "        self.opt_enc.step()\n",
        "        self._reset_critic_optimizer()\n",
        "\n",
        "        return {'disp': float(disp_piece.item()),\n",
        "                'align': float(align_piece.item()),\n",
        "                'kl': float(loss_kl.item()),\n",
        "                'total': float(loss.item())}\n",
        "\n",
        "    # ------------------------------ integration & eval -------------------------\n",
        "    @torch.no_grad()\n",
        "    def _integrate_steps(self, z0: torch.Tensor, k: int) -> torch.Tensor:\n",
        "        if k <= 0: return z0.clone()\n",
        "        z = z0.clone()\n",
        "        dt = 1.0 / float(k)\n",
        "        n = z.size(0)\n",
        "        for i in range(k):\n",
        "            t = torch.full((n,1), (i+0.5)*dt, device=device, dtype=TDTYPE)\n",
        "            z = z + dt * self.V(z, t)\n",
        "        return z\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _eval_divs_k(self, k: int, n: int, trials: int = 1, init: str = \"gauss\", mmd_max_n: int = 8192):\n",
        "        def _sample_z0(nn: int) -> torch.Tensor:\n",
        "            if init == \"gauss\":\n",
        "                return torch.randn(nn, self.cfg.D, device=device, dtype=TDTYPE)\n",
        "            elif init == \"encoder\":\n",
        "                x1 = sample_target_torch(nn)\n",
        "                mu, logv = self.Enc(x1)\n",
        "                return reparam(mu, logv)\n",
        "            else:\n",
        "                raise ValueError(\"init must be 'gauss' or 'encoder'.\")\n",
        "        trials = max(1, int(trials))\n",
        "        mmd_vals, sw2_val = [], None\n",
        "        for _ in range(trials):\n",
        "            z0 = _sample_z0(n)\n",
        "            zK = self._integrate_steps(z0, k)\n",
        "            x_tgt = sample_target_torch(n)\n",
        "            if sw2_val is None:\n",
        "                sw2_val = sliced_w2(zK, x_tgt, L=128, max_n=20000)\n",
        "            mmd_vals.append(mmd_rbf_nd(zK, x_tgt, max_n=mmd_max_n))\n",
        "        return sw2_val, float(np.mean(mmd_vals))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _estimate_aggregate_kl(self, N: int):\n",
        "        x1 = sample_target_torch(N)\n",
        "        mu, logv = self.Enc(x1)\n",
        "        z = mu + torch.exp(0.5*logv) * torch.randn_like(mu)\n",
        "        m = z.mean(dim=0, keepdim=True)               # (1,D)\n",
        "        C = (z - m).T @ (z - m) / float(N) + 1e-6*torch.eye(self.cfg.D, device=device, dtype=TDTYPE)\n",
        "        trC = torch.trace(C)\n",
        "        mm  = (m @ m.T).squeeze()\n",
        "        logdetC = torch.logdet(C)\n",
        "        kl = 0.5*(trC + mm - self.cfg.D - logdetC)\n",
        "        return float(kl.detach().cpu()), float(m.squeeze().norm().detach().cpu()), float(trC.detach().cpu()/self.cfg.D)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _flow_reconstruction_mse(self, K: int, N: int):\n",
        "        x1 = sample_target_torch(N)\n",
        "        mu, _ = self.Enc(x1)\n",
        "        zK = self._integrate_steps(mu, K)\n",
        "        mse = F.mse_loss(zK, x1)\n",
        "        return float(mse.detach().cpu())\n",
        "\n",
        "\n",
        "    def train(self, rounds=None, progress=True, seed=None):\n",
        "        seed_everything(seed)\n",
        "        rounds = rounds or self.cfg.rounds\n",
        "        # place this near the top of train(), after: rounds = rounds or self.cfg.rounds\n",
        "        anneal_T = rounds - self.cfg.post_anneal_rounds  # steps that actually anneal\n",
        "\n",
        "        def schedule(start, end, r):\n",
        "            \"\"\"Linear from r=1..anneal_T, then held constant at 'end' afterwards.\n",
        "              If anneal_T <= 0, no anneal at all: constant 'end' for all rounds.\"\"\"\n",
        "            if anneal_T <= 0:\n",
        "                return end\n",
        "            if r <= anneal_T:\n",
        "                return lin_sched(start, end, r, anneal_T)\n",
        "            return end\n",
        "\n",
        "        self.pretrain_velocity(progress=progress)\n",
        "\n",
        "        # lock subset and save initial frames (round 0)\n",
        "        self._init_crossings_subset()\n",
        "        try:\n",
        "            self._save_crossings_frame(round_idx=0)\n",
        "            if self.cfg.viz_latent:\n",
        "                self._save_latent_scatter_frame(round_idx=0)\n",
        "                self._save_latent_mu_frame(round_idx=0)           # <-- NEW\n",
        "        except Exception as e:\n",
        "            print(f\"[warn] viz @0 failed: {e}\")\n",
        "\n",
        "        if self.cfg.test_rf_every and self.cfg.test_rf_every > 0:\n",
        "            self._run_rf_probe(round_idx=0)\n",
        "\n",
        "        t0 = time.time()\n",
        "        for r in range(1, rounds+1):\n",
        "\n",
        "            #constants for losses\n",
        "            self._lam_disp = schedule(self.cfg.lam_disp_start, self.cfg.lam_disp_end, r)\n",
        "            self._lam_kl   = schedule(self.cfg.lam_kl_start,   self.cfg.lam_kl_end,   r)\n",
        "\n",
        "            align_start = (self.cfg.lam_align_start\n",
        "                      if self.cfg.lam_align_start is not None else self.cfg.lam_disp_start)\n",
        "            align_end   = (self.cfg.lam_align_end\n",
        "                      if self.cfg.lam_align_end   is not None else self.cfg.lam_disp_end)\n",
        "            self._lam_align = schedule(align_start, align_end, r)\n",
        "\n",
        "            c_loss, c_steps = self._critic_round_adaptive()\n",
        "            self.history['critic'].append(c_loss)\n",
        "\n",
        "            stats = self.organizer_step()\n",
        "            self.history['organizer'].append(stats)\n",
        "\n",
        "            # periodic viz frames (same cadence, same folder)\n",
        "            if (self.cfg.k_plot is not None) and (self.cfg.k_plot > 0) and (r % self.cfg.k_plot == 0):\n",
        "                try:\n",
        "                    self._save_crossings_frame(round_idx=r)\n",
        "                    if self.cfg.viz_latent:\n",
        "                        self._save_latent_scatter_frame(round_idx=r)\n",
        "                        self._save_latent_mu_frame(round_idx=r)   # <-- NEW\n",
        "                except Exception as e:\n",
        "                    print(f\"[warn] crossings/latent save failed at r={r}: {e}\")\n",
        "\n",
        "            # periodic RF probe (unchanged)\n",
        "            if (self.cfg.test_rf_every is not None) and (self.cfg.test_rf_every > 0) and (r % self.cfg.test_rf_every == 0):\n",
        "                try:\n",
        "                    self._run_rf_probe(round_idx=r)\n",
        "                except Exception as e:\n",
        "                    print(f\"[warn] RF probe failed at r={r}: {e}\")\n",
        "\n",
        "            # ... logging block unchanged ...\n",
        "\n",
        "            if (r % self.cfg.log_every) == 0 and progress:\n",
        "                k_eval      = self.cfg.log_k\n",
        "                trials      = self.cfg.log_trials\n",
        "                n_eval      = self.cfg.log_n\n",
        "                mmd_max_n   = self.cfg.log_mmd_max_n\n",
        "\n",
        "                sw2_g,  mmd_g    = self._eval_divs_k(k=k_eval, n=n_eval, trials=trials, init=\"gauss\",   mmd_max_n=mmd_max_n)\n",
        "                sw2_e,  mmd_e    = self._eval_divs_k(k=k_eval, n=n_eval, trials=trials, init=\"encoder\", mmd_max_n=mmd_max_n)\n",
        "\n",
        "                self.history['sw2_k'].append({'round': r, 'gauss': sw2_g, 'encoder': sw2_e})\n",
        "                self.history['mmd_k'].append({'round': r, 'gauss': mmd_g, 'encoder': mmd_e})\n",
        "\n",
        "                kl_agg, agg_mu_norm, avg_var = self._estimate_aggregate_kl(self.cfg.agg_kl_batch)\n",
        "                recon_mse = self._flow_reconstruction_mse(self.cfg.recon_k, self.cfg.recon_n)\n",
        "                self.history['agg_kl'].append({'round': r, 'kl_agg': kl_agg, 'mu_norm': agg_mu_norm, 'avg_var': avg_var})\n",
        "                self.history['recon_mse'].append({'round': r, 'mse': recon_mse, 'K': self.cfg.recon_k})\n",
        "\n",
        "                print(\n",
        "                    f\"[{r:05d}] critic {c_loss:.4f} (steps={c_steps}) | \"\n",
        "                    f\"disp {stats['disp']:.4f} align {stats['align']:.4f} kl {stats['kl']:.4f} | \"\n",
        "                    f\"lam_disp {self._lam_disp:.2f} lam_align {self._lam_align:.2f} lam_kl {self._lam_kl:.2f} | \"\n",
        "                    f\"SW2@k={k_eval} N→*: {sw2_g:.4f}  MMD: {mmd_g:.4f} | \"\n",
        "                    f\"SW2@k={k_eval} E→*: {sw2_e:.4f}  MMD: {mmd_e:.4f} | \"\n",
        "                    f\"AGG-KL≈{kl_agg:.4f} (||μ||≈{agg_mu_norm:.3f}, avg var≈{avg_var:.3f}) | \"\n",
        "                    f\"FlowRecon@K={self.cfg.recon_k} MSE≈{recon_mse:.5f}\"\n",
        "                )\n",
        "                t0 = time.time()\n",
        "\n",
        "\n",
        "    # ------------------------------ critic rounds ------------------------------\n",
        "    def _critic_round_adaptive(self):\n",
        "        losses = []\n",
        "        self._start_intra_ema()\n",
        "        steps_used = 0\n",
        "        for k in range(self.cfg.critic_adapt_max):\n",
        "            steps_used += 1\n",
        "            losses.append(self.critic_step())\n",
        "            if k+1 >= self.cfg.critic_adapt_min:\n",
        "                win = self.cfg.critic_adapt_min\n",
        "                recent = np.mean(losses[-win:])\n",
        "                early  = np.mean(losses[:win])\n",
        "                if recent <= self.cfg.critic_tol * max(early, 1e-8): break\n",
        "        self._commit_intra_ema_to_teacher()\n",
        "        avg_loss = float(np.mean(losses[-self.cfg.critic_adapt_min:]))\n",
        "        return avg_loss, steps_used\n",
        "\n",
        "    # ------------------------------ pretrain -----------------------------------\n",
        "    def pretrain_velocity(self, steps=None, progress=True):\n",
        "        steps = self.cfg.pretrain_steps if steps is None else steps\n",
        "        if steps <= 0:\n",
        "            self.V_teacher.load_state_dict(self.V.state_dict()); return\n",
        "        self._start_intra_ema()\n",
        "        t0 = time.time()\n",
        "        for it in range(1, steps+1):\n",
        "            B = self.cfg.batch\n",
        "            z0 = sample_source_torch(B, D=self.cfg.D)\n",
        "            z1 = sample_target_torch(B)\n",
        "            t  = torch.rand(B,1, device=device, dtype=TDTYPE)\n",
        "            w  = torch.ones_like(t)\n",
        "            loss = self._critic_loss(z0, z1, t, w)\n",
        "            self.opt_v.zero_grad(set_to_none=True); loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.V.parameters(), self.cfg.critic_clip)\n",
        "            self.opt_v.step()\n",
        "            self._accumulate_intra_ema()\n",
        "            if progress and (it % 200 == 0):\n",
        "                print(f\"[pretrain v (indep)] {it}/{steps}  loss={float(loss):.4f}  ({time.time()-t0:.1f}s)\")\n",
        "                t0 = time.time()\n",
        "        self._commit_intra_ema_to_teacher()\n",
        "\n",
        "    # ------------------------------- samplers ----------------------------------\n",
        "    @torch.no_grad()\n",
        "    def sample(self, n: int, nfe: int = 8):\n",
        "        z = torch.randn(n, self.cfg.D, device=device, dtype=TDTYPE)\n",
        "        dt = 1.0 / float(max(nfe,1))\n",
        "        for k in range(nfe):\n",
        "            t = torch.full((n,1), (k+0.5)*dt, device=device, dtype=TDTYPE)\n",
        "            z = z + dt * self.V(z, t)\n",
        "        return z\n",
        "\n",
        "# -------------------------- AVRC oracle sampler (q(z) init) ---------------------------\n",
        "@torch.no_grad()\n",
        "def avrc_sample_torch_2d(model_or_sampler, n: int, nfe: int = 8) -> torch.Tensor:\n",
        "    if hasattr(model_or_sampler, \"V\"):\n",
        "        return model_or_sampler.sample(n, nfe)\n",
        "    if callable(model_or_sampler):\n",
        "        out = model_or_sampler(n, nfe)\n",
        "        if not isinstance(out, torch.Tensor):\n",
        "            raise TypeError(\"Sampler must return a torch.Tensor.\")\n",
        "        return out\n",
        "    raise TypeError(\"Expected AVRC2D or callable (n,nfe)->Tensor.\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def avrc_oracle_sampler_torch_2d(model: AVRC2D):\n",
        "    def sampler(n: int, nfe: int):\n",
        "        x1 = sample_target_torch(n)\n",
        "        mu, logv = model.Enc(x1)\n",
        "        z = reparam(mu, logv)\n",
        "        if nfe <= 0: return z\n",
        "        dt = 1.0/float(max(nfe,1))\n",
        "        for k in range(nfe):\n",
        "            t = torch.full((n,1), (k+0.5)*dt, device=device, dtype=TDTYPE)\n",
        "            z = z + dt * model.V(z, t)\n",
        "        return z\n",
        "    return sampler\n",
        "\n",
        "@torch.no_grad()\n",
        "def batch_dispersion_metrics(pred: torch.Tensor, ell: torch.Tensor, t: torch.Tensor | None = None):\n",
        "    eps = 1e-8\n",
        "    resid = (pred - ell)\n",
        "    mse  = torch.mean((resid**2).sum(dim=1))\n",
        "    nmse = mse / (torch.mean((ell**2).sum(dim=1)) + eps)\n",
        "    return {\"mse\": float(mse.detach().cpu()), \"nmse\": float(nmse.detach().cpu())}\n",
        "\n",
        "def train_rectified_flow_2d(steps=10000, batch=2048, lr=1e-3, clip=1.0, log_every=200,\n",
        "                            hidden=128, depth=4, small_t_gamma: float | None = None):\n",
        "    \"\"\"\n",
        "    Standard RF: x0~N(0,I), x1~p*, t~U(0,1), x_t=(1-t)x0+t x1, target ℓ=x1-x0.\n",
        "    \"\"\"\n",
        "    Vx = VelocityX2D(hidden=hidden, depth=depth).to(device)\n",
        "    opt = torch.optim.Adam(Vx.parameters(), lr=lr, betas=(0.9,0.99))\n",
        "    for it in range(1, steps+1):\n",
        "        x0, x1, _ = make_pairs_random(batch)\n",
        "        t  = torch.rand(batch,1, device=device, dtype=TDTYPE)\n",
        "        xt = (1.0 - t)*x0 + t*x1\n",
        "        ell= (x1 - x0).detach()\n",
        "        pred = Vx(xt, t)\n",
        "        loss = F.mse_loss(pred, ell)\n",
        "        opt.zero_grad(set_to_none=True); loss.backward()\n",
        "        nn.utils.clip_grad_norm_(Vx.parameters(), clip); opt.step()\n",
        "\n",
        "        if (it % log_every) == 0:\n",
        "            disp = batch_dispersion_metrics(pred, ell, t=t)\n",
        "            extra = \"\"\n",
        "            if small_t_gamma is not None:\n",
        "                w = (1.0 - t).pow(small_t_gamma)\n",
        "                wn = (w * (pred-ell).pow(2).sum(dim=1)).mean() / ((w * ell.pow(2).sum(dim=1)).mean() + 1e-8)\n",
        "                extra = f\"  wNMSE((1-t)^{small_t_gamma})={float(wn):.4f}\"\n",
        "            print(f\"[RF] step {it}/{steps}  loss={float(loss):.4f}  NMSE={disp['nmse']:.4f}{extra}\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def rf_sampler(n: int, nfe: int):\n",
        "        x = torch.randn(n, 2, device=device, dtype=TDTYPE)\n",
        "        dt = 1.0/float(max(nfe,1))\n",
        "        for i in range(nfe):\n",
        "            t = torch.full((n,1), (i+0.5)*dt, device=device, dtype=TDTYPE)\n",
        "            x = x + dt * Vx(x, t)\n",
        "        return x\n",
        "    return Vx, rf_sampler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Tr7YQVJnFyFP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0737B-l8-q04"
      },
      "outputs": [],
      "source": [
        "! rm -rf viz_crossings3d\n",
        "! rm -rf rf_snapshots\n",
        "\n",
        "! mkdir viz_crossings3d\n",
        "! mkdir viz_rf_snapshots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyouPeiMRSRL"
      },
      "outputs": [],
      "source": [
        "from logging import debug\n",
        "# =============================== Main + 2D Viz / Bench ================================\n",
        "# Added detailed LOGGING and TIMING to diagnose stalls around benchmark_samplers_2d.\n",
        "#\n",
        "# What’s new:\n",
        "#   • benchmark_samplers_2d now logs per-sampler/K timing (sampling, SW2, MMD), throughput,\n",
        "#     and (if CUDA) memory stats; also supports chunked sampling to avoid spikes.\n",
        "#   • _plot_heat_grid logs timing; KDE fitting uses capped subsampling (kde_max_n) and\n",
        "#     contour grid is computed once and reused.\n",
        "#   • Crossings plot unchanged except for optional logging.\n",
        "#\n",
        "# If you still see a stall, likely culprits are:\n",
        "#   1) very large n with big Ks (lots of forward passes), or\n",
        "#   2) KDE fit on huge x_tgt, or\n",
        "#   3) SW2 on all n (now capped via sw2_max_n).\n",
        "#\n",
        "# You can lower n, use smaller Ks, or raise chunk_n to spread work.\n",
        "\n",
        "import os, sys, math, time, numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from time import perf_counter as _tic\n",
        "from sklearn.neighbors import KernelDensity\n",
        "\n",
        "# ---------- sync helpers ----------\n",
        "def _sync():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "def _gpu_mem():\n",
        "    if not torch.cuda.is_available():\n",
        "        return \"(CPU)\"\n",
        "    alloc = torch.cuda.memory_allocated() / 1e9\n",
        "    reserv= torch.cuda.memory_reserved() / 1e9\n",
        "    return f\"(GPU mem: alloc={alloc:.2f} GB, reserved={reserv:.2f} GB)\"\n",
        "\n",
        "# ------------- Shortcuts to samplers/metrics from the training cell -------------------\n",
        "@torch.no_grad()\n",
        "def avrc_sample_torch(model_or_sampler, n: int, nfe: int = 8) -> torch.Tensor:\n",
        "    return avrc_sample_torch_2d(model_or_sampler, n=n, nfe=nfe)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model_divergences_2d(model_or_sampler, n=200_000, nfe=8, mmd_max_n=8192, L=128, sw2_max_n=20000):\n",
        "    x_model = avrc_sample_torch(model_or_sampler, n, nfe)\n",
        "    x_tgt   = sample_target_torch(n)\n",
        "    sw2 = sliced_w2(x_model, x_tgt, L=L, max_n=sw2_max_n)\n",
        "    mmd = mmd_rbf_nd(x_model, x_tgt, max_n=mmd_max_n)\n",
        "    print(f\"[Divergences 2D]  SW2≈{sw2:.4f}   MMD≈{mmd:.4f}  (n={n}, K={nfe}, L={L})\")\n",
        "    return sw2, mmd\n",
        "\n",
        "# ----------------------------- KDE (2D) utilities ------------------------------------\n",
        "def _fit_kde_2d(X_np: np.ndarray, bw: float | None = None):\n",
        "    n, d = X_np.shape\n",
        "    if bw is None:\n",
        "        std = X_np.std(axis=0, ddof=1) + 1e-8\n",
        "        bw  = float((n ** (-1.0/(d+4))) * np.mean(std))\n",
        "        bw  = max(bw, 1e-3)\n",
        "    kde = KernelDensity(bandwidth=bw, kernel='gaussian'); kde.fit(X_np)\n",
        "    return kde, bw\n",
        "\n",
        "def _kde_grid_eval(kde: KernelDensity, xlim, ylim, gridsize=200):\n",
        "    xs = np.linspace(xlim[0], xlim[1], gridsize)\n",
        "    ys = np.linspace(ylim[0], ylim[1], gridsize)\n",
        "    Xg, Yg = np.meshgrid(xs, ys, indexing='xy')\n",
        "    pts = np.stack([Xg.ravel(), Yg.ravel()], axis=1)\n",
        "    logp = kde.score_samples(pts).reshape(gridsize, gridsize)\n",
        "    return Xg, Yg, logp\n",
        "\n",
        "# ----------------------------- Heatmaps / contours (timed) ----------------------------\n",
        "@torch.no_grad()\n",
        "def plot_output_heatmaps_2d(\n",
        "    model_or_sampler, n=200_000, nfe=8, bins=180, target_n=None,\n",
        "    xlim=None, ylim=None, cmap=\"magma\", title=\"Model vs Target\",\n",
        "    gamma: float = 0.42, vmax_percentile: float = 99.7\n",
        "):\n",
        "    import numpy as np, matplotlib.pyplot as plt, matplotlib as mpl\n",
        "    from matplotlib.colors import PowerNorm\n",
        "\n",
        "    t0 = _tic()\n",
        "    x_model = avrc_sample_torch(model_or_sampler, n=n, nfe=nfe).detach().cpu().numpy()\n",
        "    _sync(); t1 = _tic()\n",
        "    x_tgt   = sample_target_torch(n if target_n is None else target_n).detach().cpu().numpy()\n",
        "    _sync(); t2 = _tic()\n",
        "\n",
        "    # Robust bounds from target (match the probe style)\n",
        "    if xlim is None or ylim is None:\n",
        "        q = 0.997\n",
        "        xlim = xlim or (np.quantile(x_tgt[:,0], 1-q), np.quantile(x_tgt[:,0], q))\n",
        "        xlim = [val * 1.2 for val in xlim]\n",
        "        ylim = ylim or (np.quantile(x_tgt[:,1], 1-q), np.quantile(x_tgt[:,1], q))\n",
        "        ylim = [val * 1.2 for val in ylim]\n",
        "\n",
        "    # Shared bins/edges for consistent imshow extents\n",
        "    y_edges = np.linspace(xlim[0], xlim[1], bins+1)\n",
        "    z_edges = np.linspace(ylim[0], ylim[1], bins+1)\n",
        "\n",
        "    # Histograms\n",
        "    Ht, *_ = np.histogram2d(x_tgt[:,0],   x_tgt[:,1],   bins=[y_edges, z_edges], density=True)\n",
        "    Hm, *_ = np.histogram2d(x_model[:,0], x_model[:,1], bins=[y_edges, z_edges], density=True)\n",
        "\n",
        "    # Global brightness norm across both panels\n",
        "    all_vals = np.concatenate([Ht.ravel(), Hm.ravel()])\n",
        "    vmax = np.percentile(all_vals, vmax_percentile)\n",
        "    norm = PowerNorm(gamma=max(1e-3, float(gamma)), vmin=0.0, vmax=max(1e-9, vmax))\n",
        "\n",
        "    # Style to match the probe figures\n",
        "    try: plt.close('all')\n",
        "    except Exception: pass\n",
        "    mpl.rcdefaults()\n",
        "    with plt.rc_context({\n",
        "        \"figure.facecolor\": \"black\",\n",
        "        \"axes.facecolor\":   \"black\",\n",
        "        \"savefig.facecolor\":\"black\",\n",
        "        \"axes.edgecolor\":   \"white\",\n",
        "        \"axes.labelcolor\":  \"white\",\n",
        "        \"xtick.color\":      \"white\",\n",
        "        \"ytick.color\":      \"white\",\n",
        "    }):\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(10.8, 4.2), sharex=True, sharey=True)\n",
        "\n",
        "        # Left: TARGET\n",
        "        ax = axs[0]\n",
        "        ax.imshow(\n",
        "            Ht.T, origin=\"lower\",\n",
        "            extent=[xlim[0], xlim[1], ylim[0], ylim[1]],\n",
        "            cmap=cmap, norm=norm, interpolation=\"bilinear\", alpha=1.0, aspect=\"equal\"\n",
        "        )\n",
        "        ax.set_title(\"target\", color='w', pad=3)\n",
        "        ax.set_xlabel(\"x\", color='w'); ax.set_ylabel(\"y\", color='w')\n",
        "        ax.tick_params(color='w', labelcolor='w')\n",
        "\n",
        "        # Right: MODEL\n",
        "        ax = axs[1]\n",
        "        ax.imshow(\n",
        "            Hm.T, origin=\"lower\",\n",
        "            extent=[xlim[0], xlim[1], ylim[0], ylim[1]],\n",
        "            cmap=cmap, norm=norm, interpolation=\"bilinear\", alpha=1.0, aspect=\"equal\"\n",
        "        )\n",
        "        ax.set_title(f\"model (K={nfe})\", color='w', pad=3)\n",
        "        ax.set_xlabel(\"x\", color='w')\n",
        "        ax.tick_params(color='w', labelcolor='w')\n",
        "\n",
        "        fig.suptitle(title, y=0.995, color='w')\n",
        "        fig.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    print(f\"[plot_output_heatmaps_2d] sample_model={t1-t0:.2f}s, sample_target={t2-t1:.2f}s  {_gpu_mem()}\")\n",
        "\n",
        "\n",
        "# ---------------------------- 3D crossings: heatmap planes + cords ----------------------------\n",
        "from matplotlib import cm\n",
        "from matplotlib.colors import Normalize\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 (activates 3D proj)\n",
        "\n",
        "\n",
        "# ----------------------------- Benchmark grid (3-model, timed) ------------------------\n",
        "def _sample_in_chunks(sampler, n: int, nfe: int, chunk_n: int):\n",
        "    \"\"\"\n",
        "    Call sampler in chunks to avoid big allocations; returns concatenated Tensor on device.\n",
        "    \"\"\"\n",
        "    outs = []\n",
        "    done = 0\n",
        "    while done < n:\n",
        "        take = min(chunk_n, n - done)\n",
        "        out = avrc_sample_torch(sampler, n=take, nfe=nfe)  # (take,2)\n",
        "        outs.append(out)\n",
        "        done += take\n",
        "    return torch.cat(outs, dim=0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _plot_heat_grid(\n",
        "    samples_by_nameK: dict, x_tgt: torch.Tensor,\n",
        "    Ks=(2,4,8,16), bins=160, xlim=None, ylim=None, cmap=\"magma\",\n",
        "    out_dir: str | None = None, prefix: str = \"bench2d\",\n",
        "    gamma: float = 0.42, vmax_percentile: float = 99.7\n",
        "):\n",
        "    \"\"\"\n",
        "    Draws a grid with rows = K and columns = [Target | model_1 | model_2 | ...],\n",
        "    using black background + magma colormap + PowerNorm (gamma) with per-row\n",
        "    global brightness normalization including the target histogram (like the probe).\n",
        "    \"\"\"\n",
        "    import os, numpy as np, matplotlib.pyplot as plt, matplotlib as mpl\n",
        "    from matplotlib.colors import PowerNorm\n",
        "\n",
        "    t0 = _tic()\n",
        "    names = list(samples_by_nameK.keys())\n",
        "    R, C = len(Ks), 1 + len(names)   # +1 for TARGET column\n",
        "\n",
        "    x_tgt_np = x_tgt.detach().cpu().numpy()\n",
        "\n",
        "    # Robust bounds from target (match probe style)\n",
        "    if xlim is None or ylim is None:\n",
        "        q = 0.997\n",
        "        xlim = xlim or (np.quantile(x_tgt_np[:,0], 1-q), np.quantile(x_tgt_np[:,0], q))\n",
        "        xlim = [val* 1.2 for val in xlim]\n",
        "        ylim = ylim or (np.quantile(x_tgt_np[:,1], 1-q), np.quantile(x_tgt_np[:,1], q))\n",
        "        ylim = [val* 1.2 for val in ylim]\n",
        "\n",
        "    # Shared edges\n",
        "    y_edges = np.linspace(xlim[0], xlim[1], bins+1)\n",
        "    z_edges = np.linspace(ylim[0], ylim[1], bins+1)\n",
        "\n",
        "    # Target histogram (reused for all rows)\n",
        "    Ht, *_ = np.histogram2d(x_tgt_np[:,0], x_tgt_np[:,1], bins=[y_edges, z_edges], density=True)\n",
        "\n",
        "    # Style: black background, white axes, like probes\n",
        "    try: plt.close('all')\n",
        "    except Exception: pass\n",
        "    mpl.rcdefaults()\n",
        "    with plt.rc_context({\n",
        "        \"figure.facecolor\": \"black\",\n",
        "        \"axes.facecolor\":   \"black\",\n",
        "        \"savefig.facecolor\":\"black\",\n",
        "        \"axes.edgecolor\":   \"white\",\n",
        "        \"axes.labelcolor\":  \"white\",\n",
        "        \"xtick.color\":      \"white\",\n",
        "        \"ytick.color\":      \"white\",\n",
        "    }):\n",
        "        fig, axs = plt.subplots(R, C, figsize=(4.3*C, 3.1*R), sharex=True, sharey=True)\n",
        "        if R == 1 and C == 1:\n",
        "            axs = np.array([[axs]])\n",
        "        elif R == 1:\n",
        "            axs = np.array([axs])\n",
        "        elif C == 1:\n",
        "            axs = np.array([[ax] for ax in axs])\n",
        "\n",
        "        # Draw rows\n",
        "        for r, K in enumerate(Ks):\n",
        "            # Collect this row's histograms for brightness normalization (include TARGET)\n",
        "            row_histos = [Ht.ravel()]\n",
        "            panels = [(\"target\", Ht)]\n",
        "\n",
        "            for name in names:\n",
        "                x_model = samples_by_nameK[name][K].detach().cpu().numpy()\n",
        "                Hm, *_ = np.histogram2d(x_model[:,0], x_model[:,1], bins=[y_edges, z_edges], density=True)\n",
        "                panels.append((f\"{name}\", Hm))\n",
        "                row_histos.append(Hm.ravel())\n",
        "\n",
        "            vmax = np.percentile(np.concatenate(row_histos), vmax_percentile)\n",
        "            norm = PowerNorm(gamma=max(1e-3, float(gamma)), vmin=0.0, vmax=max(1e-9, vmax))\n",
        "\n",
        "            # Plot the row\n",
        "            for c, (title, H) in enumerate(panels):\n",
        "                ax = axs[r, c] if R > 1 else axs[0, c]\n",
        "                ax.imshow(\n",
        "                    H.T, origin=\"lower\",\n",
        "                    extent=[xlim[0], xlim[1], ylim[0], ylim[1]],\n",
        "                    cmap=cmap, norm=norm, interpolation=\"bilinear\", alpha=1.0, aspect=\"equal\"\n",
        "                )\n",
        "                # Titles & labels\n",
        "                if c == 0:\n",
        "                    ax.set_title(\"target\", color='w', pad=3)\n",
        "                else:\n",
        "                    ax.set_title(f\"K={K} — {title}\", color='w', pad=3)\n",
        "                if r == R-1: ax.set_xlabel(\"x\", color='w')\n",
        "                if c == 0:   ax.set_ylabel(\"y\", color='w')\n",
        "                ax.tick_params(color='w', labelcolor='w')\n",
        "\n",
        "        fig.suptitle(\"2D histograms by sampler (target in left column)\", y=0.995, color='w')\n",
        "        fig.tight_layout()\n",
        "\n",
        "        saved_path = None\n",
        "        if out_dir is not None:\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "            saved_path = os.path.join(out_dir, f\"{prefix}_hists_{R}x{C}.png\")\n",
        "            fig.savefig(saved_path, dpi=170, bbox_inches=\"tight\", facecolor='black')\n",
        "            print(f\"[heat grid saved] {saved_path}\")\n",
        "        plt.show()\n",
        "\n",
        "    t3 = _tic()\n",
        "    print(f\"[plot_grid] total draw {t3 - t0:.2f}s  {_gpu_mem()}\")\n",
        "    return saved_path if out_dir is not None else None\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def benchmark_samplers_2d(\n",
        "    name2sampler: dict,\n",
        "    Ks=(2,4,8,16),\n",
        "    n=120_000,\n",
        "    mmd_max_n=8192,\n",
        "    plot_hists: bool = True,\n",
        "    hist_bins: int = 160,\n",
        "    hist_out_dir: str | None = \"bench_hists_2d\",\n",
        "    hist_prefix: str = \"bench2d\",\n",
        "    sw2_L: int = 128,\n",
        "    sw2_max_n: int | None = 20000,\n",
        "    sample_chunk_n: int = 40000,\n",
        "    # NEW: pass-through viz knobs to match the probe style\n",
        "    hist_cmap: str = \"magma\",\n",
        "    hist_gamma: float = 0.42,\n",
        "    hist_vmax_percentile: float = 99.7,\n",
        "):\n",
        "    \"\"\"\n",
        "    Prints SW2/MMD table and (optionally) plots a grid of heatmaps\n",
        "    for all samplers using the same x_tgt draw, with detailed timing/logging.\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== 2D Sampling quality (lower is better) ===  n={n}, Ks={tuple(Ks)}, sw2_L={sw2_L}, sw2_max_n={sw2_max_n}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"[env] CUDA available\", _gpu_mem())\n",
        "    else:\n",
        "        print(\"[env] CPU only\")\n",
        "\n",
        "    # one shared target draw\n",
        "    t0 = _tic()\n",
        "    x_tgt = sample_target_torch(n)  # (n,2)\n",
        "    _sync(); t1 = _tic()\n",
        "    #print(f\"[bench] drew target x_tgt: shape={tuple(x_tgt.shape)} in {t1-t0:.2f}s  {_gpu_mem()}\")\n",
        "\n",
        "    results = {}\n",
        "    samples_for_plots = {name: {} for name in name2sampler.keys()}\n",
        "\n",
        "    #hdr = \"Model\".ljust(34) + \" | \" + \"  \".join([f\"K={K:^3d}  SW2   MMD\" for K in Ks])\n",
        "    #print(hdr); print(\"-\"*len(hdr))\n",
        "\n",
        "    for name, sampler in name2sampler.items():\n",
        "        #print(f\"[bench] ---- {name} ----\")\n",
        "        sys.stdout.flush()\n",
        "        row = name.ljust(34) + \" | \"\n",
        "        entry = {}\n",
        "\n",
        "        for K in Ks:\n",
        "            #print(f\"[bench] [{name}] K={K}: sampling...\", _gpu_mem()); sys.stdout.flush()\n",
        "            s0 = _tic()\n",
        "            x = _sample_in_chunks(sampler, n=n, nfe=K, chunk_n=sample_chunk_n)  # (n,2)\n",
        "            _sync(); s1 = _tic()\n",
        "            samp_t = s1 - s0\n",
        "            thr = n / max(samp_t, 1e-9)\n",
        "            #print(f\"[bench] [{name}] K={K}: sample done in {samp_t:.2f}s  ({thr/1e6:.2f} M pts/s)  {_gpu_mem()}\")\n",
        "\n",
        "            #print(f\"[bench] [{name}] K={K}: SW2 (L={sw2_L}, max_n={sw2_max_n})...\", end=\" \"); sys.stdout.flush()\n",
        "            m0 = _tic()\n",
        "            sw2 = sliced_w2(x, x_tgt, L=sw2_L, max_n=sw2_max_n)\n",
        "            _sync(); m1 = _tic()\n",
        "            #print(f\"done in {m1-m0:.2f}s  => {sw2:.4f}\")\n",
        "\n",
        "            #print(f\"[bench] [{name}] K={K}: MMD (max_n={mmd_max_n})...\", end=\" \"); sys.stdout.flush()\n",
        "            m2 = _tic()\n",
        "            mmd = mmd_rbf_nd(x, x_tgt, max_n=mmd_max_n)\n",
        "            _sync(); m3 = _tic()\n",
        "            #print(f\"done in {m3-m2:.2f}s  => {mmd:.4f}  {_gpu_mem()}\")\n",
        "\n",
        "            # store for grid plot (keep on CPU)\n",
        "            samples_for_plots[name][K] = x.detach().cpu()\n",
        "            entry[K] = (sw2, mmd)\n",
        "            row += f\" {sw2:5.3f} {mmd:5.3f} \"\n",
        "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "        results[name] = entry\n",
        "        print(row); print()\n",
        "\n",
        "    # optional grid of histograms (target in leftmost column; no contours)\n",
        "    if plot_hists:\n",
        "        print(\"[bench] plotting heat grids...\")\n",
        "        _plot_heat_grid(\n",
        "            samples_for_plots,\n",
        "            x_tgt=x_tgt,\n",
        "            Ks=Ks,\n",
        "            bins=hist_bins,\n",
        "            out_dir=hist_out_dir,\n",
        "            prefix=hist_prefix,\n",
        "            cmap=hist_cmap,\n",
        "            gamma=hist_gamma,\n",
        "            vmax_percentile=hist_vmax_percentile,\n",
        "        )\n",
        "\n",
        "    print(\"[bench] completed benchmark_samplers_2d.\")\n",
        "    return results\n",
        "\n",
        "# --------------------------------- Chords helpers -------------------------------------\n",
        "@torch.no_grad()\n",
        "def chords_pairs_2d(model: AVRC2D, n=4096, mode=\"encoder\"):\n",
        "    x1 = sample_target_torch(n)\n",
        "    if mode == \"encoder\":\n",
        "        mu0, _ = model.Enc(x1); x_ref = mu0\n",
        "    elif mode == \"gauss\":\n",
        "        x_ref = torch.randn(n, 2, device=device, dtype=TDTYPE)\n",
        "    else:\n",
        "        raise ValueError(\"mode must be 'encoder' or 'gauss'\")\n",
        "    return x_ref, x1\n",
        "\n",
        "# -------------------------------------- Demo -----------------------------------------\n",
        "def main():\n",
        "    print(\"[main] starting...\")\n",
        "    # 0) Choose target toy\n",
        "    set_target(\"checker\")   # e.g., \"moons\",\"spiral\",\"rings\",\"checker\",\"pinwheel\",\"scurve\",\"8g\"\n",
        "    print(f\"[main] TARGET={TARGET}\")\n",
        "\n",
        "    # 1) Train AVRC2D\n",
        "    print(\"[main] training AVRC2D...\")\n",
        "    avrc = AVRC2D(AVRCConfig2D(\n",
        "        init_default = 'gauss',\n",
        "        rounds= 450,\n",
        "        critic_adapt_max = 100,\n",
        "        post_anneal_rounds=50,\n",
        "        batch=4096,\n",
        "        pretrain_steps=10000,\n",
        "        recon_k=16, recon_n=8192,\n",
        "        agg_kl_batch=65536,\n",
        "        log_every=1,\n",
        "        k_plot = 3,\n",
        "        test_rf_every = 10,\n",
        "        viz_camera = (10, -40),\n",
        "        lam_disp_start=1.0, lam_disp_end=1.0,\n",
        "        lam_align_start=0.0, lam_align_end=1.0\n",
        "    ))\n",
        "    avrc.train(progress=True, seed=0)\n",
        "    print(\"[main] AVRC2D trained.\")\n",
        "\n",
        "    # 2) Train STANDARD Rectified Flow baseline (2D)\n",
        "    print(\"[main] training Rectified Flow baseline...\")\n",
        "    rf_model, rf_sampler = train_rectified_flow_2d(\n",
        "        steps=10000, batch=4096, lr=1e-3, clip=1.0, log_every=300, hidden=128, depth=4\n",
        "    )\n",
        "    print(\"[main] RF baseline trained.\")\n",
        "\n",
        "    # 3) Build the 3 samplers for comparison histograms\n",
        "    avrc_oracle = avrc_oracle_sampler_torch_2d(avrc)\n",
        "    samplers = {\n",
        "        \"AVRC (q(z) init)\": avrc_oracle,\n",
        "        \"AVRC (N→* joint)\": avrc,          # integrates from N(0,I) with joint velocity\n",
        "        \"Rectified Flow\":   rf_sampler,    # standard RF baseline\n",
        "    }\n",
        "\n",
        "    # 3a) Benchmark table + (3×|Ks|) histogram grid with target contours (timed/logged)\n",
        "    print(\"[main] running benchmark_samplers_2d (with timings)...\")\n",
        "    _bench_t0 = _tic()\n",
        "    _ = benchmark_samplers_2d(\n",
        "        samplers,\n",
        "        Ks=(2,4,8,16,32),\n",
        "        n=100_000,\n",
        "        mmd_max_n=8192,\n",
        "        plot_hists=True,\n",
        "        hist_bins=180,\n",
        "        hist_out_dir=\"bench_hists_2d\",\n",
        "        hist_prefix=f\"{TARGET}\",\n",
        "        sw2_L=128,\n",
        "        sw2_max_n=20000,\n",
        "        sample_chunk_n=40000,\n",
        "        # new viz knobs (optional; these match your probe look)\n",
        "        hist_cmap=\"magma\",\n",
        "        hist_gamma=0.42,\n",
        "        hist_vmax_percentile=99.7,\n",
        "    )\n",
        "    _bench_t1 = _tic()\n",
        "    print(f\"[main] benchmark_samplers_2d finished in {_bench_t1 - _bench_t0:.2f}s\")\n",
        "\n",
        "    # 4) Individual heatmaps if desired\n",
        "    print(\"[main] plotting per-model heatmaps...\")\n",
        "    plot_output_heatmaps_2d(avrc,        n=150_000, nfe=8, title=f\"AVRC (N→*) on '{TARGET}'\")\n",
        "    plot_output_heatmaps_2d(avrc_oracle, n=150_000, nfe=8, title=f\"AVRC (q(z) init) on '{TARGET}'\")\n",
        "    plot_output_heatmaps_2d(rf_sampler,  n=150_000, nfe=8, title=f\"Rectified Flow on '{TARGET}'\")\n",
        "    print(\"[main] per-model heatmaps done.\")\n",
        "\n",
        "    # 5) Crossings plots\n",
        "    print(\"[main] crossings plots...\")\n",
        "    xr_enc, xt_enc = chords_pairs_2d(avrc, n=600, mode=\"encoder\")\n",
        "    # Learned coupling (E#p_data) — encoder pairs on t=0 ↔ t=1\n",
        "    plot_crossings_hist_and_chords_2d(\n",
        "        avrc, pairs_mode=\"encoder\", n_pairs=6000, subset_lines=50,\n",
        "        bins=160, plane_mode=\"heatmap\", title=f\"Crossings (encoder pairs) — {TARGET}\"\n",
        "    )\n",
        "\n",
        "    # Independent coupling to Gaussian — N(0,I) on t=0 ↔ p* on t=1\n",
        "    plot_crossings_hist_and_chords_2d(\n",
        "        pairs_mode=\"gauss\", n_pairs=6000, subset_lines=50,\n",
        "        bins=160, plane_mode=\"heatmap\", title=f\"Crossings (gaussian pairs) — {TARGET}\"\n",
        "    )\n",
        "\n",
        "    print(\"[main] crossings done.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #main()\n",
        "    import contextlib\n",
        "    with open(\"log.txt\", \"w\") as f, \\\n",
        "         contextlib.redirect_stdout(f), \\\n",
        "         contextlib.redirect_stderr(f):\n",
        "        main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkFNelfaGIzj"
      },
      "outputs": [],
      "source": [
        "import os, re, glob, shutil, subprocess, sys\n",
        "from pathlib import Path\n",
        "\n",
        "def numeric_key(path: str):\n",
        "    \"\"\"Sort by the last number in the basename; fallback to name for ties.\"\"\"\n",
        "    base = os.path.basename(path)\n",
        "    m = re.search(r'(\\d+)(?!.*\\d)', base)  # last run of digits\n",
        "    return (int(m.group(1)) if m else float('inf'), base.lower())\n",
        "\n",
        "def ensure_dir(p: Path):\n",
        "    if p.exists():\n",
        "        # fully clear any previous staging dir\n",
        "        for child in p.iterdir():\n",
        "            try:\n",
        "                if child.is_symlink() or child.is_file():\n",
        "                    child.unlink()\n",
        "                else:\n",
        "                    shutil.rmtree(child)\n",
        "            except FileNotFoundError:\n",
        "                pass\n",
        "    else:\n",
        "        p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def link_or_copy(src: Path, dst: Path):\n",
        "    try:\n",
        "        os.symlink(os.path.realpath(src), dst)\n",
        "    except (OSError, NotImplementedError):\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "def build_sequence(src_glob: str, stage_dir: Path) -> int:\n",
        "    files = sorted(glob.glob(src_glob), key=numeric_key)\n",
        "    files = [Path(f) for f in files if Path(f).is_file()]\n",
        "    if not files:\n",
        "        print(f\"[SKIP] No files matched: {src_glob}\")\n",
        "        return 0\n",
        "\n",
        "    ensure_dir(stage_dir)\n",
        "\n",
        "    # Create contiguous sequence frame_000000.png, frame_000001.png, ...\n",
        "    count = 0\n",
        "    for i, f in enumerate(files):\n",
        "        dst = stage_dir / f\"frame_{i:06d}.png\"\n",
        "        link_or_copy(f, dst)\n",
        "        count += 1\n",
        "\n",
        "    # If only one frame, duplicate last so the clip isn't zero-length\n",
        "    if count == 1:\n",
        "        dst2 = stage_dir / f\"frame_{1:06d}.png\"\n",
        "        link_or_copy(files[0], dst2)\n",
        "        count = 2\n",
        "\n",
        "    print(f\"[OK] Sequenced {count} frames → {stage_dir}/frame_%06d.png\")\n",
        "    # Show a couple of examples for sanity\n",
        "    head = list(sorted(stage_dir.glob(\"frame_*.png\")))[:3]\n",
        "    tail = list(sorted(stage_dir.glob(\"frame_*.png\")))[-3:]\n",
        "    if head:\n",
        "        print(\"  first frames:\", [p.name for p in head])\n",
        "    if tail and len(tail) != len(head):\n",
        "        print(\"  last  frames:\", [p.name for p in tail])\n",
        "    return count\n",
        "\n",
        "def encode_from_stage(stage_dir: Path, out_path: Path, fps: int):\n",
        "    cmd = [\n",
        "        \"ffmpeg\", \"-y\",\n",
        "        \"-framerate\", str(fps),\n",
        "        \"-i\", str(stage_dir / \"frame_%06d.png\"),\n",
        "        \"-vf\", \"pad=ceil(iw/2)*2:ceil(ih/2)*2:0:0\",\n",
        "        \"-c:v\", \"libx264\",\n",
        "        \"-pix_fmt\", \"yuv420p\",\n",
        "        \"-crf\", \"18\",\n",
        "        \"-movflags\", \"+faststart\",\n",
        "        \"-r\", str(fps),\n",
        "        str(out_path),\n",
        "    ]\n",
        "    print(\"[CMD]\", \" \".join(cmd))\n",
        "    proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    print(proc.stdout)\n",
        "    if proc.returncode != 0:\n",
        "        raise RuntimeError(f\"ffmpeg failed for {out_path}\")\n",
        "\n",
        "def make_movie(name: str, src_glob: str, out_path: str, fps: int):\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    stage_dir = Path(out_path).with_suffix(\"\").parent / (\".seq_\" + Path(out_path).stem)\n",
        "    count = build_sequence(src_glob, stage_dir)\n",
        "    if count == 0:\n",
        "        return\n",
        "    encode_from_stage(stage_dir, Path(out_path), fps)\n",
        "    # Clean up staging to avoid clutter; set to False to keep for debugging\n",
        "    cleanup = True\n",
        "    if cleanup:\n",
        "        try:\n",
        "            shutil.rmtree(stage_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Could not remove stage dir {stage_dir}: {e}\")\n",
        "\n",
        "# ---- Run all four builds ----\n",
        "jobs = [\n",
        "    (\"Crossings movie\",         \"viz_crossings3d/cross_*.png\",         \"viz_crossings3d/crossings_evolution.mp4\", 5),\n",
        "    (\"Latent movie\",            \"viz_crossings3d/latent_[0-9]*.png\",   \"viz_crossings3d/latent_evolution.mp4\",     8),\n",
        "    (\"Latent means (μ) movie\",  \"viz_crossings3d/latent_mu_*.png\",     \"viz_crossings3d/latent_mu_evolution.mp4\",  8),\n",
        "    (\"RF probe movie\",          \"rf_snapshots/rf_probe_*.png\",         \"rf_snapshots/rf_probe_evolution.mp4\",      3),\n",
        "]\n",
        "\n",
        "for name, g, out, fps in jobs:\n",
        "    make_movie(name, g, out, fps)\n",
        "\n",
        "print(\"\\nAll done.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}