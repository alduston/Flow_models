# -*- coding: utf-8 -*-
"""LSI_cotrain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zm8lskVmpz7Z556NhpuLGUCjk0IDKlQo
"""

import math
import os
import random
import numpy as np
from typing import Any, Dict, Tuple

import torch
from torch import nn, optim
from torch.nn import functional as F
from torch.utils.data import DataLoader, TensorDataset
import torchvision
from torchvision import transforms, utils as tv_utils

# ---------------------------------------------------------------------------
# Imports & Checks
# ---------------------------------------------------------------------------
try:
    from tqdm import tqdm
except ImportError:
    def tqdm(iterable, *args, **kwargs): return iterable

try:
    import lpips
    LPIPS_AVAILABLE = True
except ImportError:
    LPIPS_AVAILABLE = False
    print("Warning: lpips not available, perceptual loss/metrics will be skipped")

try:
    from torchmetrics.image.fid import FrechetInceptionDistance
    TORCHMETRICS_AVAILABLE = True
except ImportError:
    TORCHMETRICS_AVAILABLE = False
    print("Warning: torchmetrics not available, FID will be -1")

# ---------------------------------------------------------------------------
# Utils
# ---------------------------------------------------------------------------

def seed_everything(seed: int = 42) -> None:
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def default_device() -> torch.device:
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")

def ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)

def ensure_parent(path: str) -> None:
    parent = os.path.dirname(path)
    if parent:
        ensure_dir(parent)

def save_checkpoint(state: Dict[str, Any], path: str) -> None:
    ensure_parent(path)
    torch.save(state, path)

def make_group_norm(num_channels: int, num_groups: int = 16) -> nn.GroupNorm:
    best = min(num_groups, num_channels)
    while num_channels % best != 0 and best > 1:
        best -= 1
    return nn.GroupNorm(best, num_channels)

# ---------------------------------------------------------------------------
# Math Utilities
# ---------------------------------------------------------------------------

def sample_log_uniform_times(B: int, t_min: float, t_max: float, device: torch.device) -> torch.Tensor:
    u = torch.rand(B, device=device)
    log_t_min = math.log(t_min)
    log_t_max = math.log(t_max)
    return torch.exp(log_t_min + u * (log_t_max - log_t_min))

def get_ou_params(t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
    alpha = torch.exp(-t)
    sigma = torch.sqrt(1.0 - torch.exp(-2.0 * t) + 1e-8)
    return alpha, sigma

# ---------------------------------------------------------------------------
# Metrics: SW2, MMD, FID, Diversity
# ---------------------------------------------------------------------------

def compute_sw2(
    x: torch.Tensor,
    y: torch.Tensor,
    n_projections: int = 1000,
    theta: torch.Tensor | None = None,
) -> float:
    """Sliced Wasserstein-2 distance in latent space.

    If `theta` is provided, it must be a fixed bank of unit-norm projection vectors with shape [D, n_projections]
    (or [D, K] where K >= n_projections). This makes SW2 deterministic across eval calls.
    """
    x = x.detach().float()
    y = y.detach().float()
    device = x.device
    N, D = x.shape

    if theta is None:
        # Random projections
        theta = torch.randn(D, n_projections, device=device)
        theta = theta / torch.norm(theta, dim=0, keepdim=True).clamp_min(1e-12)
    else:
        # Allow CPU or GPU theta; move to device and slice to requested projections.
        theta = theta.to(device)
        assert theta.dim() == 2 and theta.shape[0] == D, f"theta must have shape [D, K] with D={D}; got {tuple(theta.shape)}"
        assert theta.shape[1] >= n_projections, f"theta has only {theta.shape[1]} projections; need >= {n_projections}"
        theta = theta[:, :n_projections]

    # Project
    proj_x = x @ theta
    proj_y = y @ theta

    # Sort along sample axis
    proj_x, _ = torch.sort(proj_x, dim=0)
    proj_y, _ = torch.sort(proj_y, dim=0)

    # Mean squared difference
    w2 = torch.mean((proj_x - proj_y) ** 2)
    return w2.item()

# NOTE: MMD computation removed (we only use fixed sliced-W2 in evaluation).
def compute_diversity(imgs: torch.Tensor, lpips_fn: Any) -> float:
    """
    Computes pairwise LPIPS diversity.
    Higher = more diverse samples. Near 0 = mode collapse.
    """
    if lpips_fn is None: return 0.0

    # Ensure we have enough images
    N = imgs.shape[0]
    if N < 2: return 0.0

    # Shuffle
    perm = torch.randperm(N)
    imgs = imgs[perm]

    # Split into two halves
    half = N // 2
    set1 = imgs[:half]
    set2 = imgs[half:2*half]

    # Adapt for LPIPS (Need 3 channels)
    if set1.shape[1] == 1:
        set1 = set1.repeat(1, 3, 1, 1)
        set2 = set2.repeat(1, 3, 1, 1)

    # Compute distance
    with torch.no_grad():
        dist = lpips_fn(set1, set2)

    return dist.mean().item()

def evaluate_fid(real_loader, generated_imgs, device, batch_size=128):
    if not TORCHMETRICS_AVAILABLE:
        return -1.0

    fid = FrechetInceptionDistance(feature=2048, normalize=True).to(device)

    def to_01(x):
        # assumes x is in [-1, 1]
        x = (x * 0.5) + 0.5
        return x.clamp(0.0, 1.0)

    # Real stats
    for batch, _ in real_loader:
        batch = batch.to(device).float()
        if batch.shape[1] == 1:
            batch = batch.repeat(1, 3, 1, 1)
        fid.update(to_01(batch), real=True)

    # Fake stats
    n = generated_imgs.shape[0]
    for i in range(0, n, batch_size):
        batch = generated_imgs[i:i+batch_size].to(device).float()
        if batch.shape[1] == 1:
            batch = batch.repeat(1, 3, 1, 1)
        fid.update(to_01(batch), real=False)

    return fid.compute().item()


def log_latent_stats(name, z):
    with torch.no_grad():
        mean_norm = z.mean(0).norm().item()
        std_mean = z.std(0).mean().item()
        max_val = z.abs().max().item()
        print(f"  [{name}] Latent Stats | Mean Norm: {mean_norm:.4f} | Avg Std: {std_mean:.4f} | Max: {max_val:.4f}")

# ---------------------------------------------------------------------------
# Models
# ---------------------------------------------------------------------------


import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn.functional as F

# =============================================================================
# Helper Functions for Evaluation Metrics
# =============================================================================

def extract_inception_features(images, device, batch_size=100, inception_model=None):
    """
    Extract Inception features for FID/KID computation.
    Uses InceptionV3 with final FC layer removed to get 2048-dim features.

    Args:
        images: [N, C, H, W] tensor of images (in [-1, 1] range)
        device: torch device
        batch_size: batch size for feature extraction
        inception_model: optional pre-loaded inception model (for reuse)

    Returns:
        features: [N, 2048] tensor of Inception features
        inception_model: the loaded model (for reuse)
    """
    # Lazy load inception model
    if inception_model is None:
        from torchvision.models import inception_v3
        inception_model = inception_v3(pretrained=True, transform_input=False)
        inception_model.fc = torch.nn.Identity()  # Remove final FC layer
        inception_model = inception_model.to(device)
        inception_model.eval()

    features_list = []

    with torch.no_grad():
        for i in range(0, len(images), batch_size):
            batch = images[i:i + batch_size].to(device)

            # Inception expects 3-channel images of size 299x299
            if batch.shape[1] == 1:
                batch = batch.repeat(1, 3, 1, 1)

            # Resize to 299x299
            batch = F.interpolate(batch, size=(299, 299), mode='bilinear', align_corners=False)

            # Normalize to ImageNet stats (Inception expects [0, 1] input, we have [-1, 1])
            batch = (batch + 1) / 2  # [-1, 1] -> [0, 1]
            batch = (batch - torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)) / \
                    torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)

            feat = inception_model(batch)
            features_list.append(feat.cpu())

    return torch.cat(features_list, 0), inception_model


def compute_fid_from_features(real_features, fake_features):
    """
    Compute FID from pre-extracted Inception features.

    FID = ||mu_r - mu_f||^2 + Tr(Sigma_r + Sigma_f - 2*sqrt(Sigma_r @ Sigma_f))

    Args:
        real_features: [N, D] tensor of real image features
        fake_features: [M, D] tensor of fake image features

    Returns:
        fid: scalar FID value
    """
    # Convert to numpy for numerical stability in matrix sqrt
    real_features = real_features.cpu().numpy()
    fake_features = fake_features.cpu().numpy()

    mu_r = np.mean(real_features, axis=0)
    mu_f = np.mean(fake_features, axis=0)

    sigma_r = np.cov(real_features, rowvar=False)
    sigma_f = np.cov(fake_features, rowvar=False)

    # Compute sqrt(Sigma_r @ Sigma_f) via eigendecomposition
    # covmean = scipy.linalg.sqrtm(sigma_r @ sigma_f)
    from scipy import linalg

    diff = mu_r - mu_f

    # Product might be almost singular
    covmean, _ = linalg.sqrtm(sigma_r @ sigma_f, disp=False)

    # Numerical error might give slight imaginary component
    if np.iscomplexobj(covmean):
        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):
            m = np.max(np.abs(covmean.imag))
            raise ValueError(f"Imaginary component {m}")
        covmean = covmean.real

    fid = diff @ diff + np.trace(sigma_r) + np.trace(sigma_f) - 2 * np.trace(covmean)

    return float(fid)


def compute_kid(real_features, fake_features, num_subsets=100, subset_size=1000):
    """
    Compute KID (Kernel Inception Distance) using polynomial kernel with subsampling.

    Uses polynomial kernel: k(x, y) = (x^T y / d + 1)^3
    KID = MMD^2 between real and fake Inception features

    Args:
        real_features: [N, D] tensor of real image features
        fake_features: [M, D] tensor of fake image features
        num_subsets: number of random subsets for averaging
        subset_size: size of each subset

    Returns:
        kid_mean: mean KID across subsets
    """
    # Ensure tensors
    if not isinstance(real_features, torch.Tensor):
        real_features = torch.tensor(real_features)
    if not isinstance(fake_features, torch.Tensor):
        fake_features = torch.tensor(fake_features)

    n_real = real_features.shape[0]
    n_fake = fake_features.shape[0]
    d = real_features.shape[1]

    # Ensure we have enough samples
    subset_size = min(subset_size, n_real, n_fake)

    def polynomial_kernel(x, y):
        # k(x, y) = (x^T y / d + 1)^3
        return ((x @ y.T) / d + 1) ** 3

    kid_values = []

    for _ in range(num_subsets):
        # Random subsets
        real_idx = torch.randperm(n_real)[:subset_size]
        fake_idx = torch.randperm(n_fake)[:subset_size]

        real_subset = real_features[real_idx]
        fake_subset = fake_features[fake_idx]

        # Compute kernel matrices
        k_rr = polynomial_kernel(real_subset, real_subset)
        k_ff = polynomial_kernel(fake_subset, fake_subset)
        k_rf = polynomial_kernel(real_subset, fake_subset)

        # MMD^2 unbiased estimator
        m = subset_size

        # Remove diagonal for unbiased estimate
        diag_rr = torch.diag(k_rr)
        diag_ff = torch.diag(k_ff)

        sum_rr = (k_rr.sum() - diag_rr.sum()) / (m * (m - 1))
        sum_ff = (k_ff.sum() - diag_ff.sum()) / (m * (m - 1))
        sum_rf = k_rf.mean()

        mmd2 = sum_rr + sum_ff - 2 * sum_rf
        kid_values.append(mmd2.item())

    return np.mean(kid_values)


def compute_lsi_gap(score_net, encoder_mus, encoder_logvars, cfg, device,
                    num_samples=5000, num_time_points=50, batch_size=128):
    """
    Compute LSI gap metric in score parameterization.

    LSI Gap = E_{x, t, z_t|x} || s_theta(z_t, t) - s_LSI(z_t, t; x) ||^2

    where s_LSI = -Sigma_t(x)^{-1} (z_t - mu_t(x))

    The network is eps-parameterized: eps_pred = -sigma_t * s_theta
    So we convert: || s_theta - s_LSI ||^2 = (1/sigma_t^2) || eps_pred - eps_LSI ||^2

    Args:
        score_net: the score network (eps-parameterized)
        encoder_mus: [N, C, H, W] tensor of encoder means
        encoder_logvars: [N, C, H, W] tensor of encoder log-variances
        cfg: config dict with t_min, t_max
        device: torch device
        num_samples: number of data samples to use
        num_time_points: number of time points in discretization
        batch_size: batch size for computation

    Returns:
        lsi_gap: scalar LSI gap value (in score parameterization)
    """
    if score_net is None:
        return 0.0

    score_net.eval()

    # Use a subset of data for stability (randomly sample indices)
    n_data = encoder_mus.shape[0]
    num_samples = min(num_samples, n_data)
    sample_indices = torch.randperm(n_data)[:num_samples]

    # Time discretization: linear grid matching typical sampler discretization
    t_min, t_max = cfg["t_min"], cfg["t_max"]
    time_grid = torch.linspace(t_min, t_max, num_time_points, device=device)

    total_lsi_gap = 0.0
    total_count = 0

    with torch.no_grad():
        # Process in batches
        for i in range(0, num_samples, batch_size):
            batch_indices = sample_indices[i:i + batch_size]
            batch_mu = encoder_mus[batch_indices].to(device)
            batch_logvar = encoder_logvars[batch_indices].to(device)
            batch_var = torch.exp(batch_logvar)
            batch_std = torch.exp(0.5 * batch_logvar)

            bsz = batch_mu.shape[0]

            # Sample z0 from posterior
            eps_0 = torch.randn_like(batch_mu)
            z0 = batch_mu + batch_std * eps_0

            # For each time point, compute LSI gap
            for t_val in time_grid:
                t = t_val.expand(bsz)
                alpha, sigma = get_ou_params(t.view(bsz, 1, 1, 1))

                # Diffuse: z_t = alpha * z0 + sigma * noise
                noise = torch.randn_like(z0)
                z_t = alpha * z0 + sigma * noise

                # Compute LSI target (in eps parameterization)
                # mu_t = alpha * mu, var_t = alpha^2 * var_0 + sigma^2
                mu_t = alpha * batch_mu
                var_t = (alpha ** 2) * batch_var + (sigma ** 2)

                # eps_LSI = sigma * Sigma_t^{-1} (z_t - mu_t)
                eps_target_lsi = sigma * ((z_t - mu_t) / (var_t + 1e-8))

                # Network prediction (eps parameterization)
                eps_pred = score_net(z_t, t)

                # Convert to score parameterization for the gap
                # s_theta = -eps_pred / sigma, s_LSI = -eps_LSI / sigma
                # || s_theta - s_LSI ||^2 = (1/sigma^2) || eps_pred - eps_LSI ||^2

                sigma_sq = sigma ** 2 + 1e-8  # [B, 1, 1, 1]
                eps_diff_sq = (eps_pred - eps_target_lsi) ** 2  # [B, C, H, W]

                # Sum over spatial/channel dims, divide by sigma^2, then average over batch
                score_gap_per_sample = (eps_diff_sq / sigma_sq).sum(dim=(1, 2, 3))  # [B]

                total_lsi_gap += score_gap_per_sample.sum().item()
                total_count += bsz

    return total_lsi_gap / total_count if total_count > 0 else 0.0

class VAE(nn.Module):
    def __init__(self, latent_channels: int = 4, base_ch: int = 32):
        super().__init__()
        # Encoder
        self.enc_conv_in = nn.Conv2d(1, base_ch, 3, 1, 1)
        self.enc_blocks = nn.ModuleList([
            nn.Sequential(VAEResBlock(base_ch, base_ch), nn.Conv2d(base_ch, base_ch*2, 3, 2, 1)),
            nn.Sequential(VAEResBlock(base_ch*2, base_ch*2), nn.Conv2d(base_ch*2, base_ch*4, 3, 2, 1)),
            nn.Sequential(VAEResBlock(base_ch*4, base_ch*4), AttentionBlock(base_ch*4), VAEResBlock(base_ch*4, base_ch*4))
        ])
        self.mu = nn.Conv2d(base_ch*4, latent_channels, 1)
        self.logvar = nn.Conv2d(base_ch*4, latent_channels, 1)

        # Decoder
        self.dec_conv_in = nn.Conv2d(latent_channels, base_ch*4, 1)
        self.dec_blocks = nn.ModuleList([
            nn.Sequential(VAEResBlock(base_ch*4, base_ch*4), AttentionBlock(base_ch*4), VAEResBlock(base_ch*4, base_ch*4)),
            nn.Sequential(nn.Upsample(scale_factor=2), nn.Conv2d(base_ch*4, base_ch*2, 3, 1, 1), VAEResBlock(base_ch*2, base_ch*2)),
            nn.Sequential(nn.Upsample(scale_factor=2), nn.Conv2d(base_ch*2, base_ch, 3, 1, 1), VAEResBlock(base_ch, base_ch))
        ])
        self.dec_out = nn.Sequential(
            nn.GroupNorm(16, base_ch), nn.SiLU(), nn.Conv2d(base_ch, 1, 3, 1, 1)
        )

    def encode(self, x):
        h = self.enc_conv_in(x)
        for block in self.enc_blocks: h = block(h)
        return self.mu(h), self.logvar(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        h = self.dec_conv_in(z)
        for block in self.dec_blocks: h = block(h)
        return torch.tanh(self.dec_out(h))

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

class VAEResBlock(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.net = nn.Sequential(
            make_group_norm(in_ch), nn.SiLU(), nn.Conv2d(in_ch, out_ch, 3, 1, 1),
            make_group_norm(out_ch), nn.SiLU(), nn.Conv2d(out_ch, out_ch, 3, 1, 1)
        )
        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()
    def forward(self, x): return self.net(x) + self.skip(x)

class AttentionBlock(nn.Module):
    def __init__(self, ch):
        super().__init__()
        self.norm = make_group_norm(ch)
        self.qkv = nn.Conv2d(ch, ch*3, 1)
        self.proj = nn.Conv2d(ch, ch, 1)
    def forward(self, x):
        B, C, H, W = x.shape
        q, k, v = self.qkv(self.norm(x)).reshape(B, 3, C, -1).chunk(3, 1)
        attn = (q.transpose(-2, -1) @ k) * (C ** -0.5)
        attn = attn.softmax(dim=-1)
        h = (v @ attn.transpose(-2, -1)).reshape(B, C, H, W)
        return x + self.proj(h)

class TimeEmbedding(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.mlp = nn.Sequential(nn.Linear(dim, 4*dim), nn.SiLU(), nn.Linear(4*dim, 4*dim))
    def forward(self, t):
        half = self.mlp[0].in_features // 2
        freqs = torch.exp(torch.linspace(0, math.log(10000), half, device=t.device))
        args = t[:, None] * freqs[None, :]
        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)
        return self.mlp(emb)

class UNetModel(nn.Module):
    def __init__(self, in_channels=4, base_channels=32, channel_mults=(1, 2, 4), num_res_blocks=2):
        super().__init__()
        self.time_embed = TimeEmbedding(base_channels)
        self.head = nn.Conv2d(in_channels, base_channels, 3, 1, 1)
        self.downs = nn.ModuleList()
        ch = base_channels
        chs = [ch]
        for i, mult in enumerate(channel_mults):
            out_ch = base_channels * mult
            for _ in range(num_res_blocks):
                self.downs.append(ResBlock(ch, out_ch, base_channels*4))
                ch = out_ch
                chs.append(ch)
            if i != len(channel_mults)-1:
                self.downs.append(nn.Conv2d(ch, ch, 3, 2, 1))
                chs.append(ch)
        self.mid = nn.ModuleList([
            ResBlock(ch, ch, base_channels*4), AttentionBlock(ch), ResBlock(ch, ch, base_channels*4)
        ])
        self.ups = nn.ModuleList()
        for i, mult in reversed(list(enumerate(channel_mults))):
            out_ch = base_channels * mult
            for _ in range(num_res_blocks + 1):
                skip = chs.pop()
                self.ups.append(ResBlock(ch+skip, out_ch, base_channels*4))
                ch = out_ch
            if i != 0:
                self.ups.append(nn.Sequential(nn.Upsample(scale_factor=2), nn.Conv2d(ch, ch, 3, 1, 1)))
        self.out = nn.Sequential(make_group_norm(ch), nn.SiLU(), nn.Conv2d(ch, in_channels, 3, 1, 1))
    def forward(self, x, t):
        emb = self.time_embed(t)
        h = self.head(x)
        hs = [h]
        for layer in self.downs:
            if isinstance(layer, ResBlock): h = layer(h, emb)
            else: h = layer(h)
            hs.append(h)
        for layer in self.mid:
            if isinstance(layer, ResBlock): h = layer(h, emb)
            else: h = layer(h)
        for layer in self.ups:
            if isinstance(layer, ResBlock):
                h = torch.cat([h, hs.pop()], dim=1)
                h = layer(h, emb)
            else: h = layer(h)
        return self.out(h)

class ResBlock(nn.Module):
    def __init__(self, in_ch, out_ch, t_dim):
        super().__init__()
        self.block1 = nn.Sequential(make_group_norm(in_ch), nn.SiLU(), nn.Conv2d(in_ch, out_ch, 3, 1, 1))
        self.time_proj = nn.Linear(t_dim, out_ch)
        self.block2 = nn.Sequential(make_group_norm(out_ch), nn.SiLU(), nn.Conv2d(out_ch, out_ch, 3, 1, 1))
        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()
    def forward(self, x, t_emb):
        h = self.block1(x)
        h = h + self.time_proj(t_emb)[:, :, None, None]
        return self.block2(h) + self.skip(x)

# ---------------------------------------------------------------------------
# Sampling
# ---------------------------------------------------------------------------

# ---------------------------------------------------------------------------
# 1. Advanced Sampling (Universal Sampler)
# ---------------------------------------------------------------------------

class UniversalSampler:
    def __init__(self, method="heun_sde", num_steps=20, t_min=1e-4, t_max=3.0):
        self.num_steps = num_steps
        self.t_min = t_min
        self.t_max = t_max
        self.method = method  # 'heun_sde', 'euler_ode', 'rk4_ode'

    def get_ode_derivative(self, x, t, unet):
        B = x.shape[0]
        t_vec = t.expand(B)
        eps_pred = unet(x, t_vec)
        _, sigma = get_ou_params(t_vec.view(B, 1, 1, 1))
        inv_sigma = 1.0 / (sigma + 1e-8)
        return -x + inv_sigma * eps_pred

    def step_euler_ode(self, x, t_curr, t_next, unet):
        dt = t_next - t_curr
        d_curr = self.get_ode_derivative(x, t_curr, unet)
        return x + dt * d_curr

    def step_rk4_ode(self, x, t_curr, t_next, unet):
        dt = t_next - t_curr
        half_dt = dt * 0.5
        t_half = t_curr + half_dt

        k1 = self.get_ode_derivative(x, t_curr, unet)
        k2 = self.get_ode_derivative(x + half_dt * k1, t_half, unet)
        k3 = self.get_ode_derivative(x + half_dt * k2, t_half, unet)
        k4 = self.get_ode_derivative(x + dt * k3, t_next, unet)

        return x + (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)

    def step_heun_sde(self, x, t_curr, t_next, unet):
        B = x.shape[0]
        dt = t_next - t_curr

        # Predictor (Euler)
        t_vec = t_curr.expand(B)
        eps_pred = unet(x, t_vec)
        _, sigma = get_ou_params(t_vec.view(B, 1, 1, 1))
        d_curr = -x + (1.0 / (sigma + 1e-8)) * eps_pred
        x_proposed = x + dt * d_curr

        # Corrector (Heun)
        if t_next > self.t_min:
            t_next_vec = t_next.expand(B)
            eps_next = unet(x_proposed, t_next_vec)
            _, sigma_next = get_ou_params(t_next_vec.view(B, 1, 1, 1))
            d_next = -x_proposed + (1.0 / (sigma_next + 1e-8)) * eps_next
            x = x + 0.5 * dt * (d_curr + d_next)
        else:
            x = x_proposed

        return x

    def sample(self, unet, shape=None, device=None, x_init=None, generator=None):
        """
        If x_init is provided, uses it as the fixed starting noise (for a noise bank).
        Otherwise, samples fresh noise with torch.randn(shape).
        """
        unet.eval()

        if x_init is None:
            assert shape is not None and device is not None
            x = torch.randn(shape, device=device, generator=generator)
            device = x.device
        else:
            x = x_init
            device = x.device

        ts = torch.logspace(
            math.log10(self.t_max),
            math.log10(self.t_min),
            self.num_steps + 1,
            device=device
        )

        for i in range(self.num_steps):
            t_curr = ts[i]
            t_next = ts[i + 1]

            if self.method == "rk4_ode":
                x = self.step_rk4_ode(x, t_curr, t_next, unet)
            elif self.method == "euler_ode":
                x = self.step_euler_ode(x, t_curr, t_next, unet)
            else:
                x = self.step_heun_sde(x, t_curr, t_next, unet)

        return x



def evaluate_current_state(
    epoch_idx,
    prefix,
    vae,
    unet,
    loader,
    cfg,
    device,
    lpips_fn,
    fixed_noise_bank=None,
    fixed_posterior_eps_bank_A=None,
    fixed_posterior_eps_bank_B=None,
    fixed_sw2_theta=None,
):
    """
    Full MNIST test-set evaluation (uses `loader` directly).

    Compared configurations per call:
      - VAE_Rec_eps: posterior-sampled reconstructions using z = mu + std * eps_A
      - If unet is provided: latent diffusion sampling with the requested samplers, decode(z0)

    Metrics computed:
      - FID: Frechet Inception Distance (image quality)
      - KID: Kernel Inception Distance (image quality, lower variance than FID)
      - SW2: Sliced Wasserstein-2 distance in latent space
      - LSI Gap: Score matching quality metric (score parameterization)
      - Diversity: LPIPS-based sample diversity

    For maximum comparability across models/runs:
      - fixed_noise_bank: fixed initial latent noise x_T bank for diffusion sampling
      - fixed_posterior_eps_bank_A/B: fixed eps banks for posterior sampling
      - fixed_sw2_theta: fixed SW2 projection bank
    """
    print(f"\n--- Evaluation: {prefix} @ Ep {epoch_idx} ---")
    vae.eval()
    if unet is not None:
        unet.eval()

    target_count = len(loader.dataset)
    bs = cfg["batch_size"]
    latent_shape = (cfg["latent_channels"], 8, 8)
    sw2_nproj = int(cfg.get("sw2_n_projections", 1000))

    # Validate banks
    if fixed_noise_bank is not None:
        assert fixed_noise_bank.shape[0] >= target_count
        assert tuple(fixed_noise_bank.shape[1:]) == latent_shape
    if fixed_posterior_eps_bank_A is not None:
        assert fixed_posterior_eps_bank_A.shape[0] >= target_count
        assert tuple(fixed_posterior_eps_bank_A.shape[1:]) == latent_shape
    if fixed_posterior_eps_bank_B is not None:
        assert fixed_posterior_eps_bank_B.shape[0] >= target_count
        assert tuple(fixed_posterior_eps_bank_B.shape[1:]) == latent_shape

    # -----------------------------------------------------------------------
    # Collect data: latents, images, encoder outputs
    # -----------------------------------------------------------------------
    real_latents_A, real_latents_B, real_imgs = [], [], []
    encoder_mus, encoder_logvars = [], []
    bank_idx = 0

    with torch.no_grad():
        for x, _ in loader:
            x = x.to(device)
            mu, logvar = vae.encode(x)
            std = torch.exp(0.5 * logvar)
            bsz = x.shape[0]

            encoder_mus.append(mu.cpu())
            encoder_logvars.append(logvar.cpu())

            # Posterior samples
            epsA = fixed_posterior_eps_bank_A[bank_idx:bank_idx + bsz].to(device) \
                   if fixed_posterior_eps_bank_A is not None else torch.randn_like(std)
            zA = mu + std * epsA
            real_latents_A.append(zA.cpu())

            if fixed_posterior_eps_bank_B is not None:
                epsB = fixed_posterior_eps_bank_B[bank_idx:bank_idx + bsz].to(device)
                real_latents_B.append((mu + std * epsB).cpu())

            real_imgs.append(x.cpu())
            bank_idx += bsz
            if bank_idx >= target_count:
                break

    real_latents_A = torch.cat(real_latents_A, 0)[:target_count]
    real_imgs = torch.cat(real_imgs, 0)[:target_count]
    encoder_mus = torch.cat(encoder_mus, 0)[:target_count]
    encoder_logvars = torch.cat(encoder_logvars, 0)[:target_count]
    real_flat_A = real_latents_A.view(target_count, -1).to(device)

    if fixed_posterior_eps_bank_B is not None:
        real_latents_B = torch.cat(real_latents_B, 0)[:target_count]
        real_flat_B = real_latents_B.view(target_count, -1).to(device)
    else:
        real_flat_B = None

    # -----------------------------------------------------------------------
    # Pre-compute shared quantities
    # -----------------------------------------------------------------------
    print("  Extracting Inception features...")
    real_features, inception_model = extract_inception_features(
        real_imgs, device, batch_size=cfg.get("fid_batch_size", bs)
    )
    real_features = real_features.to(device)

    # LSI gap (computed once, applies to all diffusion methods)
    lsi_gap_unet = compute_lsi_gap(
        unet, encoder_mus, encoder_logvars, cfg, device,
        num_samples=min(5000, target_count), num_time_points=50, batch_size=bs
    )

    # -----------------------------------------------------------------------
    # Sampler configurations
    # -----------------------------------------------------------------------
    configs = [("VAE_Rec_eps", 0, "Recon (posterior z)")]
    if unet is not None:
        configs.extend([
            ("heun_sde", 20, "Baseline (Heun)"),
            ("rk4_ode",  10, "Smoothness (RK4)"),
        ])

    results = []

    # -----------------------------------------------------------------------
    # Evaluation sweep
    # -----------------------------------------------------------------------
    for method, steps, desc in configs:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        with torch.no_grad():
            if method == "VAE_Rec_eps":
                # Decode posterior-sampled latents
                fake_imgs = torch.cat([
                    vae.decode(real_latents_A[i:i + bs].to(device)).cpu()
                    for i in range(0, len(real_latents_A), bs)
                ], 0)

                # SW2: aggregated posterior floor
                if real_flat_B is not None:
                    w2 = compute_sw2(real_flat_A, real_flat_B, n_projections=sw2_nproj, theta=fixed_sw2_theta)
                else:
                    perm = torch.randperm(real_flat_A.size(0), device=device)
                    half = real_flat_A.size(0) // 2
                    w2 = compute_sw2(real_flat_A[perm[:half]], real_flat_A[perm[half:2*half]],
                                     n_projections=sw2_nproj, theta=fixed_sw2_theta)
                lsi_gap = 0.0

            else:
                # Diffusion sampling
                sampler = UniversalSampler(method=method, num_steps=steps,
                                           t_min=cfg["t_min"], t_max=cfg["t_max"])
                fake_latents_list, fake_imgs_list = [], []

                for i in range(0, target_count, bs):
                    batch_sz = min(bs, target_count - i)
                    if fixed_noise_bank is not None:
                        xT = fixed_noise_bank[i:i + batch_sz].to(device)
                        z_gen = sampler.sample(unet, x_init=xT)
                    else:
                        z_gen = sampler.sample(unet, shape=(batch_sz, *latent_shape), device=device)
                    fake_latents_list.append(z_gen.cpu())
                    fake_imgs_list.append(vae.decode(z_gen).cpu())

                fake_latents = torch.cat(fake_latents_list, 0)
                fake_imgs = torch.cat(fake_imgs_list, 0)
                fake_flat = fake_latents.view(fake_latents.shape[0], -1).to(device)
                w2 = compute_sw2(real_flat_A, fake_flat, n_projections=sw2_nproj, theta=fixed_sw2_theta)
                lsi_gap = lsi_gap_unet

        # Compute image metrics
        fake_features, inception_model = extract_inception_features(
            fake_imgs, device, batch_size=cfg.get("fid_batch_size", bs),
            inception_model=inception_model
        )
        fake_features = fake_features.to(device)

        fid = compute_fid_from_features(real_features, fake_features)
        kid = compute_kid(real_features, fake_features, num_subsets=100, subset_size=1000)
        div = compute_diversity(fake_imgs.to(device), lpips_fn) if LPIPS_AVAILABLE else 0.0

        results.append({
            "config": f"{method}@{steps}",
            "desc": desc,
            "fid": fid,
            "kid": kid,
            "w2": w2,
            "div": div,
            "lsi_gap": lsi_gap,
        })

        # Save samples
        if method in ("VAE_Rec_eps",) or "rk4" in method or "heun" in method:
            save_path = os.path.join("samples", f"{prefix}_{method}_{steps}_ep{epoch_idx}.png")
            panel = fake_imgs[:16] if fake_imgs.shape[0] >= 16 else fake_imgs
            tv_utils.save_image((panel + 1) / 2, save_path, nrow=4, padding=2)

    # -----------------------------------------------------------------------
    # Print results
    # -----------------------------------------------------------------------
    print(f"\n  >>> Sweep Results [{prefix}] <<<")
    print(f"  {'Config':<15} | {'Desc':<20} | {'FID':<8} | {'KID':<10} | {'SW2':<10} | {'Div':<8} | {'LSI Gap':<10}")
    print("  " + "-" * 100)
    for r in results:
        print(f"  {r['config']:<15} | {r['desc']:<20} | {r['fid']:<8.2f} | {r['kid']:<10.4f} | "
              f"{r['w2']:<10.6f} | {r['div']:<8.4f} | {r['lsi_gap']:<10.4f}")
    print("  " + "-" * 100 + "\n")



# ---------------------------------------------------------------------------
# Data & Training
# ---------------------------------------------------------------------------





def make_dataloaders(batch_size, num_workers):
    tf = transforms.Compose([
        transforms.Pad(2),
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    train = torchvision.datasets.FashionMNIST("./data", train=True, download=True, transform=tf)
    test = torchvision.datasets.FashionMNIST("./data", train=False, download=True, transform=tf)
    tl = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)
    vl = DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    return tl, vl


def train_vae_cotrained(cfg):
    device = default_device()
    train_l, test_l = make_dataloaders(cfg["batch_size"], cfg["num_workers"])

    vae = VAE(latent_channels=cfg["latent_channels"]).to(device)
    eval_freq = cfg.get("eval_freq", 10)

    # --- Online Models ---
    unet_lsi = UNetModel(in_channels=cfg["latent_channels"]).to(device)
    unet_control = UNetModel(in_channels=cfg["latent_channels"]).to(device)

    # --- OPTIONAL: Load from Checkpoint ---
    if cfg.get("load_from_checkpoint", False):
        print(f"--> Loading checkpoints from {cfg['ckpt_dir']}...")
        try:
            vae.load_state_dict(torch.load(os.path.join(cfg["ckpt_dir"], "vae_cotrained.pt"), map_location=device))
            print("    Loaded VAE.")
        except Exception as e:
            print(f"    Warning: Could not load VAE ({e})")

        try:
            unet_lsi.load_state_dict(torch.load(os.path.join(cfg["ckpt_dir"], "unet_lsi.pt"), map_location=device))
            print("    Loaded UNet LSI.")
        except Exception as e:
            print(f"    Warning: Could not load UNet LSI ({e})")

        try:
            unet_control.load_state_dict(torch.load(os.path.join(cfg["ckpt_dir"], "unet_control.pt"), map_location=device))
            print("    Loaded UNet Control.")
        except Exception as e:
            print(f"    Warning: Could not load UNet Control ({e})")

    # --- EMA Models (Score Heads Only) ---
    # We create copies and detach them from the graph
    # IMPORTANT: We initialize EMA *after* potential checkpoint loading so they match.
    unet_lsi_ema = UNetModel(in_channels=cfg["latent_channels"]).to(device)
    unet_lsi_ema.load_state_dict(unet_lsi.state_dict())
    unet_lsi_ema.eval()
    for p in unet_lsi_ema.parameters(): p.requires_grad = False

    unet_control_ema = UNetModel(in_channels=cfg["latent_channels"]).to(device)
    unet_control_ema.load_state_dict(unet_control.state_dict())
    unet_control_ema.eval()
    for p in unet_control_ema.parameters(): p.requires_grad = False

    ema_decay = 0.999  # Standard decay for optimization stability


    # --- Asymmetric LR Settings ---
    # Split params_joint into groups to assign specific LRs
    score_w_vae = cfg.get("score_w_vae", cfg["score_w"])
    opt_joint = optim.AdamW([
        {'params': vae.parameters(), 'lr': cfg["lr_vae"]},      # VAE stays at lower LR
        {'params': unet_lsi.parameters(), 'lr': cfg["lr_ldm"]/score_w_vae}  # LSI Head gets higher LR
        #{'params': unet_lsi.parameters(), 'lr': cfg["lr_ldm"]}  # LSI Head gets higher LR
    ], weight_decay=1e-4)

    # Control baseline uses lr_ldm naturally
    opt_control = optim.AdamW(unet_control.parameters(), lr=cfg["lr_ldm"], weight_decay=1e-4)

    # For Metrics
    lpips_fn = lpips.LPIPS(net='vgg').to(device) if LPIPS_AVAILABLE else None

    # -----------------------------------------------------------------------
    # Fixed evaluation banks for apples-to-apples comparability
    # -----------------------------------------------------------------------
    if cfg.get("use_fixed_eval_banks", True):
        N_test = len(test_l.dataset)
        latent_shape = (cfg["latent_channels"], 8, 8)
        seed = int(cfg.get("seed", 0))

        # Fixed latent noise for diffusion initial states (x_T)
        g_noise = torch.Generator(device="cpu").manual_seed(seed + 12345)
        fixed_noise_bank = torch.randn((N_test, *latent_shape), generator=g_noise)

        # Fixed eps banks for posterior sampling:
        #   z_A = mu + std * eps_A,  z_B = mu + std * eps_B
        g_postA = torch.Generator(device="cpu").manual_seed(seed + 54321)
        g_postB = torch.Generator(device="cpu").manual_seed(seed + 98765)
        fixed_posterior_eps_bank_A = torch.randn((N_test, *latent_shape), generator=g_postA)
        fixed_posterior_eps_bank_B = torch.randn((N_test, *latent_shape), generator=g_postB)

        # Fixed SW2 projection bank (for deterministic sliced-W2)
        D = cfg["latent_channels"] * 8 * 8
        K = int(cfg.get("sw2_n_projections", 1000))
        g_theta = torch.Generator(device="cpu").manual_seed(seed + 22222)
        theta = torch.randn((D, K), generator=g_theta)
        theta = theta / torch.norm(theta, dim=0, keepdim=True).clamp_min(1e-12)
        fixed_sw2_theta = theta
    else:
        fixed_noise_bank = None
        fixed_posterior_eps_bank_A = None
        fixed_posterior_eps_bank_B = None
        fixed_sw2_theta = None

    print("--> Starting Dual Co-training...")
    for ep in range(cfg["epochs_vae"]):
        vae.train(); unet_lsi.train(); unet_control.train()
        metrics = {k: 0.0 for k in ["loss", "recon", "kl", "score_lsi", "score_control", "perc"]}
        mu_stats = []

        for x, _ in tqdm(train_l, desc=f"Ep {ep+1}", leave=False):
            x = x.to(device)
            B = x.shape[0]

            # --- VAE & LSI (Joint) ---
            x_rec, mu, logvar = vae(x)
            if len(mu_stats) < 5: mu_stats.append(mu.detach())

            recon = F.mse_loss(x_rec, x)

            if LPIPS_AVAILABLE:
                x_3c = x.repeat(1, 3, 1, 1)
                x_rec_3c = x_rec.repeat(1, 3, 1, 1)
                perc = lpips_fn(x_rec_3c, x_3c).mean()
            else:
                perc = torch.tensor(0.0, device=device)

            mod_kl = -0.5 * torch.mean(1 - mu.pow(2) - logvar.exp())
            #kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())

            kl = mod_kl
            #mod_kl = kl

            t = sample_log_uniform_times(B, cfg["t_min"], cfg["t_max"], device)
            z0 = vae.reparameterize(mu, logvar)
            alpha, sigma = get_ou_params(t.view(B,1,1,1))

            noise = torch.randn_like(z0)
            z_t = alpha * z0 + sigma * noise

            var_0 = torch.exp(logvar)
            mu_t = alpha * mu
            var_t = (alpha**2) * var_0 + (sigma**2)

            eps_target_lsi = sigma * ((z_t - mu_t) / (var_t + 1e-8))
            eps_pred_lsi = unet_lsi(z_t, t)
            score_loss_lsi = F.mse_loss(eps_pred_lsi, eps_target_lsi)

            # Separate weights for score loss: score_w for UNet, score_w_vae for VAE gradients
            score_w_vae = cfg.get("score_w_vae", cfg["score_w"])
            loss_joint = recon + cfg["perc_w"]*perc + cfg["kl_w"]*mod_kl + score_w_vae*score_loss_lsi

            opt_joint.zero_grad()
            loss_joint.backward()
            nn.utils.clip_grad_norm_(list(vae.parameters()) + list(unet_lsi.parameters()), 1.0)
            opt_joint.step()

            # --- EMA Update (LSI) ---
            with torch.no_grad():
                for p_online, p_ema in zip(unet_lsi.parameters(), unet_lsi_ema.parameters()):
                    p_ema.data.mul_(ema_decay).add_(p_online.data, alpha=1 - ema_decay)

            # --- Control (Tweedie/DSM) ---
            z_t_detached = z_t.detach()
            eps_pred_control = unet_control(z_t_detached, t)
            score_loss_control = cfg["score_w"] * F.mse_loss(eps_pred_control, noise)

            opt_control.zero_grad()
            score_loss_control.backward()
            nn.utils.clip_grad_norm_(unet_control.parameters(), 1.0)
            opt_control.step()

            # --- EMA Update (Control) ---
            with torch.no_grad():
                for p_online, p_ema in zip(unet_control.parameters(), unet_control_ema.parameters()):
                    p_ema.data.mul_(ema_decay).add_(p_online.data, alpha=1 - ema_decay)

            metrics["loss"] += loss_joint.item()
            metrics["recon"] += recon.item()
            metrics["kl"] += mod_kl.item()
            metrics["score_lsi"] += score_loss_lsi.item()
            metrics["score_control"] += score_loss_control.item()
            metrics["perc"] += perc.item()

        # --- Updated Logging ---
        print(f"Ep {ep+1} | LSI: {metrics['score_lsi']/len(train_l):.4f} | Ctrl: {metrics['score_control']/len(train_l):.4f} | "
              f"Rec: {metrics['recon']/len(train_l):.4f} | KL: {metrics['kl']/len(train_l):.4f} | Perc: {metrics['perc']/len(train_l):.4f}")

        if len(mu_stats) > 0:
            log_latent_stats("VAE_Train", torch.cat(mu_stats, 0))

        if (ep + 1) % eval_freq == 0:
            # NOTE: We now pass the EMA models to evaluation for better stability metrics
            evaluate_current_state(
                ep + 1,
                "LSI_Diff",
                vae,
                unet_lsi_ema, # Use EMA
                test_l,
                cfg,
                device,
                lpips_fn,
                fixed_noise_bank=fixed_noise_bank,
                fixed_posterior_eps_bank_A=fixed_posterior_eps_bank_A,
                fixed_posterior_eps_bank_B=fixed_posterior_eps_bank_B,
                fixed_sw2_theta=fixed_sw2_theta,
            )
            evaluate_current_state(
                ep + 1,
                "Ctrl_Diff",
                vae,
                unet_control_ema, # Use EMA
                test_l,
                cfg,
                device,
                lpips_fn,
                fixed_noise_bank=fixed_noise_bank,
                fixed_posterior_eps_bank_A=fixed_posterior_eps_bank_A,
                fixed_posterior_eps_bank_B=fixed_posterior_eps_bank_B,
                fixed_sw2_theta=fixed_sw2_theta,
            )

    # ===========================================================================
    # REFINEMENT STAGE: Freeze VAE, train only score networks
    # ===========================================================================
    epochs_refine = cfg.get("epochs_refine", 20)
    lr_refine = cfg.get("lr_refine", 1e-4)

    if epochs_refine > 0:
        print(f"\n--> Starting Refinement Stage ({epochs_refine} epochs, lr={lr_refine})...")

        # Freeze VAE
        vae.eval()
        for p in vae.parameters():
            p.requires_grad = False

        # Create new optimizers for refinement stage (score nets only)
        opt_lsi_refine = optim.AdamW(unet_lsi.parameters(), lr=lr_refine, weight_decay=1e-4)
        opt_control_refine = optim.AdamW(unet_control.parameters(), lr=lr_refine, weight_decay=1e-4)

        for ep in range(epochs_refine):
            unet_lsi.train(); unet_control.train()
            metrics_refine = {k: 0.0 for k in ["score_lsi", "score_control"]}

            for x, _ in tqdm(train_l, desc=f"Refine Ep {ep+1}", leave=False):
                x = x.to(device)
                B = x.shape[0]

                # --- Forward pass through frozen VAE ---
                with torch.no_grad():
                    _, mu, logvar = vae(x)
                    z0 = vae.reparameterize(mu, logvar)

                t = sample_log_uniform_times(B, cfg["t_min"], cfg["t_max"], device)
                alpha, sigma = get_ou_params(t.view(B,1,1,1))

                noise = torch.randn_like(z0)
                z_t = alpha * z0 + sigma * noise

                # --- LSI Score Training ---
                var_0 = torch.exp(logvar)
                mu_t = alpha * mu
                var_t = (alpha**2) * var_0 + (sigma**2)

                eps_target_lsi = sigma * ((z_t - mu_t) / (var_t + 1e-8))
                eps_pred_lsi = unet_lsi(z_t, t)
                score_loss_lsi = F.mse_loss(eps_pred_lsi, eps_target_lsi)

                opt_lsi_refine.zero_grad()
                score_loss_lsi.backward()
                nn.utils.clip_grad_norm_(unet_lsi.parameters(), 1.0)
                opt_lsi_refine.step()

                # --- EMA Update (LSI) ---
                with torch.no_grad():
                    for p_online, p_ema in zip(unet_lsi.parameters(), unet_lsi_ema.parameters()):
                        p_ema.data.mul_(ema_decay).add_(p_online.data, alpha=1 - ema_decay)

                # --- Control (Tweedie/DSM) Score Training ---
                eps_pred_control = unet_control(z_t.detach(), t)
                score_loss_control = F.mse_loss(eps_pred_control, noise)

                opt_control_refine.zero_grad()
                score_loss_control.backward()
                nn.utils.clip_grad_norm_(unet_control.parameters(), 1.0)
                opt_control_refine.step()

                # --- EMA Update (Control) ---
                with torch.no_grad():
                    for p_online, p_ema in zip(unet_control.parameters(), unet_control_ema.parameters()):
                        p_ema.data.mul_(ema_decay).add_(p_online.data, alpha=1 - ema_decay)

                metrics_refine["score_lsi"] += score_loss_lsi.item()
                metrics_refine["score_control"] += score_loss_control.item()

            # --- Refinement Logging ---
            print(f"Refine Ep {ep+1} | LSI: {metrics_refine['score_lsi']/len(train_l):.4f} | "
                  f"Ctrl: {metrics_refine['score_control']/len(train_l):.4f}")

            if (ep + 1) % eval_freq == 0:
                evaluate_current_state(
                    cfg["epochs_vae"] + ep + 1,
                    "LSI_Diff_Refine",
                    vae,
                    unet_lsi_ema,
                    test_l,
                    cfg,
                    device,
                    lpips_fn,
                    fixed_noise_bank=fixed_noise_bank,
                    fixed_posterior_eps_bank_A=fixed_posterior_eps_bank_A,
                    fixed_posterior_eps_bank_B=fixed_posterior_eps_bank_B,
                    fixed_sw2_theta=fixed_sw2_theta,
                )
                evaluate_current_state(
                    cfg["epochs_vae"] + ep + 1,
                    "Ctrl_Diff_Refine",
                    vae,
                    unet_control_ema,
                    test_l,
                    cfg,
                    device,
                    lpips_fn,
                    fixed_noise_bank=fixed_noise_bank,
                    fixed_posterior_eps_bank_A=fixed_posterior_eps_bank_A,
                    fixed_posterior_eps_bank_B=fixed_posterior_eps_bank_B,
                    fixed_sw2_theta=fixed_sw2_theta,
                )

    save_checkpoint(vae.state_dict(), os.path.join(cfg["ckpt_dir"], "vae_cotrained.pt"))
    # Save the EMA versions of the score heads as they are the stable result
    save_checkpoint(unet_lsi_ema.state_dict(), os.path.join(cfg["ckpt_dir"], "unet_lsi.pt"))
    save_checkpoint(unet_control_ema.state_dict(), os.path.join(cfg["ckpt_dir"], "unet_control.pt"))


# ---------------------------------------------------------------------------
# Evaluation
# ---------------------------------------------------------------------------

import matplotlib.pyplot as plt
import numpy as np

def main():

    # User Config
    cfg = {
        "batch_size": 128,
        "num_workers": 2,
        "score_w": 1.0,
        "lr_vae": 2e-4,
        "lr_ldm": 1e-3,
        "lr_refine": 1e-4,
        "epochs_vae": 100,
        "epochs_refine": 20,
        "latent_channels": 2,
        "kl_w": .01,
        "score_w_vae": 0.75,
        "perc_w": 1.0,
        "t_min": 5e-5,
        "t_max": 2.5,
        "ckpt_dir": "checkpoints_mnist_comp",
        "seed": 42,
        "use_fixed_eval_banks": True,
        "sw2_n_projections": 1000,
        "load_from_checkpoint": False,
        "eval_freq": 5,
    }

    seed_everything(cfg["seed"])
    ensure_dir(cfg["ckpt_dir"])
    ensure_dir("samples")

    print("=== Dual Co-Training: LSI vs Control (Tweedie) ===")
    train_vae_cotrained(cfg)

if __name__ == "__main__":
  main()